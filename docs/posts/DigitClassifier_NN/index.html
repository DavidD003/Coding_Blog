<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="David De Sa">
<meta name="dcterms.date" content="2023-03-14">

<title>Davids Coding - A Simple Digit Classifier</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../EigenFlower.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Davids Coding</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/daviddesa03/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@davidscoding"><i class="bi bi-youtube" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/DavidD003"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">A Simple Digit Classifier</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">pyTorch</div>
                <div class="quarto-category">NeuralNetworks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>David De Sa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 14, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tldr" id="toc-tldr" class="nav-link active" data-scroll-target="#tldr">TL;DR</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#what" id="toc-what" class="nav-link" data-scroll-target="#what">What?</a></li>
  <li><a href="#why" id="toc-why" class="nav-link" data-scroll-target="#why">Why?</a></li>
  <li><a href="#who" id="toc-who" class="nav-link" data-scroll-target="#who">Who?</a></li>
  <li><a href="#how" id="toc-how" class="nav-link" data-scroll-target="#how">How?</a></li>
  </ul></li>
  <li><a href="#code-review" id="toc-code-review" class="nav-link" data-scroll-target="#code-review">Code Review</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition">Data Acquisition</a>
  <ul class="collapse">
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation">Segmentation</a></li>
  <li><a href="#setup-dataloaders" id="toc-setup-dataloaders" class="nav-link" data-scroll-target="#setup-dataloaders">Setup DataLoaders</a></li>
  </ul></li>
  <li><a href="#laying-the-nn-foundations" id="toc-laying-the-nn-foundations" class="nav-link" data-scroll-target="#laying-the-nn-foundations">Laying The NN Foundations</a></li>
  <li><a href="#building-the-nn" id="toc-building-the-nn" class="nav-link" data-scroll-target="#building-the-nn">Building the NN</a></li>
  <li><a href="#calculating-loss" id="toc-calculating-loss" class="nav-link" data-scroll-target="#calculating-loss">Calculating Loss</a></li>
  <li><a href="#measuring-accuracy" id="toc-measuring-accuracy" class="nav-link" data-scroll-target="#measuring-accuracy">Measuring Accuracy</a></li>
  <li><a href="#execute-training" id="toc-execute-training" class="nav-link" data-scroll-target="#execute-training">Execute Training</a></li>
  <li><a href="#building-test-infrastructure" id="toc-building-test-infrastructure" class="nav-link" data-scroll-target="#building-test-infrastructure">Building Test Infrastructure</a></li>
  <li><a href="#experimenting-to-optimize-nn" id="toc-experimenting-to-optimize-nn" class="nav-link" data-scroll-target="#experimenting-to-optimize-nn">Experimenting To Optimize NN</a>
  <ul class="collapse">
  <li><a href="#first-try" id="toc-first-try" class="nav-link" data-scroll-target="#first-try">First Try</a></li>
  <li><a href="#second-try" id="toc-second-try" class="nav-link" data-scroll-target="#second-try">Second Try</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>The best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! Part 2 of 2. I built this in following along with the awesome lessons over at <a href="course.fast.ai">fast.ai</a>. Many thanks to that team.</p>
</section>
<section id="overview" class="level1">
<h1>Overview</h1>
<p>We’re going to jump in where we left off in Part 1: We’ll just reimport our data, and get into it.</p>
<section id="what" class="level3">
<h3 class="anchored" data-anchor-id="what">What?</h3>
<p>We’re going to create a neural network that, given a picture of a numeric digit, identifies the number.</p>
</section>
<section id="why" class="level3">
<h3 class="anchored" data-anchor-id="why">Why?</h3>
<p>Does this really need explaining? Because the technology is amazing in both practical and philosophical ways.</p>
</section>
<section id="who" class="level3">
<h3 class="anchored" data-anchor-id="who">Who?</h3>
<p><a href="https://davidd003.github.io/Coding_Blog/about.html">Who am I</a>!? Who are you?!</p>
</section>
<section id="how" class="level3">
<h3 class="anchored" data-anchor-id="how">How?</h3>
<p>Using <a href="https://pytorch.org/">PyTorch</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, an opensource toolkit for building neural networks. Truly an elevator ride up onto the shoulders of giants.</p>
</section>
</section>
<section id="code-review" class="level1 page-columns page-full">
<h1>Code Review</h1>
<p>We’ll first just re-import our data, and then get into building the groundwork for our neural network</p>
<p>Let’s get into it!</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The required dependencies!:<code>scikit-learn</code>, <code>fastbook</code>, <code>matplotlib</code></p>
</div>
</div>
</div>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Install dependency</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> fastbook</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>fastbook.setup_book()</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>matplotlib.rc(<span class="st">"image"</span>, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="data-acquisition" class="level2">
<h2 class="anchored" data-anchor-id="data-acquisition">Data Acquisition</h2>
<p>A quick flashback to Part 1 of my Building A Digit Classifier blog!:</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Understand Your Input!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the <a href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">source page</a>:</p>
<blockquote class="blockquote">
<p>We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>mnist <span class="op">=</span> load_digits()</span>
<span id="cb2-2"><a href="#cb2-2"></a>stacked <span class="op">=</span> []</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co"># This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb2-5"><a href="#cb2-5"></a>    stacked.append([])</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co"># Assign all images to the right collection in the 'stacked' list, indexed by target</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mnist[<span class="st">"target"</span>])):</span>
<span id="cb2-8"><a href="#cb2-8"></a>    stacked[mnist[<span class="st">"target"</span>][i]].append(mnist[<span class="st">"images"</span>][i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="segmentation" class="level3">
<h3 class="anchored" data-anchor-id="segmentation">Segmentation</h3>
<p>The next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn’t generate a validation set with a large imbalance in the number of classes to be tested in it.</p>
<p>First we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.</p>
<p>We’ll print out the size of these collections and take a peek at a sample to make sure we indexed right.</p>
<div id="fig-checkSamp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># To make dataset a tensor, make it same number of dimensions</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>stacked <span class="op">=</span> tensor([x[:<span class="dv">174</span>] <span class="cf">for</span> x <span class="kw">in</span> stacked])</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># Segmentation: Pull 20 of each digit out of training set</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>test <span class="op">=</span> [dig[<span class="op">-</span><span class="dv">20</span>:] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb3-5"><a href="#cb3-5"></a>train <span class="op">=</span> [dig[:<span class="op">-</span><span class="dv">20</span>] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># Confirm counts of samples</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>[<span class="bu">len</span>(test[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)], [<span class="bu">len</span>(train[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb3-8"><a href="#cb3-8"></a>show_image(stacked[<span class="dv">3</span>][<span class="dv">0</span>])  <span class="co"># Check sample</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="79">
<pre><code>&lt;Axes: &gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-2.png" width="93" height="93" class="figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Is it a 3?</figcaption><p></p>
</figure>
</div>
<p>Nice.</p>
<p>It’s important to keep track of what’s what.</p>
<div class="cell" data-execution_count="4">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="bu">type</span>(train), <span class="bu">type</span>(train[<span class="dv">0</span>]), <span class="bu">type</span>(train[<span class="dv">0</span>][<span class="dv">0</span>]), [</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="bu">type</span>(test), <span class="bu">type</span>(test[<span class="dv">0</span>]), <span class="bu">type</span>(test[<span class="dv">0</span>][<span class="dv">0</span>])]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="80">
<pre><code>(list, torch.Tensor, torch.Tensor, [list, torch.Tensor, torch.Tensor])</code></pre>
</div>
</div>
<p>Ok so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.</p>
<p>Instead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it’s an easy conversion:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>train <span class="op">=</span> torch.stack(train)</span>
<span id="cb7-3"><a href="#cb7-3"></a>test <span class="op">=</span> torch.stack(test)</span>
<span id="cb7-4"><a href="#cb7-4"></a>train.shape, test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>(torch.Size([10, 154, 8, 8]), torch.Size([10, 20, 8, 8]))</code></pre>
</div>
</div>
<p>Now here is a critical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size.</p>
</section>
<section id="setup-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="setup-dataloaders">Setup DataLoaders</h3>
<p>First get data into the requisite shape for the processes that will follow.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Build training/valid sets for passing to DataLoader</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>train_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> train]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a>test_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> test]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a>train_y, test_y <span class="op">=</span> [], []</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb9-6"><a href="#cb9-6"></a>    train_y.extend([n]<span class="op">*</span><span class="dv">154</span>)</span>
<span id="cb9-7"><a href="#cb9-7"></a>    test_y.extend([n]<span class="op">*</span><span class="dv">20</span>)</span>
<span id="cb9-8"><a href="#cb9-8"></a>train_y <span class="op">=</span> tensor(train_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-9"><a href="#cb9-9"></a>test_y <span class="op">=</span> tensor(test_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-10"><a href="#cb9-10"></a>train_x.shape, test_x.shape, train_y.shape, test_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="82">
<pre><code>(torch.Size([1540, 64]),
 torch.Size([200, 64]),
 torch.Size([1540, 1]),
 torch.Size([200, 1]))</code></pre>
</div>
</div>
<p>Note the unsqueezing such that the <code>_y</code> tensors have a singleton dimension. Might seem strange on first blush but makes sense when considering that our ‘target’ just happens to be individual integers. But in other use cases, the target could be something more complex, requiring a larger data structure. In which case that dimension could be larger than a singleton.</p>
<p>Now we can prime our DataLoaders. DataLoaders are a super handy object from the Pytorch library that will make life easier by holding our data for us, and serving it up in randomized batches.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Zip each input data item to its target output</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x, train_y))</span>
<span id="cb11-3"><a href="#cb11-3"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(test_x, test_y))</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Define DataLoader objects to pass to learner</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">25</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb11-6"><a href="#cb11-6"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a>dls <span class="op">=</span> DataLoaders(dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="laying-the-nn-foundations" class="level2">
<h2 class="anchored" data-anchor-id="laying-the-nn-foundations">Laying The NN Foundations</h2>
<p>Every NN needs weights and biases. We’re going to want to initialize lots of them, and in differently shaped structures, so first we define a function to help us do that:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">.1</span>): <span class="cf">return</span> (torch.randn(size)<span class="op">*</span>std).requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Requiring Grad!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Note that any memory structures created with that function will also store their gradient in memory as computation is performed on them. This will be critical to executing back propagation of the gradient so as to optimize our function.</p>
</div>
</div>
</div>
<p>Now we can generate our weights and biases. We define the structure sizes with our input and output size in mind, and the principles of matrix multiplication.</p>
<p>The general structure we will start with will be, of course, an initial layer with one neuron for each input, which will yield a single activation per input. After a nonlinearty, the second layer will have 10 weights and biases, such that the output is a 1D vector of size 10, per input. Each output neuron will represent an estimation of likelihood that the integer is that which the neuron represents (0-9).</p>
<p><a href="#fig-Layer1">Figure&nbsp;2</a> shows the start of the proces… A row in the input batch represents a single image. At the end of the 1<sup>st</sup> layer, it is reduced to a single activation value to be passed through a non-linearity prior to going through the 2<sup>nd</sup> layer</p>
<div id="fig-Layer1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Layer 1 Diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Visualizing Layer 1</figcaption><p></p>
</figure>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Matrix Multiplication!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Never fear! It’s easy. I never forgot the trick Dr.&nbsp;Mohammed Jaber taught us 4th year as we had to use these methods from first year: ROW. COLUMN.</p>
<p>Take row i from the first matrix, and column j from the second. The element in index i,j of the resulting matrix will be the dot product of that row and column vector.</p>
</div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Have to provide extra dimension for matrix mutiplication, making it a single-column vector</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb13-3"><a href="#cb13-3"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># Have to provide an extra dimension (1) for matrix multiplication</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb13-6"><a href="#cb13-6"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Layer 1:
<ul>
<li>64 weights, one per pixel. Formatted in a column vector because they are the second term in the matrix multiplication equation, with a single input (image) being fed as a single row vector of 64 elements, one per pixel.</li>
<li>1 bias; it will be broadcast such that the same bias is added to all instances in our batch. See <a href="#fig-Layer1">Figure&nbsp;2</a></li>
</ul></li>
<li>Layer 2:
<ul>
<li>10 weights, to generate one output neuron per category/class to assign.</li>
<li>1 bias, to be <em>broadcast</em> across each input vector.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul></li>
</ul>
<p>We will use the basic <a href="https://deepai.org/machine-learning-glossary-and-terms/relu">ReLU</a> activation function as the non-linearity between the two linear layers.</p>
</section>
<section id="building-the-nn" class="level2">
<h2 class="anchored" data-anchor-id="building-the-nn">Building the NN</h2>
<p>Similarly to our parameter generation function, we want o make our little network into a function block so that we can treat it as a black box to plug and play with other code sections:</p>
<div class="cell" data-execution_count="10">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">def</span> myModel(xb):</span>
<span id="cb14-2"><a href="#cb14-2"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1<span class="op">+</span>b1</span>
<span id="cb14-3"><a href="#cb14-3"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb14-4"><a href="#cb14-4"></a>    res <span class="op">=</span> res<span class="op">@</span>w2<span class="op">+</span>b2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s take it for a spin! We’ll manually identify a subset of our training dataset for testing purposes.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>mini_samp <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>mini_x <span class="op">=</span> train_x[:mini_samp]</span>
<span id="cb15-3"><a href="#cb15-3"></a>mini_y <span class="op">=</span> train_y[:mini_samp]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="12">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>res <span class="op">=</span> myModel(mini_x)</span>
<span id="cb16-2"><a href="#cb16-2"></a>res, res.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="88">
<pre><code>(tensor([[-0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491],
         [-0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491],
         [-0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491],
         [-0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491],
         [-0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491, -0.0491]], grad_fn=&lt;AddBackward0&gt;),
 torch.Size([5, 10]))</code></pre>
</div>
</div>
<p>Excellent! For the 5 input images passed in, we have an output that is 5 vectors of size 10, that is, each input has 10 output values, one for each possible classification outcome 0 through 9.</p>
<p>And note that the result tensor retains the gradient! This comes into play in the next step.</p>
</section>
<section id="calculating-loss" class="level2">
<h2 class="anchored" data-anchor-id="calculating-loss">Calculating Loss</h2>
<p>Loss is the measure of the neural networks success. The most critical characteristic of a loss function is that it be differentiable with a smooth gradient. But just because the math doesn’t break doesn’t mean the function will eb any good. There are a variety of well understood functions that can be used, with selection being a matter of preference as well as the use case at hand.</p>
<p>Since we are assigning a single class to the input, from multiple options, we’ll use the SoftMax function. It reduces all outputs to a number between 0 and 1, with a given number being exponentially closer to one, the larger it is compared to the other numbers in the set.</p>
<p>This is a double-hitter: it allows the net to learn to win by giving the correct digit’s output neuron the highest activation, while also motivating the reduction in activation of the incorrect neurons.</p>
<div class="cell" data-execution_count="13">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> my_loss(preds, target, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    maxes <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-3"><a href="#cb18-3"></a>    correctIndices <span class="op">=</span> [r[t] <span class="cf">for</span> r, t <span class="kw">in</span> <span class="bu">zip</span>(maxes, target)]</span>
<span id="cb18-4"><a href="#cb18-4"></a>    resy <span class="op">=</span> [<span class="op">-</span>torch.log(tens) <span class="cf">for</span> tens <span class="kw">in</span> correctIndices]</span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="cf">return</span> <span class="bu">sum</span>(resy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Keep in mind that we need to ensure these functions can work with different batch sizes. Fortunately, the PyTorch built in functions are designed for this.</p>
<p>Our function first takes the SoftMax of the outputs for each input. Then, it pulls out the resulting value for the index of the correct class. Recall, this will be a number between 0 and 1. If we took this number alone as our loss, measure, there would be a couple of issues.</p>
<p>First of all, convention is that we want to <em>reduce</em> loss in the course of our optimization. But we have defined the index of the <em>highest</em> softmax result as our classification integer. So that doesn’t jive. Secondly, it isn’t desirable to have the measure of loss constrained to such a small range 0 to 1, because this essentially reduces the information we’re getting - large differences in inputs will show very small changes in loss. For effective learning, we would want a strong learning signal, i.e.&nbsp;small changes to input parameters reflecting meaningful differences in the measure of loss.</p>
<p>The solution? First, taking the log of the SoftMax activation. As the following code shows, a difference of 0.001 in the input to the log function creates a differences of more than 0.01 in the output, an order of magnitude difference.</p>
<div class="cell" data-execution_count="14">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>torch.log(Tensor([<span class="fl">.5</span>, <span class="fl">.499</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="90">
<pre><code>tensor([-0.6931, -0.6951])</code></pre>
</div>
</div>
<p>After the log, we took the negative of those values. That’s because the log of a number between 0 and 1 will always be negative, getting absolutely larger the further from 1 it is. Seeing as we want the output to be absolutely smaller the closer to 1 it is, taking the negative accomplishes this perfectly.</p>
<p>Lastly, we summed the loss measure across the samples in the mini batch since we want to optimize globally, not just for individual samples.</p>
<p>So let’s test it!</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># Testing the loss function works(assuming mini_x of 3 samples)</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>lossResults <span class="op">=</span> my_loss(myModel(mini_x), mini_y)</span>
<span id="cb21-3"><a href="#cb21-3"></a>lossResults</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="91">
<pre><code>tensor([11.5129], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>This output might look nice, but it’s a roll of the dice!</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Beware Of Implementing Your Own Math
</div>
</div>
<div class="callout-body-container callout-body">
<p>While the formula for the SoftMax is easy enough to understand, Our home-brewed version won’t work. I tried to do so at first and had all kinds of headaches arising from arithmetic underflow in the outcomes! How the hell do you deal with that? The best solution: Use the function implementation baked into the PyTorch libraries! Let the pro’s handle those <em>deep</em> issues. We’re still riding on training wheels!</p>
</div>
</div>
<p>It turns out the <code>-log</code> of the SoftMax is the Cross Entropy Loss Function. And just importing it from the PyTorch library will save us from the pitfall described in the preceding blurb… So here is our final loss function!</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">def</span> my_loss(preds, target):</span>
<span id="cb23-2"><a href="#cb23-2"></a>    loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb23-3"><a href="#cb23-3"></a>    <span class="co"># Turn tensor of singleton tensors one per target into a single tensor with all as elements</span></span>
<span id="cb23-4"><a href="#cb23-4"></a>    tgts <span class="op">=</span> target.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-5"><a href="#cb23-5"></a>    <span class="cf">return</span> loss_fn(preds, tgts)  <span class="co"># Nice</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now if you’re reading this you’re probably thinking that <em>gradient descent</em> is the next step here, so as to reduce the loss between minibatches, but we’ll take a quick detour first.</p>
</section>
<section id="measuring-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="measuring-accuracy">Measuring Accuracy</h2>
<p>Before going further, let’s take a minute to set up some functions we’ll need in the training process. The first yields the percentage accuracy of the model across a batch, and the second function builds on that to iterate across the batchers in our valdiation set and yield the accuracy rate across a whole epoch<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">def</span> batch_accuracy(mdl, xb, yb):</span>
<span id="cb24-2"><a href="#cb24-2"></a>    otpts <span class="op">=</span> mdl(xb)  <span class="co"># Get output activations from model</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>    <span class="co"># The indices of the max activation is the predicted digit of the input</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>    preds <span class="op">=</span> otpts.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb24-5"><a href="#cb24-5"></a>    <span class="co"># Types must be tensors to return sequence of true/false</span></span>
<span id="cb24-6"><a href="#cb24-6"></a>    correct <span class="op">=</span> preds <span class="op">==</span> yb.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-7"><a href="#cb24-7"></a>    <span class="co">#                    Use view to take it from shape=[5,1] to [5], same as preds. else will broadcast and end result all messed up</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a></span>
<span id="cb24-11"><a href="#cb24-11"></a><span class="kw">def</span> validate_epoch(mdl):</span>
<span id="cb24-12"><a href="#cb24-12"></a>    outcomes <span class="op">=</span> [batch_accuracy(mdl, xb, yb) <span class="cf">for</span> xb, yb <span class="kw">in</span> valid_dl]</span>
<span id="cb24-13"><a href="#cb24-13"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(outcomes).mean().item(), <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>It’s always good to double check these functions are working as intended after making them… Let’s grab some validation set data, double checking along the way that the outputs (integer assignment) match the input (drawings of digit):</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb25-2"><a href="#cb25-2"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb25-4"><a href="#cb25-4"></a>    show_image(xv[i].view((<span class="dv">8</span>, <span class="dv">8</span>)))</span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="bu">print</span>(yv[i].data.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>1
6
6
3
3</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-2.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-3.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-4.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-5.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-6.png" width="93" height="93"></p>
</div>
</div>
<p>Ok nice, now lets see that our validation function is yielding the correct accuracy score on classifying a mini batch. Since we haven’t trained the model yet we’ll expect the performacne to be junk but that doesn’t mean we can’t test the % accuracy function.</p>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb27-2"><a href="#cb27-2"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb27-3"><a href="#cb27-3"></a>o <span class="op">=</span> myModel(xv)</span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="bu">print</span>(o.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices, yv, batch_accuracy(myModel, xv, yv))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([0, 0, 0, 0, 0]) tensor([[3],
        [8],
        [0],
        [5],
        [6]]) tensor(0.2000)</code></pre>
</div>
</div>
<p>We want to see the last number spit out be an accurate representation of how many numbers match between the first and second tensor output. I don’t know what the final blog output will be since the inputs to the neural net are random every time, but I re ran this code cell a number of times, seeing correct assignments of one or two integers to confirm the functions were working as expected.</p>
</section>
<section id="execute-training" class="level2">
<h2 class="anchored" data-anchor-id="execute-training">Execute Training</h2>
<p>Alright now we get to the good stuff! First we’ll make an iterator to load our test dataset.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>testLoader <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And now we we will run our first training loop… the steps are:</p>
<ul>
<li>Define learning rate<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Pull out a minibatch of inputs and target values from the test data set.</li>
<li>Calculate the loss on this batch by passing the outputs from the model through the loss function</li>
<li>Execute the <code>.backward()</code> method to calculate the gradient for all parameters.</li>
<li>The next bits will be executed with torch.no_grad(), because we don’t want the math inherent to calibrating the parameters themselves to have its gradient captured.</li>
<li>Having the gradient stored in the tensor object of each parameter, update each parameter by subtracting the gadient multiplied by the learning rate.</li>
<li>Reset the gradient to zero for the next learning iteration.</li>
</ul>
<div class="cell" data-execution_count="21">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb30-3"><a href="#cb30-3"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb30-4"><a href="#cb30-4"></a>loss.backward()</span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb30-7"><a href="#cb30-7"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb30-8"><a href="#cb30-8"></a>        p.grad.zero_()</span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loss before training: 2.314004421234131  |   Loss after training: 2.3128905296325684</code></pre>
</div>
</div>
<p>Success! Our measure of loss has reduced after re running the same batch with the adjusted N parameters. Across a large enough dataset<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, we would expect to see a reduced error rate associated with this.</p>
<p>Let’s try it again, this time printing out the average values of the parameters in the weights and bias vectors, to get a sense for how much they change from one iteration to the next.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the learning (copied from last cell):</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Average parameters:  tensor([[-0.0224,  0.0000],
        [ 0.0370,  0.0000],
        [-0.0443,  0.0000],
        [-0.0491,  0.0000]])
Loss before training: 2.307793617248535  |   Loss after training: 2.3069679737091064
Average parameters:  tensor([[-0.0226,  0.0000],
        [ 0.0370,  0.0000],
        [-0.0443,  0.0000],
        [-0.0491,  0.0000]])</code></pre>
</div>
</div>
<p>What we see here is that our change in paramters created a reduction in the measure of loss, <em>even though</em> it wasn’t even identifiable at 4 decimal points accuracy!</p>
<p>So let’s take it to the next level. We trained on one minibatch of data. Let’s try doing a whole epoch- iterating over every minibatch in the training set.</p>
<p>First, we’ll reset our weights to random numbers, clearing the slate of that last learning:</p>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb34-2"><a href="#cb34-2"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb34-4"><a href="#cb34-4"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb34-5"><a href="#cb34-5"></a></span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="co"># Print average param values again:</span></span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb34-8"><a href="#cb34-8"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb34-9"><a href="#cb34-9"></a><span class="cf">else</span>:</span>
<span id="cb34-10"><a href="#cb34-10"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb34-11"><a href="#cb34-11"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb34-12"><a href="#cb34-12"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb34-13"><a href="#cb34-13"></a>        p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([ 0.0039,  0.1684,  0.0347, -0.0492])</code></pre>
</div>
</div>
<p>And now execute learning.</p>
<div class="cell" data-execution_count="24">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb36-3"><a href="#cb36-3"></a>    loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb36-4"><a href="#cb36-4"></a>    loss.backward()</span>
<span id="cb36-5"><a href="#cb36-5"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb36-6"><a href="#cb36-6"></a>        <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb36-7"><a href="#cb36-7"></a>            p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb36-8"><a href="#cb36-8"></a>            p.grad.zero_()</span>
<span id="cb36-9"><a href="#cb36-9"></a>    <span class="bu">print</span>(<span class="st">"Loss: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()))  <span class="co"># Prints loss for each minibatch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.199075937271118
Loss: 2.2563600540161133
Loss: 2.2406294345855713
Loss: 2.255134344100952
Loss: 2.3489809036254883
Loss: 2.2468106746673584
Loss: 2.2854390144348145
Loss: 2.2784039974212646
Loss: 2.376652717590332
Loss: 2.32565975189209
Loss: 2.2398929595947266
Loss: 2.3289480209350586
Loss: 2.2844693660736084
Loss: 2.2962541580200195
Loss: 2.2766005992889404
Loss: 2.2820425033569336
Loss: 2.208583116531372
Loss: 2.2875723838806152
Loss: 2.2431399822235107
Loss: 2.2146613597869873
Loss: 2.203139305114746
Loss: 2.2511353492736816
Loss: 2.165519952774048
Loss: 2.128993511199951
Loss: 2.2865686416625977
Loss: 2.320838689804077
Loss: 2.237919807434082
Loss: 2.288886785507202
Loss: 2.1258111000061035
Loss: 2.175995111465454
Loss: 2.274925708770752
Loss: 2.125221014022827
Loss: 2.3225929737091064
Loss: 2.2280237674713135
Loss: 2.1571156978607178
Loss: 2.1414029598236084
Loss: 2.1865856647491455
Loss: 2.2655110359191895
Loss: 2.188227653503418
Loss: 2.2199552059173584
Loss: 2.3190643787384033
Loss: 2.2241218090057373
Loss: 2.222172975540161
Loss: 2.2191574573516846
Loss: 2.1026976108551025
Loss: 2.240053653717041
Loss: 2.174922227859497
Loss: 2.0920538902282715
Loss: 2.1837966442108154
Loss: 2.2589786052703857
Loss: 2.058783769607544
Loss: 2.119903802871704</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.1753287315368652
Loss: 2.136204481124878
Loss: 2.1862473487854004
Loss: 2.1653738021850586
Loss: 2.1605117321014404
Loss: 2.150763988494873
Loss: 2.1495931148529053
Loss: 2.20322322845459
Loss: 2.0865442752838135
Loss: 2.222790479660034</code></pre>
</div>
</div>
<p>Hmmm…. sometimes when I run this I see the loss stay around the same value, other times it drops just a little before stabilizing…</p>
<p>We are going to need a better way to look at the trending of the loss function across iterations, so we may as well make that function now:</p>
<div class="cell" data-execution_count="25">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a><span class="kw">def</span> calc_grad(xb, yb, model, f_loss):</span>
<span id="cb39-2"><a href="#cb39-2"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb39-3"><a href="#cb39-3"></a>    loss <span class="op">=</span> f_loss(preds, yb)</span>
<span id="cb39-4"><a href="#cb39-4"></a>    loss.backward()</span>
<span id="cb39-5"><a href="#cb39-5"></a>    <span class="cf">return</span> loss.data.item()  <span class="co"># Return the loss (see why later)</span></span>
<span id="cb39-6"><a href="#cb39-6"></a></span>
<span id="cb39-7"><a href="#cb39-7"></a></span>
<span id="cb39-8"><a href="#cb39-8"></a>calc_grad(mini_x, mini_y, myModel, my_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="101">
<pre><code>2.3867080211639404</code></pre>
</div>
</div>
<p>This is another step towards modularizing (if that is a word) our code. Making use of that last function, our training loop now looks like this:</p>
<div class="cell" data-execution_count="26">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a><span class="co"># Initializing weights again for a fresh start</span></span>
<span id="cb41-2"><a href="#cb41-2"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb41-3"><a href="#cb41-3"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb41-4"><a href="#cb41-4"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb41-5"><a href="#cb41-5"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb41-6"><a href="#cb41-6"></a></span>
<span id="cb41-7"><a href="#cb41-7"></a><span class="bu">print</span>(<span class="st">"PreTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb41-8"><a href="#cb41-8"></a></span>
<span id="cb41-9"><a href="#cb41-9"></a></span>
<span id="cb41-10"><a href="#cb41-10"></a><span class="kw">def</span> train_epoch(model, lr, params, f_loss):</span>
<span id="cb41-11"><a href="#cb41-11"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb41-12"><a href="#cb41-12"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb41-13"><a href="#cb41-13"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb41-14"><a href="#cb41-14"></a>            <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb41-15"><a href="#cb41-15"></a>                p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb41-16"><a href="#cb41-16"></a>                p.grad.zero_()</span>
<span id="cb41-17"><a href="#cb41-17"></a></span>
<span id="cb41-18"><a href="#cb41-18"></a></span>
<span id="cb41-19"><a href="#cb41-19"></a>train_epoch(myModel, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb41-20"><a href="#cb41-20"></a><span class="bu">print</span>(<span class="st">"PostTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PreTrain Accuracy: 0.195</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>PostTrain Accuracy: 0.2</code></pre>
</div>
</div>
<p>UH OH!</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Overfitting
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Our loss is going down, but our error is erratic, staying the same, or going up! This is an indicator that we’ve reached a point where the mathematical strategy we’re using to optimize is now playing games with us. The modifications <em>do</em> reduce loss, but that is no longer aligned with an improvement with the task. Recall that a loss function <em>should</em> always correlate with NN efficacy, but this can break down at the fringes.</p>
</div>
</div>
</div>
<p>Is this a problem? Yes! But one that can be solved. Overall we are still in good territory.. We know the math and programming is working from a nuts and bolts perspective. What this indicates to us is that we need to tune the <em>metaparameters</em> of our neural network function. How many layers are there? How wide are the layers? We may be in a situation where the NN doesn’t have sufficient <em>capacity</em><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> to get us the outcomes we’re looking for.</p>
</section>
<section id="building-test-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="building-test-infrastructure">Building Test Infrastructure</h2>
<p>We’ve built an NN from first principles now, putting together the building blocks of parameter initialization, running the algebra through the net, backpropagation to compute the gradient, and parameter calibration through the gradient descent method. This all executing repeatedly across batches of data.</p>
<p>Before we adjust the network structure to optimize the outcomes, let’s define a yet higher encapsulation of these tools to further condense the commands needed to execute this whole process, as well as enable more execution flexibility. This is where the <code>BasicOptim</code> class omes in:</p>
<div class="cell" data-execution_count="27">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="kw">class</span> BasicOptim:</span>
<span id="cb44-2"><a href="#cb44-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr): <span class="va">self</span>.params, <span class="va">self</span>.lr <span class="op">=</span> <span class="bu">list</span>(params), lr</span>
<span id="cb44-3"><a href="#cb44-3"></a></span>
<span id="cb44-4"><a href="#cb44-4"></a>    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb44-5"><a href="#cb44-5"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb44-6"><a href="#cb44-6"></a>            p.data <span class="op">-=</span> p.grad.data <span class="op">*</span> <span class="va">self</span>.lr</span>
<span id="cb44-7"><a href="#cb44-7"></a></span>
<span id="cb44-8"><a href="#cb44-8"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb44-9"><a href="#cb44-9"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb44-10"><a href="#cb44-10"></a>            p.grad <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now, we can condense the training loop, and also neatly add the function of tracking loss across batches.</p>
<div class="cell" data-execution_count="28">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="kw">def</span> train_epoch(model, opt, lr, params, f_loss):</span>
<span id="cb45-2"><a href="#cb45-2"></a>    losses <span class="op">=</span> []  <span class="co"># Will allow for recording epoch wise loss</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb45-4"><a href="#cb45-4"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb45-5"><a href="#cb45-5"></a>        opt.step()</span>
<span id="cb45-6"><a href="#cb45-6"></a>        losses.append(calc_grad(xb, yb, model, f_loss))</span>
<span id="cb45-7"><a href="#cb45-7"></a>        opt.zero_grad()</span>
<span id="cb45-8"><a href="#cb45-8"></a>    <span class="cf">return</span> tensor(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And per usual, give it a test:</p>
<div class="cell" data-execution_count="29">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a>my_opt <span class="op">=</span> BasicOptim([w1, b1, w2, b2], <span class="fl">0.01</span>)</span>
<span id="cb46-2"><a href="#cb46-2"></a>res <span class="op">=</span> train_epoch(myModel, my_opt, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb46-3"><a href="#cb46-3"></a>res.mean(), res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="105">
<pre><code>(tensor(2.0876),
 tensor([2.1980, 2.1252, 2.1773, 2.1694, 2.1912, 2.2293, 2.1469, 2.0619, 2.1294, 2.1239, 2.0797, 2.0302, 2.0124, 2.0873, 2.0908, 2.0695, 2.2541, 2.0502, 2.0780, 2.2446, 2.0911, 2.0455, 2.1385, 2.1759,
         1.9776, 2.0968, 2.0866, 2.0702, 2.0632, 2.1360, 2.0806, 2.0838, 2.0376, 2.0701, 1.8794, 2.1304, 2.0233, 1.9755, 2.0896, 2.0513, 2.0882, 2.0572, 2.1628, 2.0935, 2.1328, 1.9948, 2.1807, 2.1097,
         1.9890, 2.0817, 2.1764, 2.1576, 1.9863, 2.0150, 2.0294, 2.1838, 1.9823, 2.0590, 2.0871, 2.1473, 1.8388, 2.0286]))</code></pre>
</div>
</div>
<p>Great: bug free. But… still this issue of the loss bouncing around withot a clear improvement trend…</p>
<p>Knowing that I want to experiment with the models metaprameters to improve it, I’ll finally define yet another object to contain those we’ve made so far and run trials with customized parameters, and providing the means for monitoring these trials more easily.</p>
<div class="cell" data-execution_count="30">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="kw">class</span> cTrial:</span>
<span id="cb48-2"><a href="#cb48-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, numE<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.01</span>, model<span class="op">=</span>myModel, opt<span class="op">=</span>my_opt, params<span class="op">=</span>[w1, b1, w2, b2], f_loss<span class="op">=</span>my_loss):</span>
<span id="cb48-3"><a href="#cb48-3"></a>        <span class="va">self</span>.numE <span class="op">=</span> numE</span>
<span id="cb48-4"><a href="#cb48-4"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb48-5"><a href="#cb48-5"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb48-6"><a href="#cb48-6"></a>        <span class="va">self</span>.opt <span class="op">=</span> opt</span>
<span id="cb48-7"><a href="#cb48-7"></a>        <span class="va">self</span>.params <span class="op">=</span> params</span>
<span id="cb48-8"><a href="#cb48-8"></a>        <span class="va">self</span>.f_loss <span class="op">=</span> f_loss</span>
<span id="cb48-9"><a href="#cb48-9"></a>        <span class="va">self</span>.res <span class="op">=</span> []</span>
<span id="cb48-10"><a href="#cb48-10"></a>        <span class="va">self</span>.valids <span class="op">=</span> []</span>
<span id="cb48-11"><a href="#cb48-11"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> []  <span class="co"># For tracking change in weights across learning</span></span>
<span id="cb48-12"><a href="#cb48-12"></a></span>
<span id="cb48-13"><a href="#cb48-13"></a>    <span class="kw">def</span> run(<span class="va">self</span>, numE<span class="op">=</span><span class="va">None</span>, wkLr<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb48-14"><a href="#cb48-14"></a>        <span class="va">self</span>.valids <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb48-15"><a href="#cb48-15"></a>        epch_losses <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb48-16"><a href="#cb48-16"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> [[], [], [], []]  <span class="co"># 4 contents, w1,b1,w2,b2</span></span>
<span id="cb48-17"><a href="#cb48-17"></a>        <span class="cf">if</span> numE <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb48-18"><a href="#cb48-18"></a>            numE <span class="op">=</span> <span class="va">self</span>.numE</span>
<span id="cb48-19"><a href="#cb48-19"></a>        <span class="cf">if</span> wkLr <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb48-20"><a href="#cb48-20"></a>            wkLr <span class="op">=</span> <span class="va">self</span>.lr</span>
<span id="cb48-21"><a href="#cb48-21"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numE):</span>
<span id="cb48-22"><a href="#cb48-22"></a>            <span class="co"># -- Record wts for analysis</span></span>
<span id="cb48-23"><a href="#cb48-23"></a>            <span class="va">self</span>.wtsHist[<span class="dv">0</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb48-24"><a href="#cb48-24"></a>            <span class="va">self</span>.wtsHist[<span class="dv">1</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb48-25"><a href="#cb48-25"></a>            <span class="va">self</span>.wtsHist[<span class="dv">2</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb48-26"><a href="#cb48-26"></a>            <span class="va">self</span>.wtsHist[<span class="dv">3</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb48-27"><a href="#cb48-27"></a>            <span class="co"># --</span></span>
<span id="cb48-28"><a href="#cb48-28"></a>            res <span class="op">=</span> train_epoch(<span class="va">self</span>.model, <span class="va">self</span>.opt, <span class="va">self</span>.lr,</span>
<span id="cb48-29"><a href="#cb48-29"></a>                              <span class="va">self</span>.params, <span class="va">self</span>.f_loss)</span>
<span id="cb48-30"><a href="#cb48-30"></a>            epch_losses.append(res)</span>
<span id="cb48-31"><a href="#cb48-31"></a>            <span class="va">self</span>.valids.append(validate_epoch(<span class="va">self</span>.model))</span>
<span id="cb48-32"><a href="#cb48-32"></a>        <span class="va">self</span>.res <span class="op">=</span> torch.stack(epch_losses)</span>
<span id="cb48-33"><a href="#cb48-33"></a>        <span class="va">self</span>.valids <span class="op">=</span> tensor(<span class="va">self</span>.valids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This object allows for defining all the parameters necessary for a trial, and then running it one line of code, alternaitvely overwriting the number of Epochs, and learning rate parameters.</p>
</section>
<section id="experimenting-to-optimize-nn" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experimenting-to-optimize-nn">Experimenting To Optimize NN</h2>
<p>Let’s get into it. We make a trial object and run it:</p>
<div class="cell" data-execution_count="31">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a>my_try <span class="op">=</span> cTrial()</span>
<span id="cb49-2"><a href="#cb49-2"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb49-3"><a href="#cb49-3"></a>my_try.res.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="107">
<pre><code>torch.Size([50, 62])</code></pre>
</div>
</div>
<p>Our functions are logging loss for each mini batch across all epochs. So this makes sense. We had 1540 data points, with minibatches of 25. <code>25*62=1550</code>, so we have 61 minibatches of 25 one last minibatch of 15. And 50 collections of 62, one for each epoch.</p>
<section id="first-try" class="level3">
<h3 class="anchored" data-anchor-id="first-try">First Try</h3>
<p>So let’s see the results by taking the average loss across each epoch, hopefully, it’ll be dropping!</p>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-loss1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-loss1-output-1.png" width="585" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Average loss across epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>So loss is going down, it’s working right? The real test isn’t reduction to our measure loss, but to see that carry through to an increase in testing accuracy:</p>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>plt.plot(my_try.valids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-val1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-val1-output-1.png" width="595" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Prediction accuracy across epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Disaster strikes! We’re never getting more than 20% accuracy! Well, they call it <em>deep learning</em>, so let’s make our model deeper. Two layers? A pittance!</p>
</section>
<section id="second-try" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="second-try">Second Try</h3>
<p>Our Model V2 will look like this:</p>
<ul>
<li>64 input activations (image pixels) to 32 neurons in the first layer, with a unique (not broadcasted) bias for each neuron</li>
<li>The next layer will reduce the 32 neurons to 16, with anothe runique bias for each</li>
<li>The next will reduce teh 16 layers to 10 (output layer activations) with a unique bias for each</li>
<li>Between each layer we will use the ReLu for nonlinearity.</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Visualizing V2 Of Our NN
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Can you picture the matrix multiplication? It’s important to get it straight! What would the equivalent of <a href="#fig-Layer1">Figure&nbsp;2</a> look like for our NN V2?</p>
</div>
</div>
</div>
<p>Defining parameters:</p>
<div class="cell" data-execution_count="34">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a><span class="co"># Initialize parameters using the functions defined above to make it easy to generate variously sized tensors</span></span>
<span id="cb53-2"><a href="#cb53-2"></a>W1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb53-3"><a href="#cb53-3"></a>B1 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb53-4"><a href="#cb53-4"></a>W2 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb53-5"><a href="#cb53-5"></a>B2 <span class="op">=</span> init_params(<span class="dv">16</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb53-6"><a href="#cb53-6"></a>W3 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb53-7"><a href="#cb53-7"></a>B3 <span class="op">=</span> init_params(<span class="dv">10</span>)  <span class="co"># Feature-specific biases added</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Define the model:</p>
<div class="cell" data-execution_count="35">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="kw">def</span> mdlV2(xb):</span>
<span id="cb54-2"><a href="#cb54-2"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1<span class="op">+</span>B1</span>
<span id="cb54-3"><a href="#cb54-3"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb54-4"><a href="#cb54-4"></a>    res <span class="op">=</span> res<span class="op">@</span>W2<span class="op">+</span>B2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb54-5"><a href="#cb54-5"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb54-6"><a href="#cb54-6"></a>    res <span class="op">=</span> res<span class="op">@</span>W3<span class="op">+</span>B3  <span class="co"># returns 10 features for each input</span></span>
<span id="cb54-7"><a href="#cb54-7"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Test it with a mini sample to ensure the Tensors were defined in the correct size to achieve our purpose:</p>
<div class="cell" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>mdlV2(mini_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="112">
<pre><code>tensor([[-0.2428,  0.5232, -0.7904, -0.7885, -0.3272, -0.0872, -0.2902,  0.4390, -0.2662,  1.2232],
        [-0.5748,  0.5424, -1.3543, -0.7292, -0.3614,  0.0895, -0.2838,  0.4246, -0.1013,  1.0879],
        [-0.5503,  0.4186, -1.0044, -0.9231,  0.2918,  0.0088, -0.4464,  0.1199,  0.0782,  0.6630],
        [-0.3652,  0.5230, -0.9236, -1.1226, -0.3005,  0.1551, -0.5508,  0.6067, -0.5363,  1.1322],
        [-0.5096,  0.5700, -1.0935, -0.7603, -0.0838,  0.0959, -0.1930,  0.2362, -0.0920,  1.0207]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Run it!</p>
<div id="fig-V2" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<div class="cell page-columns page-full" data-execution_count="37">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV2, opt<span class="op">=</span>BasicOptim(</span>
<span id="cb57-2"><a href="#cb57-2"></a>    [W1, B1, W2, B2, W3, B3], <span class="fl">0.01</span>), params<span class="op">=</span>[W1, B1, W2, B2, W3, B3])</span>
<span id="cb57-3"><a href="#cb57-3"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb57-4"><a href="#cb57-4"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb57-5"><a href="#cb57-5"></a>plt.plot(my_try.valids)</span>
<span id="cb57-6"><a href="#cb57-6"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display column-page">
<p><img src="index_files/figure-html/cell-38-output-1.png" width="575" height="414" class="figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Accuracy Increasing As Loss Reduces</figcaption><p></p>
</figure>
</div>
<p>Fantastic! Like an Olympic high jumper, we vault just over the top of the target! Time for champagne?! Not so fast! These results are those from exeucting on the training set. Remember when we partitioned off the validation set? Let’s see how the model does on that data.</p>
<p>The point of a validation set is to increase cofnidence that our model hasn’t faux-optimized by ‘playing our game’ so to speak, and learning to score well on test data while not actually generalizing across the domain of all possible inputs</p>
<div class="cell" data-execution_count="38">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="kw">def</span> confMtx(model):</span>
<span id="cb58-2"><a href="#cb58-2"></a>    conf <span class="op">=</span> torch.zeros([<span class="dv">10</span>, <span class="dv">10</span>], dtype<span class="op">=</span>torch.int32)</span>
<span id="cb58-3"><a href="#cb58-3"></a>    <span class="cf">for</span> xv, yv <span class="kw">in</span> dls[<span class="dv">1</span>]:</span>
<span id="cb58-4"><a href="#cb58-4"></a>        preds <span class="op">=</span> model(xv).<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb58-5"><a href="#cb58-5"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb58-6"><a href="#cb58-6"></a>            conf[yv[i].item()][preds[i].item()] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb58-7"><a href="#cb58-7"></a>    df <span class="op">=</span> pd.DataFrame(conf)</span>
<span id="cb58-8"><a href="#cb58-8"></a>    df.style.set_properties(<span class="op">**</span>{<span class="st">'font-size'</span>: <span class="st">'6pt'</span>}</span>
<span id="cb58-9"><a href="#cb58-9"></a>                            ).background_gradient(<span class="st">'Greys'</span>)</span>
<span id="cb58-10"><a href="#cb58-10"></a>    df.style.set_table_styles([<span class="bu">dict</span>(selector<span class="op">=</span><span class="st">'th'</span>, props<span class="op">=</span>[(</span>
<span id="cb58-11"><a href="#cb58-11"></a>        <span class="st">'text-align'</span>, <span class="st">'center'</span>)])]).set_properties(<span class="op">**</span>{<span class="st">'text-align'</span>: <span class="st">'center'</span>})</span>
<span id="cb58-12"><a href="#cb58-12"></a>    df.style.set_caption(<span class="st">"Top Axis: Predicted value. Left Axis: Actual Value"</span>)</span>
<span id="cb58-13"><a href="#cb58-13"></a>    <span class="cf">return</span> df</span>
<span id="cb58-14"><a href="#cb58-14"></a></span>
<span id="cb58-15"><a href="#cb58-15"></a>    <span class="co"># return df</span></span>
<span id="cb58-16"><a href="#cb58-16"></a>confMtx(mdlV2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="114">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>14</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>17</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>16</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>6</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>


<!-- -->

</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If it ain’t ‘py’, it ain’t python, right?<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Increasing the size of the bias vector so as to have a unique bias for each output neuron will be an opportunity to increase the capacity of the model.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>An epoch is a full pass through the entire data set, in a context where we will make multiple passes (epochs) so as to keep learning.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The learning rate is the factor by which we will multiply the gradient when adding the negative gradient to each parameter so as to update it for the next learning iteration.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The changes to parameters can be so small that the actual outcome on any single run doesn’t change across a few inputs. But with enough learning iterations we will see the desired outcome.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>(Further reading)[https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/] on capacity.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb59" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "A Simple Digit Classifier"</span></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "David De Sa"</span></span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-03-14"</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [python, pyTorch, NeuralNetworks]</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "NN101_thumbnail.png"</span></span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> true</span></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## TL;DR</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>The best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! Part 2 of 2. I built this in following along with the awesome lessons over at <span class="co">[</span><span class="ot">fast.ai</span><span class="co">](course.fast.ai)</span>. Many thanks to that team.</span>
<span id="cb59-17"><a href="#cb59-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-18"><a href="#cb59-18" aria-hidden="true" tabindex="-1"></a><span class="fu"># Overview</span></span>
<span id="cb59-19"><a href="#cb59-19" aria-hidden="true" tabindex="-1"></a>We're going to jump in where we left off in Part 1: We'll just reimport our data, and get into it.</span>
<span id="cb59-20"><a href="#cb59-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-21"><a href="#cb59-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### What?</span></span>
<span id="cb59-22"><a href="#cb59-22" aria-hidden="true" tabindex="-1"></a>We're going to create a neural network that, given a picture of a numeric digit, identifies the number.</span>
<span id="cb59-23"><a href="#cb59-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-24"><a href="#cb59-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why?</span></span>
<span id="cb59-25"><a href="#cb59-25" aria-hidden="true" tabindex="-1"></a>Does this really need explaining? Because the technology is amazing in both practical and philosophical ways.</span>
<span id="cb59-26"><a href="#cb59-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-27"><a href="#cb59-27" aria-hidden="true" tabindex="-1"></a><span class="fu">### Who?</span></span>
<span id="cb59-28"><a href="#cb59-28" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Who am I</span><span class="co">](https://davidd003.github.io/Coding_Blog/about.html)</span>!? Who are you?!</span>
<span id="cb59-29"><a href="#cb59-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-30"><a href="#cb59-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### How?</span></span>
<span id="cb59-31"><a href="#cb59-31" aria-hidden="true" tabindex="-1"></a>Using <span class="co">[</span><span class="ot">PyTorch</span><span class="co">](https://pytorch.org/)</span>^<span class="co">[</span><span class="ot">If it ain't 'py', it ain't python, right?</span><span class="co">]</span>, an opensource toolkit for building neural networks. Truly an elevator ride up onto the shoulders of giants.</span>
<span id="cb59-32"><a href="#cb59-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-33"><a href="#cb59-33" aria-hidden="true" tabindex="-1"></a><span class="fu"># Code Review</span></span>
<span id="cb59-34"><a href="#cb59-34" aria-hidden="true" tabindex="-1"></a>We'll first just re-import our data, and then get into building the groundwork for our neural network</span>
<span id="cb59-35"><a href="#cb59-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-36"><a href="#cb59-36" aria-hidden="true" tabindex="-1"></a>Let's get into it!</span>
<span id="cb59-37"><a href="#cb59-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-38"><a href="#cb59-38" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb59-39"><a href="#cb59-39" aria-hidden="true" tabindex="-1"></a>The required dependencies!:<span class="in">`scikit-learn`</span>, <span class="in">`fastbook`</span>, <span class="in">`matplotlib`</span></span>
<span id="cb59-40"><a href="#cb59-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-41"><a href="#cb59-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-44"><a href="#cb59-44" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-45"><a href="#cb59-45" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-46"><a href="#cb59-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependency</span></span>
<span id="cb59-47"><a href="#cb59-47" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb59-48"><a href="#cb59-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb59-49"><a href="#cb59-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb59-50"><a href="#cb59-50" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb59-51"><a href="#cb59-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-52"><a href="#cb59-52" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span>
<span id="cb59-53"><a href="#cb59-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-54"><a href="#cb59-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-55"><a href="#cb59-55" aria-hidden="true" tabindex="-1"></a>matplotlib.rc(<span class="st">"image"</span>, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb59-56"><a href="#cb59-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-57"><a href="#cb59-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Acquisition</span></span>
<span id="cb59-58"><a href="#cb59-58" aria-hidden="true" tabindex="-1"></a>A quick flashback to Part 1 of my Building A Digit Classifier blog!:</span>
<span id="cb59-59"><a href="#cb59-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-60"><a href="#cb59-60" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb59-61"><a href="#cb59-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understand Your Input!</span></span>
<span id="cb59-62"><a href="#cb59-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-63"><a href="#cb59-63" aria-hidden="true" tabindex="-1"></a>Pre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the <span class="co">[</span><span class="ot">source page</span><span class="co">](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)</span>: </span>
<span id="cb59-64"><a href="#cb59-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-65"><a href="#cb59-65" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.</span></span>
<span id="cb59-66"><a href="#cb59-66" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb59-67"><a href="#cb59-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-70"><a href="#cb59-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-71"><a href="#cb59-71" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-72"><a href="#cb59-72" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> load_digits()</span>
<span id="cb59-73"><a href="#cb59-73" aria-hidden="true" tabindex="-1"></a>stacked <span class="op">=</span> []</span>
<span id="cb59-74"><a href="#cb59-74" aria-hidden="true" tabindex="-1"></a><span class="co"># This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects</span></span>
<span id="cb59-75"><a href="#cb59-75" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb59-76"><a href="#cb59-76" aria-hidden="true" tabindex="-1"></a>    stacked.append([])</span>
<span id="cb59-77"><a href="#cb59-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign all images to the right collection in the 'stacked' list, indexed by target</span></span>
<span id="cb59-78"><a href="#cb59-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mnist[<span class="st">"target"</span>])):</span>
<span id="cb59-79"><a href="#cb59-79" aria-hidden="true" tabindex="-1"></a>    stacked[mnist[<span class="st">"target"</span>][i]].append(mnist[<span class="st">"images"</span>][i])</span>
<span id="cb59-80"><a href="#cb59-80" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-81"><a href="#cb59-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-82"><a href="#cb59-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Segmentation</span></span>
<span id="cb59-83"><a href="#cb59-83" aria-hidden="true" tabindex="-1"></a>The next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn't generate a validation set with a large imbalance in the number of classes to be tested in it. </span>
<span id="cb59-84"><a href="#cb59-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-85"><a href="#cb59-85" aria-hidden="true" tabindex="-1"></a>First we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.</span>
<span id="cb59-86"><a href="#cb59-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-87"><a href="#cb59-87" aria-hidden="true" tabindex="-1"></a>We'll print out the size of these collections and take a peek at a sample to make sure we indexed right.</span>
<span id="cb59-88"><a href="#cb59-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-89"><a href="#cb59-89" aria-hidden="true" tabindex="-1"></a>:::{#fig-checkSamp}</span>
<span id="cb59-92"><a href="#cb59-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-93"><a href="#cb59-93" aria-hidden="true" tabindex="-1"></a><span class="co"># | warning: False</span></span>
<span id="cb59-94"><a href="#cb59-94" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-95"><a href="#cb59-95" aria-hidden="true" tabindex="-1"></a><span class="co"># To make dataset a tensor, make it same number of dimensions</span></span>
<span id="cb59-96"><a href="#cb59-96" aria-hidden="true" tabindex="-1"></a>stacked <span class="op">=</span> tensor([x[:<span class="dv">174</span>] <span class="cf">for</span> x <span class="kw">in</span> stacked])</span>
<span id="cb59-97"><a href="#cb59-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Segmentation: Pull 20 of each digit out of training set</span></span>
<span id="cb59-98"><a href="#cb59-98" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> [dig[<span class="op">-</span><span class="dv">20</span>:] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb59-99"><a href="#cb59-99" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> [dig[:<span class="op">-</span><span class="dv">20</span>] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb59-100"><a href="#cb59-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Confirm counts of samples</span></span>
<span id="cb59-101"><a href="#cb59-101" aria-hidden="true" tabindex="-1"></a>[<span class="bu">len</span>(test[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)], [<span class="bu">len</span>(train[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb59-102"><a href="#cb59-102" aria-hidden="true" tabindex="-1"></a>show_image(stacked[<span class="dv">3</span>][<span class="dv">0</span>])  <span class="co"># Check sample</span></span>
<span id="cb59-103"><a href="#cb59-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-104"><a href="#cb59-104" aria-hidden="true" tabindex="-1"></a>Is it a 3?</span>
<span id="cb59-105"><a href="#cb59-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-106"><a href="#cb59-106" aria-hidden="true" tabindex="-1"></a>Nice.</span>
<span id="cb59-107"><a href="#cb59-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-108"><a href="#cb59-108" aria-hidden="true" tabindex="-1"></a>It's important to keep track of what's what.</span>
<span id="cb59-111"><a href="#cb59-111" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-112"><a href="#cb59-112" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-113"><a href="#cb59-113" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-114"><a href="#cb59-114" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(train), <span class="bu">type</span>(train[<span class="dv">0</span>]), <span class="bu">type</span>(train[<span class="dv">0</span>][<span class="dv">0</span>]), [</span>
<span id="cb59-115"><a href="#cb59-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">type</span>(test), <span class="bu">type</span>(test[<span class="dv">0</span>]), <span class="bu">type</span>(test[<span class="dv">0</span>][<span class="dv">0</span>])]</span>
<span id="cb59-116"><a href="#cb59-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-117"><a href="#cb59-117" aria-hidden="true" tabindex="-1"></a>Ok so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.</span>
<span id="cb59-118"><a href="#cb59-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-119"><a href="#cb59-119" aria-hidden="true" tabindex="-1"></a>Instead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it's an easy conversion:</span>
<span id="cb59-122"><a href="#cb59-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-123"><a href="#cb59-123" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-124"><a href="#cb59-124" aria-hidden="true" tabindex="-1"></a><span class="co"># Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)</span></span>
<span id="cb59-125"><a href="#cb59-125" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> torch.stack(train)</span>
<span id="cb59-126"><a href="#cb59-126" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> torch.stack(test)</span>
<span id="cb59-127"><a href="#cb59-127" aria-hidden="true" tabindex="-1"></a>train.shape, test.shape</span>
<span id="cb59-128"><a href="#cb59-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-129"><a href="#cb59-129" aria-hidden="true" tabindex="-1"></a>Now here is a critical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size. </span>
<span id="cb59-130"><a href="#cb59-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-131"><a href="#cb59-131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup DataLoaders</span></span>
<span id="cb59-132"><a href="#cb59-132" aria-hidden="true" tabindex="-1"></a>First get data into the requisite shape for the processes that will follow. </span>
<span id="cb59-135"><a href="#cb59-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-136"><a href="#cb59-136" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-137"><a href="#cb59-137" aria-hidden="true" tabindex="-1"></a><span class="co"># Build training/valid sets for passing to DataLoader</span></span>
<span id="cb59-138"><a href="#cb59-138" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> train]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb59-139"><a href="#cb59-139" aria-hidden="true" tabindex="-1"></a>test_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> test]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb59-140"><a href="#cb59-140" aria-hidden="true" tabindex="-1"></a>train_y, test_y <span class="op">=</span> [], []</span>
<span id="cb59-141"><a href="#cb59-141" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb59-142"><a href="#cb59-142" aria-hidden="true" tabindex="-1"></a>    train_y.extend([n]<span class="op">*</span><span class="dv">154</span>)</span>
<span id="cb59-143"><a href="#cb59-143" aria-hidden="true" tabindex="-1"></a>    test_y.extend([n]<span class="op">*</span><span class="dv">20</span>)</span>
<span id="cb59-144"><a href="#cb59-144" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> tensor(train_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb59-145"><a href="#cb59-145" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> tensor(test_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb59-146"><a href="#cb59-146" aria-hidden="true" tabindex="-1"></a>train_x.shape, test_x.shape, train_y.shape, test_y.shape</span>
<span id="cb59-147"><a href="#cb59-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-148"><a href="#cb59-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-149"><a href="#cb59-149" aria-hidden="true" tabindex="-1"></a>Note the unsqueezing such that the <span class="in">`_y`</span> tensors have a singleton dimension. Might seem strange on first blush but makes sense when considering that our 'target' just happens to be individual integers. But in other use cases, the target could be something more complex, requiring a larger data structure. In which case that dimension could be larger than a singleton.</span>
<span id="cb59-150"><a href="#cb59-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-151"><a href="#cb59-151" aria-hidden="true" tabindex="-1"></a>Now we can prime our DataLoaders. DataLoaders are a super handy object from the Pytorch library that will make life easier by holding our data for us, and serving it up in randomized batches.</span>
<span id="cb59-152"><a href="#cb59-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-155"><a href="#cb59-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-156"><a href="#cb59-156" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-157"><a href="#cb59-157" aria-hidden="true" tabindex="-1"></a><span class="co"># Zip each input data item to its target output</span></span>
<span id="cb59-158"><a href="#cb59-158" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x, train_y))</span>
<span id="cb59-159"><a href="#cb59-159" aria-hidden="true" tabindex="-1"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(test_x, test_y))</span>
<span id="cb59-160"><a href="#cb59-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Define DataLoader objects to pass to learner</span></span>
<span id="cb59-161"><a href="#cb59-161" aria-hidden="true" tabindex="-1"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">25</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb59-162"><a href="#cb59-162" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb59-163"><a href="#cb59-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-164"><a href="#cb59-164" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(dl, valid_dl)</span>
<span id="cb59-165"><a href="#cb59-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-166"><a href="#cb59-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-167"><a href="#cb59-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Laying The NN Foundations</span></span>
<span id="cb59-168"><a href="#cb59-168" aria-hidden="true" tabindex="-1"></a>Every NN needs weights and biases. We're going to want to initialize lots of them, and in differently shaped structures, so first we define a function to help us do that:</span>
<span id="cb59-169"><a href="#cb59-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-172"><a href="#cb59-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-173"><a href="#cb59-173" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-174"><a href="#cb59-174" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">.1</span>): <span class="cf">return</span> (torch.randn(size)<span class="op">*</span>std).requires_grad_()</span>
<span id="cb59-175"><a href="#cb59-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-176"><a href="#cb59-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-177"><a href="#cb59-177" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-178"><a href="#cb59-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-179"><a href="#cb59-179" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb59-180"><a href="#cb59-180" aria-hidden="true" tabindex="-1"></a><span class="fu">## Requiring Grad!</span></span>
<span id="cb59-181"><a href="#cb59-181" aria-hidden="true" tabindex="-1"></a>Note that any memory structures created with that function will also store their gradient in memory as computation is performed on them. This will be critical to executing back propagation of the gradient so as to optimize our function.</span>
<span id="cb59-182"><a href="#cb59-182" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-183"><a href="#cb59-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-184"><a href="#cb59-184" aria-hidden="true" tabindex="-1"></a>Now we can generate our weights and biases. We define the structure sizes with our input and output size in mind, and the principles of matrix multiplication. </span>
<span id="cb59-185"><a href="#cb59-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-186"><a href="#cb59-186" aria-hidden="true" tabindex="-1"></a>The general structure we will start with will be, of course, an initial layer with one neuron for each input, which will yield a single activation per input. After a nonlinearty, the second layer will have 10 weights and biases, such that the output is a 1D vector of size 10, per input. Each output neuron will represent an estimation of likelihood that the integer is that which the neuron represents (0-9).</span>
<span id="cb59-187"><a href="#cb59-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-188"><a href="#cb59-188" aria-hidden="true" tabindex="-1"></a>@fig-Layer1 shows the start of the proces... A row in the input batch represents a single image. At the end of the 1^st^ layer, it is reduced to a single activation value to be passed through a non-linearity prior to going through the 2^nd^ layer</span>
<span id="cb59-189"><a href="#cb59-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-190"><a href="#cb59-190" aria-hidden="true" tabindex="-1"></a><span class="al">![Visualizing Layer 1](Layer 1 Diagram.png)</span>{#fig-Layer1}  </span>
<span id="cb59-191"><a href="#cb59-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-192"><a href="#cb59-192" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip }</span>
<span id="cb59-193"><a href="#cb59-193" aria-hidden="true" tabindex="-1"></a><span class="fu">## Matrix Multiplication!</span></span>
<span id="cb59-194"><a href="#cb59-194" aria-hidden="true" tabindex="-1"></a>Never fear! It's easy. I never forgot the trick Dr. Mohammed Jaber taught us 4th year as we had to use these methods from first year: ROW. COLUMN.</span>
<span id="cb59-195"><a href="#cb59-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-196"><a href="#cb59-196" aria-hidden="true" tabindex="-1"></a>Take row i from the first matrix, and column j from the second. The element in index i,j of the resulting matrix will be the dot product of that row and column vector.</span>
<span id="cb59-197"><a href="#cb59-197" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-198"><a href="#cb59-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-201"><a href="#cb59-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-202"><a href="#cb59-202" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-203"><a href="#cb59-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Have to provide extra dimension for matrix mutiplication, making it a single-column vector</span></span>
<span id="cb59-204"><a href="#cb59-204" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb59-205"><a href="#cb59-205" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-206"><a href="#cb59-206" aria-hidden="true" tabindex="-1"></a><span class="co"># Have to provide an extra dimension (1) for matrix multiplication</span></span>
<span id="cb59-207"><a href="#cb59-207" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb59-208"><a href="#cb59-208" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-209"><a href="#cb59-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-210"><a href="#cb59-210" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Layer 1:</span>
<span id="cb59-211"><a href="#cb59-211" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>64 weights, one per pixel. Formatted in a column vector because they are the second term in the matrix multiplication equation, with a single input (image) being fed as a single row vector of 64 elements, one per pixel. </span>
<span id="cb59-212"><a href="#cb59-212" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>1 bias; it will be broadcast such that the same bias is added to all instances in our batch. See @fig-Layer1</span>
<span id="cb59-213"><a href="#cb59-213" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Layer 2:</span>
<span id="cb59-214"><a href="#cb59-214" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>10 weights, to generate one output neuron per category/class to assign. </span>
<span id="cb59-215"><a href="#cb59-215" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>1 bias, to be *broadcast* across each input vector.^<span class="co">[</span><span class="ot">Increasing the size of the bias vector so as to have a unique bias for each output neuron will be an opportunity to increase the capacity of the model.</span><span class="co">]</span></span>
<span id="cb59-216"><a href="#cb59-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-217"><a href="#cb59-217" aria-hidden="true" tabindex="-1"></a>We will use the basic <span class="co">[</span><span class="ot">ReLU</span><span class="co">](https://deepai.org/machine-learning-glossary-and-terms/relu)</span> activation function as the non-linearity between the two linear layers.</span>
<span id="cb59-218"><a href="#cb59-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-219"><a href="#cb59-219" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building the NN</span></span>
<span id="cb59-220"><a href="#cb59-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-221"><a href="#cb59-221" aria-hidden="true" tabindex="-1"></a>Similarly to our parameter generation function, we want o make our little network into a function block so that we can treat it as a black box to plug and play with other code sections:</span>
<span id="cb59-224"><a href="#cb59-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-225"><a href="#cb59-225" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-226"><a href="#cb59-226" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-227"><a href="#cb59-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-228"><a href="#cb59-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-229"><a href="#cb59-229" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> myModel(xb):</span>
<span id="cb59-230"><a href="#cb59-230" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1<span class="op">+</span>b1</span>
<span id="cb59-231"><a href="#cb59-231" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-232"><a href="#cb59-232" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>w2<span class="op">+</span>b2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb59-233"><a href="#cb59-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb59-234"><a href="#cb59-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-235"><a href="#cb59-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-236"><a href="#cb59-236" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-237"><a href="#cb59-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-238"><a href="#cb59-238" aria-hidden="true" tabindex="-1"></a>Let's take it for a spin! We'll manually identify a subset of our training dataset for testing purposes.</span>
<span id="cb59-241"><a href="#cb59-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-242"><a href="#cb59-242" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-243"><a href="#cb59-243" aria-hidden="true" tabindex="-1"></a>mini_samp <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb59-244"><a href="#cb59-244" aria-hidden="true" tabindex="-1"></a>mini_x <span class="op">=</span> train_x[:mini_samp]</span>
<span id="cb59-245"><a href="#cb59-245" aria-hidden="true" tabindex="-1"></a>mini_y <span class="op">=</span> train_y[:mini_samp]</span>
<span id="cb59-246"><a href="#cb59-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-249"><a href="#cb59-249" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-250"><a href="#cb59-250" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-251"><a href="#cb59-251" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-252"><a href="#cb59-252" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> myModel(mini_x)</span>
<span id="cb59-253"><a href="#cb59-253" aria-hidden="true" tabindex="-1"></a>res, res.shape</span>
<span id="cb59-254"><a href="#cb59-254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-255"><a href="#cb59-255" aria-hidden="true" tabindex="-1"></a>Excellent! For the 5 input images passed in, we have an output that is 5 vectors of size 10, that is, each input has 10 output values, one for each possible classification outcome 0 through 9. </span>
<span id="cb59-256"><a href="#cb59-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-257"><a href="#cb59-257" aria-hidden="true" tabindex="-1"></a>And note that the result tensor retains the gradient! This comes into play in the next step.</span>
<span id="cb59-258"><a href="#cb59-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-259"><a href="#cb59-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Calculating Loss</span></span>
<span id="cb59-260"><a href="#cb59-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-261"><a href="#cb59-261" aria-hidden="true" tabindex="-1"></a>Loss is the measure of the neural networks success. The most critical characteristic of a loss function is that it be differentiable with a smooth gradient. But just because the math doesn't break doesn't mean the function will eb any good. There are a variety of well understood functions that can be used, with selection being a matter of preference as well as the use case at hand.</span>
<span id="cb59-262"><a href="#cb59-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-263"><a href="#cb59-263" aria-hidden="true" tabindex="-1"></a>Since we are assigning a single class to the input, from multiple options, we'll use the SoftMax function. It reduces all outputs to a number between 0 and 1, with a given number being exponentially closer to one, the larger it is compared to the other numbers in the set. </span>
<span id="cb59-264"><a href="#cb59-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-265"><a href="#cb59-265" aria-hidden="true" tabindex="-1"></a>This is a double-hitter: it allows the net to learn to win by giving the correct digit's output neuron the highest activation, while also motivating the reduction in activation of the incorrect neurons. </span>
<span id="cb59-266"><a href="#cb59-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-269"><a href="#cb59-269" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-270"><a href="#cb59-270" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-271"><a href="#cb59-271" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-272"><a href="#cb59-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-273"><a href="#cb59-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-274"><a href="#cb59-274" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_loss(preds, target, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb59-275"><a href="#cb59-275" aria-hidden="true" tabindex="-1"></a>    maxes <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb59-276"><a href="#cb59-276" aria-hidden="true" tabindex="-1"></a>    correctIndices <span class="op">=</span> [r[t] <span class="cf">for</span> r, t <span class="kw">in</span> <span class="bu">zip</span>(maxes, target)]</span>
<span id="cb59-277"><a href="#cb59-277" aria-hidden="true" tabindex="-1"></a>    resy <span class="op">=</span> [<span class="op">-</span>torch.log(tens) <span class="cf">for</span> tens <span class="kw">in</span> correctIndices]</span>
<span id="cb59-278"><a href="#cb59-278" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(resy)</span>
<span id="cb59-279"><a href="#cb59-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-280"><a href="#cb59-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-281"><a href="#cb59-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-282"><a href="#cb59-282" aria-hidden="true" tabindex="-1"></a>Keep in mind that we need to ensure these functions can work with different batch sizes. Fortunately, the PyTorch built in functions are designed for this. </span>
<span id="cb59-283"><a href="#cb59-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-284"><a href="#cb59-284" aria-hidden="true" tabindex="-1"></a>Our function first takes the SoftMax of the outputs for each input. Then, it pulls out the resulting value for the index of the correct class. Recall, this will be a number between 0 and 1. If we took this number alone as our loss, measure, there would be a couple of issues.</span>
<span id="cb59-285"><a href="#cb59-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-286"><a href="#cb59-286" aria-hidden="true" tabindex="-1"></a>First of all, convention is that we want to *reduce* loss in the course of our optimization. But we have defined the index of the *highest* softmax result as our classification integer. So that doesn't jive. Secondly, it isn't desirable to have the measure of loss constrained to such a small range 0 to 1, because this essentially reduces the information we're getting - large differences in inputs will show very small changes in loss. For effective learning, we would want a strong learning signal, i.e. small changes to input parameters reflecting meaningful differences in the measure of loss.</span>
<span id="cb59-287"><a href="#cb59-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-288"><a href="#cb59-288" aria-hidden="true" tabindex="-1"></a>The solution? First, taking the log of the SoftMax activation. As the following code shows, a difference of 0.001 in the input to the log function creates a differences of more than 0.01 in the output, an order of magnitude difference.</span>
<span id="cb59-289"><a href="#cb59-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-292"><a href="#cb59-292" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-293"><a href="#cb59-293" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-294"><a href="#cb59-294" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-295"><a href="#cb59-295" aria-hidden="true" tabindex="-1"></a>torch.log(Tensor([<span class="fl">.5</span>, <span class="fl">.499</span>]))</span>
<span id="cb59-296"><a href="#cb59-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-297"><a href="#cb59-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-298"><a href="#cb59-298" aria-hidden="true" tabindex="-1"></a>After the log, we took the negative of those values. That's because the log of a number between 0 and 1 will always be negative, getting absolutely larger the further from 1 it is. Seeing as we want the output to be absolutely smaller the closer to 1 it is, taking the negative accomplishes this perfectly.</span>
<span id="cb59-299"><a href="#cb59-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-300"><a href="#cb59-300" aria-hidden="true" tabindex="-1"></a>Lastly, we summed the loss measure across the samples in the mini batch since we want to optimize globally, not just for individual samples.</span>
<span id="cb59-301"><a href="#cb59-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-302"><a href="#cb59-302" aria-hidden="true" tabindex="-1"></a>So let's test it!</span>
<span id="cb59-303"><a href="#cb59-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-306"><a href="#cb59-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-307"><a href="#cb59-307" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-308"><a href="#cb59-308" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing the loss function works(assuming mini_x of 3 samples)</span></span>
<span id="cb59-309"><a href="#cb59-309" aria-hidden="true" tabindex="-1"></a>lossResults <span class="op">=</span> my_loss(myModel(mini_x), mini_y)</span>
<span id="cb59-310"><a href="#cb59-310" aria-hidden="true" tabindex="-1"></a>lossResults</span>
<span id="cb59-311"><a href="#cb59-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-312"><a href="#cb59-312" aria-hidden="true" tabindex="-1"></a>This output might look nice, but it's a roll of the dice!</span>
<span id="cb59-313"><a href="#cb59-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-314"><a href="#cb59-314" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb59-315"><a href="#cb59-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Beware Of Implementing Your Own Math</span></span>
<span id="cb59-316"><a href="#cb59-316" aria-hidden="true" tabindex="-1"></a>While the formula for the SoftMax is easy enough to understand, Our home-brewed version won't work. I tried to do so at first and had all kinds of headaches arising from arithmetic underflow in the outcomes! How the hell do you deal with that? The best solution: Use the function implementation baked into the PyTorch libraries! Let the pro's handle those *deep* issues. We're still riding on training wheels!</span>
<span id="cb59-317"><a href="#cb59-317" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-318"><a href="#cb59-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-319"><a href="#cb59-319" aria-hidden="true" tabindex="-1"></a>It turns out the <span class="in">`-log`</span> of the SoftMax is the Cross Entropy Loss Function. And just importing it from the PyTorch library will save us from the pitfall described in the preceding blurb... So here is our final loss function!</span>
<span id="cb59-322"><a href="#cb59-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-323"><a href="#cb59-323" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-324"><a href="#cb59-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-325"><a href="#cb59-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-326"><a href="#cb59-326" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_loss(preds, target):</span>
<span id="cb59-327"><a href="#cb59-327" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb59-328"><a href="#cb59-328" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn tensor of singleton tensors one per target into a single tensor with all as elements</span></span>
<span id="cb59-329"><a href="#cb59-329" aria-hidden="true" tabindex="-1"></a>    tgts <span class="op">=</span> target.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb59-330"><a href="#cb59-330" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_fn(preds, tgts)  <span class="co"># Nice</span></span>
<span id="cb59-331"><a href="#cb59-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-332"><a href="#cb59-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-333"><a href="#cb59-333" aria-hidden="true" tabindex="-1"></a>Now if you're reading this you're probably thinking that *gradient descent* is the next step here, so as to reduce the loss between minibatches, but we'll take a quick detour first.</span>
<span id="cb59-334"><a href="#cb59-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-335"><a href="#cb59-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Measuring Accuracy</span></span>
<span id="cb59-336"><a href="#cb59-336" aria-hidden="true" tabindex="-1"></a>Before going further, let's take a minute to set up some functions we'll need in the training process. The first yields the percentage accuracy of the model across a batch, and the second function builds on that to iterate across the batchers in our valdiation set and yield the accuracy rate across a whole epoch^<span class="co">[</span><span class="ot">An epoch is a full pass through the entire data set, in a context where we will make multiple passes (epochs) so as to keep learning.</span><span class="co">]</span>.</span>
<span id="cb59-339"><a href="#cb59-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-340"><a href="#cb59-340" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-341"><a href="#cb59-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-342"><a href="#cb59-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-343"><a href="#cb59-343" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_accuracy(mdl, xb, yb):</span>
<span id="cb59-344"><a href="#cb59-344" aria-hidden="true" tabindex="-1"></a>    otpts <span class="op">=</span> mdl(xb)  <span class="co"># Get output activations from model</span></span>
<span id="cb59-345"><a href="#cb59-345" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The indices of the max activation is the predicted digit of the input</span></span>
<span id="cb59-346"><a href="#cb59-346" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> otpts.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb59-347"><a href="#cb59-347" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Types must be tensors to return sequence of true/false</span></span>
<span id="cb59-348"><a href="#cb59-348" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> preds <span class="op">==</span> yb.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb59-349"><a href="#cb59-349" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                    Use view to take it from shape=[5,1] to [5], same as preds. else will broadcast and end result all messed up</span></span>
<span id="cb59-350"><a href="#cb59-350" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span>
<span id="cb59-351"><a href="#cb59-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-352"><a href="#cb59-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-353"><a href="#cb59-353" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate_epoch(mdl):</span>
<span id="cb59-354"><a href="#cb59-354" aria-hidden="true" tabindex="-1"></a>    outcomes <span class="op">=</span> [batch_accuracy(mdl, xb, yb) <span class="cf">for</span> xb, yb <span class="kw">in</span> valid_dl]</span>
<span id="cb59-355"><a href="#cb59-355" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(outcomes).mean().item(), <span class="dv">4</span>)</span>
<span id="cb59-356"><a href="#cb59-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-357"><a href="#cb59-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-358"><a href="#cb59-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-359"><a href="#cb59-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-360"><a href="#cb59-360" aria-hidden="true" tabindex="-1"></a>It's always good to double check these functions are working as intended after making them... Let's grab some validation set data, double checking along the way that the outputs (integer assignment) match the input (drawings of digit):</span>
<span id="cb59-363"><a href="#cb59-363" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-364"><a href="#cb59-364" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-365"><a href="#cb59-365" aria-hidden="true" tabindex="-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb59-366"><a href="#cb59-366" aria-hidden="true" tabindex="-1"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb59-367"><a href="#cb59-367" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb59-368"><a href="#cb59-368" aria-hidden="true" tabindex="-1"></a>    show_image(xv[i].view((<span class="dv">8</span>, <span class="dv">8</span>)))</span>
<span id="cb59-369"><a href="#cb59-369" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(yv[i].data.item())</span>
<span id="cb59-370"><a href="#cb59-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-371"><a href="#cb59-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-372"><a href="#cb59-372" aria-hidden="true" tabindex="-1"></a>Ok nice, now lets see that our validation function is yielding the correct accuracy score on classifying a mini batch. Since we haven't trained the model yet we'll expect the performacne to be junk but that doesn't mean we can't test the % accuracy function.</span>
<span id="cb59-375"><a href="#cb59-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-376"><a href="#cb59-376" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-377"><a href="#cb59-377" aria-hidden="true" tabindex="-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb59-378"><a href="#cb59-378" aria-hidden="true" tabindex="-1"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb59-379"><a href="#cb59-379" aria-hidden="true" tabindex="-1"></a>o <span class="op">=</span> myModel(xv)</span>
<span id="cb59-380"><a href="#cb59-380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(o.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices, yv, batch_accuracy(myModel, xv, yv))</span>
<span id="cb59-381"><a href="#cb59-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-382"><a href="#cb59-382" aria-hidden="true" tabindex="-1"></a>We want to see the last number spit out be an accurate representation of how many numbers match between the first and second tensor output. I don't know what the final blog output will be since the inputs to the neural net are random every time, but I re ran this code cell a number of times, seeing correct assignments of one or two integers to confirm the functions were working as expected.</span>
<span id="cb59-383"><a href="#cb59-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-384"><a href="#cb59-384" aria-hidden="true" tabindex="-1"></a><span class="fu">## Execute Training</span></span>
<span id="cb59-385"><a href="#cb59-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-386"><a href="#cb59-386" aria-hidden="true" tabindex="-1"></a>Alright now we get to the good stuff! First we'll make an iterator to load our test dataset.</span>
<span id="cb59-389"><a href="#cb59-389" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-390"><a href="#cb59-390" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-391"><a href="#cb59-391" aria-hidden="true" tabindex="-1"></a>testLoader <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">0</span>])</span>
<span id="cb59-392"><a href="#cb59-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-393"><a href="#cb59-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-394"><a href="#cb59-394" aria-hidden="true" tabindex="-1"></a>And now we we will run our first training loop... the steps are:</span>
<span id="cb59-395"><a href="#cb59-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-396"><a href="#cb59-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define learning rate^<span class="co">[</span><span class="ot">The learning rate is the factor by which we will multiply the gradient when adding the negative gradient to each parameter so as to update it for the next learning iteration.</span><span class="co">]</span>.</span>
<span id="cb59-397"><a href="#cb59-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pull out a minibatch of inputs and target values from the test data set.</span>
<span id="cb59-398"><a href="#cb59-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the loss on this batch by passing the outputs from the model through the loss function</span>
<span id="cb59-399"><a href="#cb59-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Execute the <span class="in">`.backward()`</span> method to calculate the gradient for all parameters.</span>
<span id="cb59-400"><a href="#cb59-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The next bits will be executed with torch.no_grad(), because we don't want the math inherent to calibrating the parameters themselves to have its gradient captured.</span>
<span id="cb59-401"><a href="#cb59-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Having the gradient stored in the tensor object of each parameter, update each parameter by subtracting the gadient multiplied by the learning rate.</span>
<span id="cb59-402"><a href="#cb59-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reset the gradient to zero for the next learning iteration.</span>
<span id="cb59-403"><a href="#cb59-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-404"><a href="#cb59-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-407"><a href="#cb59-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-408"><a href="#cb59-408" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-409"><a href="#cb59-409" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-410"><a href="#cb59-410" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb59-411"><a href="#cb59-411" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb59-412"><a href="#cb59-412" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb59-413"><a href="#cb59-413" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb59-414"><a href="#cb59-414" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-415"><a href="#cb59-415" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-416"><a href="#cb59-416" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb59-417"><a href="#cb59-417" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb59-418"><a href="#cb59-418" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb59-419"><a href="#cb59-419" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb59-420"><a href="#cb59-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-421"><a href="#cb59-421" aria-hidden="true" tabindex="-1"></a>Success! Our measure of loss has reduced after re running the same batch with the adjusted N parameters. Across a large enough dataset^<span class="co">[</span><span class="ot">The changes to parameters can be so small that the actual outcome on any single run doesn't change across a few inputs. But with enough learning iterations we will see the desired outcome.</span><span class="co">]</span>, we would expect to see a reduced error rate associated with this.</span>
<span id="cb59-422"><a href="#cb59-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-423"><a href="#cb59-423" aria-hidden="true" tabindex="-1"></a>Let's try it again, this time printing out the average values of the parameters in the weights and bias vectors, to get a sense for how much they change from one iteration to the next.</span>
<span id="cb59-424"><a href="#cb59-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-427"><a href="#cb59-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-428"><a href="#cb59-428" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb59-429"><a href="#cb59-429" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-430"><a href="#cb59-430" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb59-431"><a href="#cb59-431" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb59-432"><a href="#cb59-432" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-433"><a href="#cb59-433" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-434"><a href="#cb59-434" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb59-435"><a href="#cb59-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-436"><a href="#cb59-436" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the learning (copied from last cell):</span></span>
<span id="cb59-437"><a href="#cb59-437" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb59-438"><a href="#cb59-438" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb59-439"><a href="#cb59-439" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb59-440"><a href="#cb59-440" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb59-441"><a href="#cb59-441" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-442"><a href="#cb59-442" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-443"><a href="#cb59-443" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb59-444"><a href="#cb59-444" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb59-445"><a href="#cb59-445" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb59-446"><a href="#cb59-446" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb59-447"><a href="#cb59-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-448"><a href="#cb59-448" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb59-449"><a href="#cb59-449" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb59-450"><a href="#cb59-450" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-451"><a href="#cb59-451" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb59-452"><a href="#cb59-452" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb59-453"><a href="#cb59-453" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-454"><a href="#cb59-454" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-455"><a href="#cb59-455" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb59-456"><a href="#cb59-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-457"><a href="#cb59-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-458"><a href="#cb59-458" aria-hidden="true" tabindex="-1"></a>What we see here is that our change in paramters created a reduction in the measure of loss, *even though* it wasn't even identifiable at 4 decimal points accuracy!</span>
<span id="cb59-459"><a href="#cb59-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-460"><a href="#cb59-460" aria-hidden="true" tabindex="-1"></a>So let's take it to the next level. We trained on one minibatch of data. Let's try doing a whole epoch- iterating over every minibatch in the training set.</span>
<span id="cb59-461"><a href="#cb59-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-462"><a href="#cb59-462" aria-hidden="true" tabindex="-1"></a>First, we'll reset our weights to random numbers, clearing the slate of that last learning:</span>
<span id="cb59-463"><a href="#cb59-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-466"><a href="#cb59-466" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-467"><a href="#cb59-467" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-468"><a href="#cb59-468" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb59-469"><a href="#cb59-469" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-470"><a href="#cb59-470" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb59-471"><a href="#cb59-471" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-472"><a href="#cb59-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-473"><a href="#cb59-473" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb59-474"><a href="#cb59-474" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb59-475"><a href="#cb59-475" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-476"><a href="#cb59-476" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb59-477"><a href="#cb59-477" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb59-478"><a href="#cb59-478" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb59-479"><a href="#cb59-479" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-480"><a href="#cb59-480" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb59-481"><a href="#cb59-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-482"><a href="#cb59-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-483"><a href="#cb59-483" aria-hidden="true" tabindex="-1"></a>And now execute learning.</span>
<span id="cb59-486"><a href="#cb59-486" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-487"><a href="#cb59-487" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-488"><a href="#cb59-488" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-489"><a href="#cb59-489" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb59-490"><a href="#cb59-490" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb59-491"><a href="#cb59-491" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb59-492"><a href="#cb59-492" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb59-493"><a href="#cb59-493" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-494"><a href="#cb59-494" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-495"><a href="#cb59-495" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb59-496"><a href="#cb59-496" aria-hidden="true" tabindex="-1"></a>            p.grad.zero_()</span>
<span id="cb59-497"><a href="#cb59-497" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loss: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()))  <span class="co"># Prints loss for each minibatch</span></span>
<span id="cb59-498"><a href="#cb59-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-499"><a href="#cb59-499" aria-hidden="true" tabindex="-1"></a>Hmmm.... sometimes when I run this I see the loss stay around the same value, other times it drops just a little before stabilizing... </span>
<span id="cb59-500"><a href="#cb59-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-501"><a href="#cb59-501" aria-hidden="true" tabindex="-1"></a>We are going to need a better way to look at the trending of the loss function across iterations, so we may as well make that function now:</span>
<span id="cb59-502"><a href="#cb59-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-505"><a href="#cb59-505" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-506"><a href="#cb59-506" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-507"><a href="#cb59-507" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-508"><a href="#cb59-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-509"><a href="#cb59-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-510"><a href="#cb59-510" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_grad(xb, yb, model, f_loss):</span>
<span id="cb59-511"><a href="#cb59-511" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb59-512"><a href="#cb59-512" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> f_loss(preds, yb)</span>
<span id="cb59-513"><a href="#cb59-513" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb59-514"><a href="#cb59-514" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.data.item()  <span class="co"># Return the loss (see why later)</span></span>
<span id="cb59-515"><a href="#cb59-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-516"><a href="#cb59-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-517"><a href="#cb59-517" aria-hidden="true" tabindex="-1"></a>calc_grad(mini_x, mini_y, myModel, my_loss)</span>
<span id="cb59-518"><a href="#cb59-518" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-519"><a href="#cb59-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-520"><a href="#cb59-520" aria-hidden="true" tabindex="-1"></a>This is another step towards modularizing (if that is a word) our code. Making use of that last function, our training loop now looks like this:</span>
<span id="cb59-521"><a href="#cb59-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-524"><a href="#cb59-524" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-525"><a href="#cb59-525" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-526"><a href="#cb59-526" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-527"><a href="#cb59-527" aria-hidden="true" tabindex="-1"></a><span class="co"># Initializing weights again for a fresh start</span></span>
<span id="cb59-528"><a href="#cb59-528" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb59-529"><a href="#cb59-529" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-530"><a href="#cb59-530" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb59-531"><a href="#cb59-531" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb59-532"><a href="#cb59-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-533"><a href="#cb59-533" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PreTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb59-534"><a href="#cb59-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-535"><a href="#cb59-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-536"><a href="#cb59-536" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, lr, params, f_loss):</span>
<span id="cb59-537"><a href="#cb59-537" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb59-538"><a href="#cb59-538" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb59-539"><a href="#cb59-539" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb59-540"><a href="#cb59-540" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb59-541"><a href="#cb59-541" aria-hidden="true" tabindex="-1"></a>                p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb59-542"><a href="#cb59-542" aria-hidden="true" tabindex="-1"></a>                p.grad.zero_()</span>
<span id="cb59-543"><a href="#cb59-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-544"><a href="#cb59-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-545"><a href="#cb59-545" aria-hidden="true" tabindex="-1"></a>train_epoch(myModel, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb59-546"><a href="#cb59-546" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PostTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb59-547"><a href="#cb59-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-548"><a href="#cb59-548" aria-hidden="true" tabindex="-1"></a>UH OH!</span>
<span id="cb59-549"><a href="#cb59-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-550"><a href="#cb59-550" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb59-551"><a href="#cb59-551" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overfitting</span></span>
<span id="cb59-552"><a href="#cb59-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-553"><a href="#cb59-553" aria-hidden="true" tabindex="-1"></a>Our loss is going down, but our error is erratic, staying the same, or going up! This is an indicator that we've reached a point where the mathematical strategy we're using to optimize is now playing games with us. The modifications *do* reduce loss, but that is no longer aligned with an improvement with the task. Recall that a loss function *should* always correlate with NN efficacy, but this can break down at the fringes.</span>
<span id="cb59-554"><a href="#cb59-554" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-555"><a href="#cb59-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-556"><a href="#cb59-556" aria-hidden="true" tabindex="-1"></a>Is this a problem? Yes! But one that can be solved. Overall we are still in good territory.. We know the math and programming is working from a nuts and bolts perspective. What this indicates to us is that we need to tune the *metaparameters* of our neural network function. How many layers are there? How wide are the layers? We may be in a situation where the NN doesn't have sufficient *capacity*^<span class="co">[</span><span class="ot">(Further reading)[https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/] on capacity.</span><span class="co">]</span> to get us the outcomes we're looking for.</span>
<span id="cb59-557"><a href="#cb59-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-558"><a href="#cb59-558" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building Test Infrastructure</span></span>
<span id="cb59-559"><a href="#cb59-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-560"><a href="#cb59-560" aria-hidden="true" tabindex="-1"></a>We've built an NN from first principles now, putting together the building blocks of parameter initialization, running the algebra through the net, backpropagation to compute the gradient, and parameter calibration through the gradient descent method. This all executing repeatedly across batches of data.</span>
<span id="cb59-561"><a href="#cb59-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-562"><a href="#cb59-562" aria-hidden="true" tabindex="-1"></a>Before we adjust the network structure to optimize the outcomes, let's define a yet higher encapsulation of these tools to further condense the commands needed to execute this whole process, as well as enable more execution flexibility. This is where the <span class="in">`BasicOptim`</span> class omes in:</span>
<span id="cb59-563"><a href="#cb59-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-566"><a href="#cb59-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-567"><a href="#cb59-567" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-568"><a href="#cb59-568" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-569"><a href="#cb59-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-570"><a href="#cb59-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-571"><a href="#cb59-571" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicOptim:</span>
<span id="cb59-572"><a href="#cb59-572" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr): <span class="va">self</span>.params, <span class="va">self</span>.lr <span class="op">=</span> <span class="bu">list</span>(params), lr</span>
<span id="cb59-573"><a href="#cb59-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-574"><a href="#cb59-574" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb59-575"><a href="#cb59-575" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb59-576"><a href="#cb59-576" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">-=</span> p.grad.data <span class="op">*</span> <span class="va">self</span>.lr</span>
<span id="cb59-577"><a href="#cb59-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-578"><a href="#cb59-578" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb59-579"><a href="#cb59-579" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb59-580"><a href="#cb59-580" aria-hidden="true" tabindex="-1"></a>            p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb59-581"><a href="#cb59-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-582"><a href="#cb59-582" aria-hidden="true" tabindex="-1"></a>Now, we can condense the training loop, and also neatly add the function of tracking loss across batches.</span>
<span id="cb59-583"><a href="#cb59-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-586"><a href="#cb59-586" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-587"><a href="#cb59-587" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-588"><a href="#cb59-588" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-589"><a href="#cb59-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-590"><a href="#cb59-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-591"><a href="#cb59-591" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, opt, lr, params, f_loss):</span>
<span id="cb59-592"><a href="#cb59-592" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []  <span class="co"># Will allow for recording epoch wise loss</span></span>
<span id="cb59-593"><a href="#cb59-593" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb59-594"><a href="#cb59-594" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb59-595"><a href="#cb59-595" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb59-596"><a href="#cb59-596" aria-hidden="true" tabindex="-1"></a>        losses.append(calc_grad(xb, yb, model, f_loss))</span>
<span id="cb59-597"><a href="#cb59-597" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb59-598"><a href="#cb59-598" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tensor(losses)</span>
<span id="cb59-599"><a href="#cb59-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-600"><a href="#cb59-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-601"><a href="#cb59-601" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-602"><a href="#cb59-602" aria-hidden="true" tabindex="-1"></a>And per usual, give it a test:</span>
<span id="cb59-605"><a href="#cb59-605" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-606"><a href="#cb59-606" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-607"><a href="#cb59-607" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-608"><a href="#cb59-608" aria-hidden="true" tabindex="-1"></a>my_opt <span class="op">=</span> BasicOptim([w1, b1, w2, b2], <span class="fl">0.01</span>)</span>
<span id="cb59-609"><a href="#cb59-609" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> train_epoch(myModel, my_opt, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb59-610"><a href="#cb59-610" aria-hidden="true" tabindex="-1"></a>res.mean(), res</span>
<span id="cb59-611"><a href="#cb59-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-612"><a href="#cb59-612" aria-hidden="true" tabindex="-1"></a>Great: bug free. But... still this issue of the loss bouncing around withot a clear improvement trend...</span>
<span id="cb59-613"><a href="#cb59-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-614"><a href="#cb59-614" aria-hidden="true" tabindex="-1"></a>Knowing that I want to experiment with the models metaprameters to improve it, I'll finally define yet another object to contain those we've made so far and run trials with customized parameters, and providing the means for monitoring these trials more easily.</span>
<span id="cb59-617"><a href="#cb59-617" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-618"><a href="#cb59-618" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-619"><a href="#cb59-619" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-620"><a href="#cb59-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-621"><a href="#cb59-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-622"><a href="#cb59-622" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> cTrial:</span>
<span id="cb59-623"><a href="#cb59-623" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, numE<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.01</span>, model<span class="op">=</span>myModel, opt<span class="op">=</span>my_opt, params<span class="op">=</span>[w1, b1, w2, b2], f_loss<span class="op">=</span>my_loss):</span>
<span id="cb59-624"><a href="#cb59-624" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.numE <span class="op">=</span> numE</span>
<span id="cb59-625"><a href="#cb59-625" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb59-626"><a href="#cb59-626" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb59-627"><a href="#cb59-627" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.opt <span class="op">=</span> opt</span>
<span id="cb59-628"><a href="#cb59-628" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> params</span>
<span id="cb59-629"><a href="#cb59-629" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f_loss <span class="op">=</span> f_loss</span>
<span id="cb59-630"><a href="#cb59-630" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res <span class="op">=</span> []</span>
<span id="cb59-631"><a href="#cb59-631" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> []</span>
<span id="cb59-632"><a href="#cb59-632" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> []  <span class="co"># For tracking change in weights across learning</span></span>
<span id="cb59-633"><a href="#cb59-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-634"><a href="#cb59-634" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>, numE<span class="op">=</span><span class="va">None</span>, wkLr<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb59-635"><a href="#cb59-635" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb59-636"><a href="#cb59-636" aria-hidden="true" tabindex="-1"></a>        epch_losses <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb59-637"><a href="#cb59-637" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> [[], [], [], []]  <span class="co"># 4 contents, w1,b1,w2,b2</span></span>
<span id="cb59-638"><a href="#cb59-638" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> numE <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb59-639"><a href="#cb59-639" aria-hidden="true" tabindex="-1"></a>            numE <span class="op">=</span> <span class="va">self</span>.numE</span>
<span id="cb59-640"><a href="#cb59-640" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> wkLr <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb59-641"><a href="#cb59-641" aria-hidden="true" tabindex="-1"></a>            wkLr <span class="op">=</span> <span class="va">self</span>.lr</span>
<span id="cb59-642"><a href="#cb59-642" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numE):</span>
<span id="cb59-643"><a href="#cb59-643" aria-hidden="true" tabindex="-1"></a>            <span class="co"># -- Record wts for analysis</span></span>
<span id="cb59-644"><a href="#cb59-644" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">0</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb59-645"><a href="#cb59-645" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">1</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb59-646"><a href="#cb59-646" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">2</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb59-647"><a href="#cb59-647" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">3</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb59-648"><a href="#cb59-648" aria-hidden="true" tabindex="-1"></a>            <span class="co"># --</span></span>
<span id="cb59-649"><a href="#cb59-649" aria-hidden="true" tabindex="-1"></a>            res <span class="op">=</span> train_epoch(<span class="va">self</span>.model, <span class="va">self</span>.opt, <span class="va">self</span>.lr,</span>
<span id="cb59-650"><a href="#cb59-650" aria-hidden="true" tabindex="-1"></a>                              <span class="va">self</span>.params, <span class="va">self</span>.f_loss)</span>
<span id="cb59-651"><a href="#cb59-651" aria-hidden="true" tabindex="-1"></a>            epch_losses.append(res)</span>
<span id="cb59-652"><a href="#cb59-652" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.valids.append(validate_epoch(<span class="va">self</span>.model))</span>
<span id="cb59-653"><a href="#cb59-653" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res <span class="op">=</span> torch.stack(epch_losses)</span>
<span id="cb59-654"><a href="#cb59-654" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> tensor(<span class="va">self</span>.valids)</span>
<span id="cb59-655"><a href="#cb59-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-656"><a href="#cb59-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-657"><a href="#cb59-657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-658"><a href="#cb59-658" aria-hidden="true" tabindex="-1"></a>This object allows for defining all the parameters necessary for a trial, and then running it one line of code, alternaitvely overwriting the number of Epochs, and learning rate parameters.</span>
<span id="cb59-659"><a href="#cb59-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-660"><a href="#cb59-660" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experimenting To Optimize NN</span></span>
<span id="cb59-661"><a href="#cb59-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-662"><a href="#cb59-662" aria-hidden="true" tabindex="-1"></a>Let's get into it. We make a trial object and run it:</span>
<span id="cb59-663"><a href="#cb59-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-666"><a href="#cb59-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-667"><a href="#cb59-667" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-668"><a href="#cb59-668" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-669"><a href="#cb59-669" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial()</span>
<span id="cb59-670"><a href="#cb59-670" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb59-671"><a href="#cb59-671" aria-hidden="true" tabindex="-1"></a>my_try.res.shape</span>
<span id="cb59-672"><a href="#cb59-672" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-673"><a href="#cb59-673" aria-hidden="true" tabindex="-1"></a>Our functions are logging loss for each mini batch across all epochs. So this makes sense. We had 1540 data points, with minibatches of 25. <span class="in">`25*62=1550`</span>, so we have 61 minibatches of 25 one last minibatch of 15. And 50 collections of 62, one for each epoch.</span>
<span id="cb59-674"><a href="#cb59-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-675"><a href="#cb59-675" aria-hidden="true" tabindex="-1"></a><span class="fu">### First Try</span></span>
<span id="cb59-676"><a href="#cb59-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-677"><a href="#cb59-677" aria-hidden="true" tabindex="-1"></a>So let's see the results by taking the average loss across each epoch, hopefully, it'll be dropping!</span>
<span id="cb59-680"><a href="#cb59-680" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-681"><a href="#cb59-681" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-loss1</span></span>
<span id="cb59-682"><a href="#cb59-682" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Average loss across epochs"</span></span>
<span id="cb59-683"><a href="#cb59-683" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-684"><a href="#cb59-684" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb59-685"><a href="#cb59-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-686"><a href="#cb59-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-687"><a href="#cb59-687" aria-hidden="true" tabindex="-1"></a>So loss is going down, it's working right? The real test isn't reduction to our measure loss, but to see that carry through to an increase in testing accuracy:</span>
<span id="cb59-688"><a href="#cb59-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-691"><a href="#cb59-691" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-692"><a href="#cb59-692" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-val1</span></span>
<span id="cb59-693"><a href="#cb59-693" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Prediction accuracy across epochs"</span></span>
<span id="cb59-694"><a href="#cb59-694" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-695"><a href="#cb59-695" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb59-696"><a href="#cb59-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-697"><a href="#cb59-697" aria-hidden="true" tabindex="-1"></a>Disaster strikes! We're never getting more than 20% accuracy! Well, they call it *deep learning*, so let's make our model deeper. Two layers? A pittance!</span>
<span id="cb59-698"><a href="#cb59-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-699"><a href="#cb59-699" aria-hidden="true" tabindex="-1"></a><span class="fu">### Second Try</span></span>
<span id="cb59-700"><a href="#cb59-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-701"><a href="#cb59-701" aria-hidden="true" tabindex="-1"></a>Our Model V2 will look like this:</span>
<span id="cb59-702"><a href="#cb59-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-703"><a href="#cb59-703" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>64 input activations (image pixels) to 32 neurons in the first layer, with a unique (not broadcasted) bias for each neuron</span>
<span id="cb59-704"><a href="#cb59-704" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>The next layer will reduce the 32 neurons to 16, with anothe runique bias for each</span>
<span id="cb59-705"><a href="#cb59-705" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>The next will reduce teh 16 layers to 10 (output layer activations) with a unique bias for each</span>
<span id="cb59-706"><a href="#cb59-706" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Between each layer we will use the ReLu for nonlinearity.</span>
<span id="cb59-707"><a href="#cb59-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-708"><a href="#cb59-708" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb59-709"><a href="#cb59-709" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualizing V2 Of Our NN</span></span>
<span id="cb59-710"><a href="#cb59-710" aria-hidden="true" tabindex="-1"></a>Can you picture the matrix multiplication? It's important to get it straight! What would the equivalent of @fig-Layer1 look like for our NN V2?</span>
<span id="cb59-711"><a href="#cb59-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-712"><a href="#cb59-712" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-713"><a href="#cb59-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-714"><a href="#cb59-714" aria-hidden="true" tabindex="-1"></a>Defining parameters:</span>
<span id="cb59-717"><a href="#cb59-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-718"><a href="#cb59-718" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-719"><a href="#cb59-719" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-720"><a href="#cb59-720" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize parameters using the functions defined above to make it easy to generate variously sized tensors</span></span>
<span id="cb59-721"><a href="#cb59-721" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb59-722"><a href="#cb59-722" aria-hidden="true" tabindex="-1"></a>B1 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-723"><a href="#cb59-723" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb59-724"><a href="#cb59-724" aria-hidden="true" tabindex="-1"></a>B2 <span class="op">=</span> init_params(<span class="dv">16</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-725"><a href="#cb59-725" aria-hidden="true" tabindex="-1"></a>W3 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb59-726"><a href="#cb59-726" aria-hidden="true" tabindex="-1"></a>B3 <span class="op">=</span> init_params(<span class="dv">10</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-727"><a href="#cb59-727" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-728"><a href="#cb59-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-729"><a href="#cb59-729" aria-hidden="true" tabindex="-1"></a>Define the model:</span>
<span id="cb59-732"><a href="#cb59-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-733"><a href="#cb59-733" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-734"><a href="#cb59-734" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-735"><a href="#cb59-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-736"><a href="#cb59-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-737"><a href="#cb59-737" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mdlV2(xb):</span>
<span id="cb59-738"><a href="#cb59-738" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1<span class="op">+</span>B1</span>
<span id="cb59-739"><a href="#cb59-739" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-740"><a href="#cb59-740" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W2<span class="op">+</span>B2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb59-741"><a href="#cb59-741" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-742"><a href="#cb59-742" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W3<span class="op">+</span>B3  <span class="co"># returns 10 features for each input</span></span>
<span id="cb59-743"><a href="#cb59-743" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb59-744"><a href="#cb59-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-745"><a href="#cb59-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-746"><a href="#cb59-746" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-747"><a href="#cb59-747" aria-hidden="true" tabindex="-1"></a>Test it with a mini sample to ensure the Tensors were defined in the correct size to achieve our purpose:</span>
<span id="cb59-750"><a href="#cb59-750" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-751"><a href="#cb59-751" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-752"><a href="#cb59-752" aria-hidden="true" tabindex="-1"></a>mdlV2(mini_x)</span>
<span id="cb59-753"><a href="#cb59-753" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-754"><a href="#cb59-754" aria-hidden="true" tabindex="-1"></a>Run it!</span>
<span id="cb59-755"><a href="#cb59-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-756"><a href="#cb59-756" aria-hidden="true" tabindex="-1"></a>:::{#fig-V2}</span>
<span id="cb59-759"><a href="#cb59-759" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-760"><a href="#cb59-760" aria-hidden="true" tabindex="-1"></a><span class="co"># | column: page</span></span>
<span id="cb59-761"><a href="#cb59-761" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-762"><a href="#cb59-762" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV2, opt<span class="op">=</span>BasicOptim(</span>
<span id="cb59-763"><a href="#cb59-763" aria-hidden="true" tabindex="-1"></a>    [W1, B1, W2, B2, W3, B3], <span class="fl">0.01</span>), params<span class="op">=</span>[W1, B1, W2, B2, W3, B3])</span>
<span id="cb59-764"><a href="#cb59-764" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb59-765"><a href="#cb59-765" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb59-766"><a href="#cb59-766" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb59-767"><a href="#cb59-767" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb59-768"><a href="#cb59-768" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-769"><a href="#cb59-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-770"><a href="#cb59-770" aria-hidden="true" tabindex="-1"></a>Accuracy Increasing As Loss Reduces</span>
<span id="cb59-771"><a href="#cb59-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-772"><a href="#cb59-772" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb59-773"><a href="#cb59-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-774"><a href="#cb59-774" aria-hidden="true" tabindex="-1"></a>Fantastic! Like an Olympic high jumper, we vault just over the top of the target! Time for champagne?! Not so fast! These results are those from exeucting on the training set. Remember when we partitioned off the validation set? Let's see how the model does on that data.</span>
<span id="cb59-775"><a href="#cb59-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-776"><a href="#cb59-776" aria-hidden="true" tabindex="-1"></a>The point of a validation set is to increase cofnidence that our model hasn't faux-optimized by 'playing our game' so to speak, and learning to score well on test data while not actually generalizing across the domain of all possible inputs</span>
<span id="cb59-779"><a href="#cb59-779" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-780"><a href="#cb59-780" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-781"><a href="#cb59-781" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-782"><a href="#cb59-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-783"><a href="#cb59-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-784"><a href="#cb59-784" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confMtx(model):</span>
<span id="cb59-785"><a href="#cb59-785" aria-hidden="true" tabindex="-1"></a>    conf <span class="op">=</span> torch.zeros([<span class="dv">10</span>, <span class="dv">10</span>], dtype<span class="op">=</span>torch.int32)</span>
<span id="cb59-786"><a href="#cb59-786" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xv, yv <span class="kw">in</span> dls[<span class="dv">1</span>]:</span>
<span id="cb59-787"><a href="#cb59-787" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model(xv).<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb59-788"><a href="#cb59-788" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb59-789"><a href="#cb59-789" aria-hidden="true" tabindex="-1"></a>            conf[yv[i].item()][preds[i].item()] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb59-790"><a href="#cb59-790" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(conf)</span>
<span id="cb59-791"><a href="#cb59-791" aria-hidden="true" tabindex="-1"></a>    df.style.set_properties(<span class="op">**</span>{<span class="st">'font-size'</span>: <span class="st">'6pt'</span>}</span>
<span id="cb59-792"><a href="#cb59-792" aria-hidden="true" tabindex="-1"></a>                            ).background_gradient(<span class="st">'Greys'</span>)</span>
<span id="cb59-793"><a href="#cb59-793" aria-hidden="true" tabindex="-1"></a>    df.style.set_table_styles([<span class="bu">dict</span>(selector<span class="op">=</span><span class="st">'th'</span>, props<span class="op">=</span>[(</span>
<span id="cb59-794"><a href="#cb59-794" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text-align'</span>, <span class="st">'center'</span>)])]).set_properties(<span class="op">**</span>{<span class="st">'text-align'</span>: <span class="st">'center'</span>})</span>
<span id="cb59-795"><a href="#cb59-795" aria-hidden="true" tabindex="-1"></a>    df.style.set_caption(<span class="st">"Top Axis: Predicted value. Left Axis: Actual Value"</span>)</span>
<span id="cb59-796"><a href="#cb59-796" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df</span>
<span id="cb59-797"><a href="#cb59-797" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-798"><a href="#cb59-798" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return df</span></span>
<span id="cb59-799"><a href="#cb59-799" aria-hidden="true" tabindex="-1"></a>confMtx(mdlV2)</span>
<span id="cb59-800"><a href="#cb59-800" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-803"><a href="#cb59-803" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-804"><a href="#cb59-804" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-805"><a href="#cb59-805" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-806"><a href="#cb59-806" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-809"><a href="#cb59-809" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-810"><a href="#cb59-810" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-811"><a href="#cb59-811" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-812"><a href="#cb59-812" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-815"><a href="#cb59-815" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-816"><a href="#cb59-816" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-817"><a href="#cb59-817" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-818"><a href="#cb59-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-821"><a href="#cb59-821" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-822"><a href="#cb59-822" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-823"><a href="#cb59-823" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-824"><a href="#cb59-824" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-827"><a href="#cb59-827" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-828"><a href="#cb59-828" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-829"><a href="#cb59-829" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-830"><a href="#cb59-830" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-833"><a href="#cb59-833" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-834"><a href="#cb59-834" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-835"><a href="#cb59-835" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-836"><a href="#cb59-836" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-839"><a href="#cb59-839" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-840"><a href="#cb59-840" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-841"><a href="#cb59-841" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-842"><a href="#cb59-842" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb59-845"><a href="#cb59-845" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb59-846"><a href="#cb59-846" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb59-847"><a href="#cb59-847" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb59-848"><a href="#cb59-848" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>