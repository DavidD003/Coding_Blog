<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="David De Sa">
<meta name="dcterms.date" content="2023-03-23">

<title>Davids Coding - A Simple Digit Classifier</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../EigenFlower.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Davids Coding</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/daviddesa03/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://youtube.com/@davidscoding"><i class="bi bi-youtube" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/DavidD003"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">A Simple Digit Classifier</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
                                <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">pyTorch</div>
                <div class="quarto-category">NeuralNetworks</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>David De Sa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 23, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#tldr" id="toc-tldr" class="nav-link active" data-scroll-target="#tldr">TL;DR</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview">Overview</a>
  <ul class="collapse">
  <li><a href="#what" id="toc-what" class="nav-link" data-scroll-target="#what">What?</a></li>
  <li><a href="#why" id="toc-why" class="nav-link" data-scroll-target="#why">Why?</a></li>
  <li><a href="#who" id="toc-who" class="nav-link" data-scroll-target="#who">Who?</a></li>
  <li><a href="#how" id="toc-how" class="nav-link" data-scroll-target="#how">How?</a></li>
  </ul></li>
  <li><a href="#code-review" id="toc-code-review" class="nav-link" data-scroll-target="#code-review">Code Review</a>
  <ul class="collapse">
  <li><a href="#data-acquisition" id="toc-data-acquisition" class="nav-link" data-scroll-target="#data-acquisition">Data Acquisition</a>
  <ul class="collapse">
  <li><a href="#segmentation" id="toc-segmentation" class="nav-link" data-scroll-target="#segmentation">Segmentation</a></li>
  <li><a href="#setup-dataloaders" id="toc-setup-dataloaders" class="nav-link" data-scroll-target="#setup-dataloaders">Setup DataLoaders</a></li>
  </ul></li>
  <li><a href="#laying-the-nn-foundations" id="toc-laying-the-nn-foundations" class="nav-link" data-scroll-target="#laying-the-nn-foundations">Laying The NN Foundations</a></li>
  <li><a href="#building-the-nn" id="toc-building-the-nn" class="nav-link" data-scroll-target="#building-the-nn">Building the NN</a></li>
  <li><a href="#calculating-loss" id="toc-calculating-loss" class="nav-link" data-scroll-target="#calculating-loss">Calculating Loss</a></li>
  <li><a href="#measuring-accuracy" id="toc-measuring-accuracy" class="nav-link" data-scroll-target="#measuring-accuracy">Measuring Accuracy</a></li>
  <li><a href="#execute-training" id="toc-execute-training" class="nav-link" data-scroll-target="#execute-training">Execute Training</a></li>
  <li><a href="#building-test-infrastructure" id="toc-building-test-infrastructure" class="nav-link" data-scroll-target="#building-test-infrastructure">Building Test Infrastructure</a></li>
  <li><a href="#experimenting-to-optimize-nn" id="toc-experimenting-to-optimize-nn" class="nav-link" data-scroll-target="#experimenting-to-optimize-nn">Experimenting To Optimize NN</a>
  <ul class="collapse">
  <li><a href="#first-try" id="toc-first-try" class="nav-link" data-scroll-target="#first-try">First Try</a></li>
  <li><a href="#second-try" id="toc-second-try" class="nav-link" data-scroll-target="#second-try">Second Try</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#outro" id="toc-outro" class="nav-link" data-scroll-target="#outro">Outro</a>
  <ul class="collapse">
  <li><a href="#getting-here" id="toc-getting-here" class="nav-link" data-scroll-target="#getting-here">Getting Here</a></li>
  <li><a href="#lessons-learned" id="toc-lessons-learned" class="nav-link" data-scroll-target="#lessons-learned">Lessons Learned</a></li>
  <li><a href="#nn-graveyard" id="toc-nn-graveyard" class="nav-link" data-scroll-target="#nn-graveyard">NN Graveyard</a>
  <ul class="collapse">
  <li><a href="#the-funnel" id="toc-the-funnel" class="nav-link" data-scroll-target="#the-funnel">The Funnel</a></li>
  <li><a href="#bad-parameter-initialization" id="toc-bad-parameter-initialization" class="nav-link" data-scroll-target="#bad-parameter-initialization">Bad Parameter Initialization</a></li>
  <li><a href="#better-parameter-initialization" id="toc-better-parameter-initialization" class="nav-link" data-scroll-target="#better-parameter-initialization">Better Parameter Initialization</a></li>
  <li><a href="#different-learning-rates" id="toc-different-learning-rates" class="nav-link" data-scroll-target="#different-learning-rates">Different Learning Rates</a></li>
  <li><a href="#the-wormhole" id="toc-the-wormhole" class="nav-link" data-scroll-target="#the-wormhole">The Wormhole</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TL;DR</h2>
<p>The best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! Part 2 of 2. I built this in following along with the awesome lessons over at <a href="course.fast.ai">fast.ai</a>. Many thanks to that team.</p>
</section>
<section id="overview" class="level1">
<h1>Overview</h1>
<p>We’re going to jump in where we left off in Part 1: We’ll just reimport our data, and get into it.</p>
<section id="what" class="level3">
<h3 class="anchored" data-anchor-id="what">What?</h3>
<p>We’re going to create a neural network that, given a picture of a numeric digit, identifies the number.</p>
</section>
<section id="why" class="level3">
<h3 class="anchored" data-anchor-id="why">Why?</h3>
<p>Does this really need explaining? Because the technology is amazing in both practical and philosophical ways.</p>
</section>
<section id="who" class="level3">
<h3 class="anchored" data-anchor-id="who">Who?</h3>
<p><a href="https://davidd003.github.io/Coding_Blog/about.html">Who am I</a>!? Who are you?!</p>
</section>
<section id="how" class="level3">
<h3 class="anchored" data-anchor-id="how">How?</h3>
<p>Using <a href="https://pytorch.org/">PyTorch</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, an opensource toolkit for building neural networks. Truly an elevator ride up onto the shoulders of giants.</p>
</section>
</section>
<section id="code-review" class="level1 page-columns page-full">
<h1>Code Review</h1>
<p>We’ll first just re-import our data, and then get into building the groundwork for our neural network</p>
<p>Let’s get into it!</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>The required dependencies!:<code>scikit-learn</code>, <code>fastbook</code>, <code>matplotlib</code></p>
</div>
</div>
</div>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Install dependency</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">import</span> fastbook</span>
<span id="cb1-6"><a href="#cb1-6"></a></span>
<span id="cb1-7"><a href="#cb1-7"></a>fastbook.setup_book()</span>
<span id="cb1-8"><a href="#cb1-8"></a></span>
<span id="cb1-9"><a href="#cb1-9"></a></span>
<span id="cb1-10"><a href="#cb1-10"></a>matplotlib.rc(<span class="st">"image"</span>, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="data-acquisition" class="level2">
<h2 class="anchored" data-anchor-id="data-acquisition">Data Acquisition</h2>
<p>A quick flashback to Part 1 of my Building A Digit Classifier blog!:</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Understand Your Input!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the <a href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits">source page</a>:</p>
<blockquote class="blockquote">
<p>We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.</p>
</blockquote>
</div>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a>mnist <span class="op">=</span> load_digits()</span>
<span id="cb2-2"><a href="#cb2-2"></a>stacked <span class="op">=</span> []</span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="co"># This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects</span></span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb2-5"><a href="#cb2-5"></a>    stacked.append([])</span>
<span id="cb2-6"><a href="#cb2-6"></a><span class="co"># Assign all images to the right collection in the 'stacked' list, indexed by target</span></span>
<span id="cb2-7"><a href="#cb2-7"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mnist[<span class="st">"target"</span>])):</span>
<span id="cb2-8"><a href="#cb2-8"></a>    stacked[mnist[<span class="st">"target"</span>][i]].append(mnist[<span class="st">"images"</span>][i])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="segmentation" class="level3">
<h3 class="anchored" data-anchor-id="segmentation">Segmentation</h3>
<p>The next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn’t generate a validation set with a large imbalance in the number of classes to be tested in it.</p>
<p>First we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.</p>
<p>We’ll print out the size of these collections and take a peek at a sample to make sure we indexed right.</p>
<div id="fig-checkSamp" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># To make dataset a tensor, make it same number of dimensions</span></span>
<span id="cb3-2"><a href="#cb3-2"></a>stacked <span class="op">=</span> tensor([x[:<span class="dv">174</span>] <span class="cf">for</span> x <span class="kw">in</span> stacked])</span>
<span id="cb3-3"><a href="#cb3-3"></a><span class="co"># Segmentation: Pull 20 of each digit out of training set</span></span>
<span id="cb3-4"><a href="#cb3-4"></a>test <span class="op">=</span> [dig[<span class="op">-</span><span class="dv">20</span>:] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb3-5"><a href="#cb3-5"></a>train <span class="op">=</span> [dig[:<span class="op">-</span><span class="dv">20</span>] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb3-6"><a href="#cb3-6"></a><span class="co"># Confirm counts of samples</span></span>
<span id="cb3-7"><a href="#cb3-7"></a>[<span class="bu">len</span>(test[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)], [<span class="bu">len</span>(train[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb3-8"><a href="#cb3-8"></a>show_image(stacked[<span class="dv">3</span>][<span class="dv">0</span>])  <span class="co"># Check sample</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="203">
<pre><code>&lt;Axes: &gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-2.png" width="93" height="93" class="figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Is it a 3?</figcaption><p></p>
</figure>
</div>
<p>Nice.</p>
<p>It’s important to keep track of what’s what.</p>
<div class="cell" data-execution_count="4">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="bu">type</span>(train), <span class="bu">type</span>(train[<span class="dv">0</span>]), <span class="bu">type</span>(train[<span class="dv">0</span>][<span class="dv">0</span>]), [</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="bu">type</span>(test), <span class="bu">type</span>(test[<span class="dv">0</span>]), <span class="bu">type</span>(test[<span class="dv">0</span>][<span class="dv">0</span>])]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="204">
<pre><code>(list, torch.Tensor, torch.Tensor, [list, torch.Tensor, torch.Tensor])</code></pre>
</div>
</div>
<p>Ok so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.</p>
<p>Instead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it’s an easy conversion:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>train <span class="op">=</span> torch.stack(train)</span>
<span id="cb7-3"><a href="#cb7-3"></a>test <span class="op">=</span> torch.stack(test)</span>
<span id="cb7-4"><a href="#cb7-4"></a>train.shape, test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="205">
<pre><code>(torch.Size([10, 154, 8, 8]), torch.Size([10, 20, 8, 8]))</code></pre>
</div>
</div>
<p>Now here is a critical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size.</p>
</section>
<section id="setup-dataloaders" class="level3">
<h3 class="anchored" data-anchor-id="setup-dataloaders">Setup DataLoaders</h3>
<p>First get data into the requisite shape for the processes that will follow.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># Build training/valid sets for passing to DataLoader</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>train_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> train]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a>test_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> test]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb9-4"><a href="#cb9-4"></a>train_y, test_y <span class="op">=</span> [], []</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb9-6"><a href="#cb9-6"></a>    train_y.extend([n]<span class="op">*</span><span class="dv">154</span>)</span>
<span id="cb9-7"><a href="#cb9-7"></a>    test_y.extend([n]<span class="op">*</span><span class="dv">20</span>)</span>
<span id="cb9-8"><a href="#cb9-8"></a>train_y <span class="op">=</span> tensor(train_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-9"><a href="#cb9-9"></a>test_y <span class="op">=</span> tensor(test_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-10"><a href="#cb9-10"></a>train_x.shape, test_x.shape, train_y.shape, test_y.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="206">
<pre><code>(torch.Size([1540, 64]),
 torch.Size([200, 64]),
 torch.Size([1540, 1]),
 torch.Size([200, 1]))</code></pre>
</div>
</div>
<p>Note the unsqueezing such that the <code>_y</code> tensors have a singleton dimension. Might seem strange on first blush but makes sense when considering that our ‘target’ just happens to be individual integers. But in other use cases, the target could be something more complex, requiring a larger data structure. In which case that dimension could be larger than a singleton.</p>
<p>Now we can prime our DataLoaders. DataLoaders are a super handy object from the Pytorch library that will make life easier by holding our data for us, and serving it up in randomized batches.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># Zip each input data item to its target output</span></span>
<span id="cb11-2"><a href="#cb11-2"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x, train_y))</span>
<span id="cb11-3"><a href="#cb11-3"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(test_x, test_y))</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># Define DataLoader objects to pass to learner</span></span>
<span id="cb11-5"><a href="#cb11-5"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">25</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb11-6"><a href="#cb11-6"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb11-7"><a href="#cb11-7"></a></span>
<span id="cb11-8"><a href="#cb11-8"></a>dls <span class="op">=</span> DataLoaders(dl, valid_dl)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="laying-the-nn-foundations" class="level2">
<h2 class="anchored" data-anchor-id="laying-the-nn-foundations">Laying The NN Foundations</h2>
<p>Every NN needs weights and biases. We’re going to want to initialize lots of them, and in differently shaped structures, so first we define a function to help us do that:</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">.1</span>): <span class="cf">return</span> (torch.randn(size)<span class="op">*</span>std).requires_grad_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Requiring Grad!
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Note that any memory structures created with that function will also store their gradient in memory as computation is performed on them. This will be critical to executing back propagation of the gradient so as to optimize our function.</p>
</div>
</div>
</div>
<p>Now we can generate our weights and biases. We define the structure sizes with our input and output size in mind, and the principles of matrix multiplication.</p>
<p>The general structure we will start with will be, of course, an initial layer with one neuron for each input, which will yield a single activation per input. After a nonlinearty, the second layer will have 10 weights and biases, such that the output is a 1D vector of size 10, per input. Each output neuron will represent an estimation of likelihood that the integer is that which the neuron represents (0-9).</p>
<p><a href="#fig-Layer1">Figure&nbsp;2</a> shows the start of the proces… A row in the input batch represents a single image. At the end of the 1<sup>st</sup> layer, it is reduced to a single activation value to be passed through a non-linearity prior to going through the 2<sup>nd</sup> layer</p>
<div id="fig-Layer1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="Layer 1 Diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Visualizing Layer 1</figcaption><p></p>
</figure>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Matrix Multiplication!
</div>
</div>
<div class="callout-body-container callout-body">
<p>Never fear! It’s easy. I never forgot the trick Dr.&nbsp;Mohammed Jaber taught us 4th year as we had to use these methods from first year: ROW. COLUMN.</p>
<p>Take row i from the first matrix, and column j from the second. The element in index i,j of the resulting matrix will be the dot product of that row and column vector.</p>
</div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># Have to provide extra dimension for matrix mutiplication, making it a single-column vector</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb13-3"><a href="#cb13-3"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="co"># Have to provide an extra dimension (1) for matrix multiplication</span></span>
<span id="cb13-5"><a href="#cb13-5"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb13-6"><a href="#cb13-6"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<ul>
<li>Layer 1:
<ul>
<li>64 weights, one per pixel. Formatted in a column vector because they are the second term in the matrix multiplication equation, with a single input (image) being fed as a single row vector of 64 elements, one per pixel.</li>
<li>1 bias; it will be broadcast such that the same bias is added to all instances in our batch. See <a href="#fig-Layer1">Figure&nbsp;2</a></li>
</ul></li>
<li>Layer 2:
<ul>
<li>10 weights, to generate one output neuron per category/class to assign.</li>
<li>1 bias, to be <em>broadcast</em> across each input vector.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
</ul></li>
</ul>
<p>We will use the basic <a href="https://deepai.org/machine-learning-glossary-and-terms/relu">ReLU</a> activation function as the non-linearity between the two linear layers.</p>
</section>
<section id="building-the-nn" class="level2">
<h2 class="anchored" data-anchor-id="building-the-nn">Building the NN</h2>
<p>Similarly to our parameter generation function, we want o make our little network into a function block so that we can treat it as a black box to plug and play with other code sections:</p>
<div class="cell" data-execution_count="10">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="kw">def</span> myModel(xb):</span>
<span id="cb14-2"><a href="#cb14-2"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1<span class="op">+</span>b1</span>
<span id="cb14-3"><a href="#cb14-3"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb14-4"><a href="#cb14-4"></a>    res <span class="op">=</span> res<span class="op">@</span>w2<span class="op">+</span>b2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb14-5"><a href="#cb14-5"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s take it for a spin! We’ll manually identify a subset of our training dataset for testing purposes.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>mini_samp <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>mini_x <span class="op">=</span> train_x[:mini_samp]</span>
<span id="cb15-3"><a href="#cb15-3"></a>mini_y <span class="op">=</span> train_y[:mini_samp]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="12">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>res <span class="op">=</span> myModel(mini_x)</span>
<span id="cb16-2"><a href="#cb16-2"></a>res, res.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="212">
<pre><code>(tensor([[-0.5597,  0.2744, -0.3157,  0.2799,  0.0328,  0.3295,  0.1971, -0.2445, -0.4771, -0.1101],
         [-0.6673,  0.3459, -0.3709,  0.3525,  0.0524,  0.4128,  0.2520, -0.2844, -0.5670, -0.1212],
         [-0.7400,  0.3942, -0.4082,  0.4016,  0.0657,  0.4691,  0.2890, -0.3114, -0.6277, -0.1287],
         [-0.4700,  0.2148, -0.2696,  0.2193,  0.0165,  0.2600,  0.1513, -0.2112, -0.4022, -0.1009],
         [-0.5622,  0.2761, -0.3170,  0.2816,  0.0333,  0.3315,  0.1984, -0.2454, -0.4792, -0.1104]], grad_fn=&lt;AddBackward0&gt;),
 torch.Size([5, 10]))</code></pre>
</div>
</div>
<p>Excellent! For the 5 input images passed in, we have an output that is 5 vectors of size 10, that is, each input has 10 output values, one for each possible classification outcome 0 through 9.</p>
<p>And note that the result tensor retains the gradient! This comes into play in the next step.</p>
</section>
<section id="calculating-loss" class="level2">
<h2 class="anchored" data-anchor-id="calculating-loss">Calculating Loss</h2>
<p>Loss is the measure of the neural networks success. The most critical characteristic of a loss function is that it be differentiable with a smooth gradient. But just because the math doesn’t break doesn’t mean the function will eb any good. There are a variety of well understood functions that can be used, with selection being a matter of preference as well as the use case at hand.</p>
<p>Since we are assigning a single class to the input, from multiple options, we’ll use the SoftMax function. It reduces all outputs to a number between 0 and 1, with a given number being exponentially closer to one, the larger it is compared to the other numbers in the set.</p>
<p>This is a double-hitter: it allows the net to learn to win by giving the correct digit’s output neuron the highest activation, while also motivating the reduction in activation of the incorrect neurons.</p>
<div class="cell" data-execution_count="13">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="kw">def</span> my_loss(preds, target, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb18-2"><a href="#cb18-2"></a>    maxes <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb18-3"><a href="#cb18-3"></a>    correctIndices <span class="op">=</span> [r[t] <span class="cf">for</span> r, t <span class="kw">in</span> <span class="bu">zip</span>(maxes, target)]</span>
<span id="cb18-4"><a href="#cb18-4"></a>    resy <span class="op">=</span> [<span class="op">-</span>torch.log(tens) <span class="cf">for</span> tens <span class="kw">in</span> correctIndices]</span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="cf">return</span> <span class="bu">sum</span>(resy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Keep in mind that we need to ensure these functions can work with different batch sizes. Fortunately, the PyTorch built in functions are designed for this.</p>
<p>Our function first takes the SoftMax of the outputs for each input. Then, it pulls out the resulting value for the index of the correct class. Recall, this will be a number between 0 and 1. If we took this number alone as our loss, measure, there would be a couple of issues.</p>
<p>First of all, convention is that we want to <em>reduce</em> loss in the course of our optimization. But we have defined the index of the <em>highest</em> softmax result as our classification integer. So that doesn’t jive. Secondly, it isn’t desirable to have the measure of loss constrained to such a small range 0 to 1, because this essentially reduces the information we’re getting - large differences in inputs will show very small changes in loss. For effective learning, we would want a strong learning signal, i.e.&nbsp;small changes to input parameters reflecting meaningful differences in the measure of loss.</p>
<p>The solution? First, taking the log of the SoftMax activation. As the following code shows, a difference of 0.001 in the input to the log function creates a differences of more than 0.01 in the output, an order of magnitude difference.</p>
<div class="cell" data-execution_count="14">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>torch.log(Tensor([<span class="fl">.5</span>, <span class="fl">.499</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="214">
<pre><code>tensor([-0.6931, -0.6951])</code></pre>
</div>
</div>
<p>After the log, we took the negative of those values. That’s because the log of a number between 0 and 1 will always be negative, getting absolutely larger the further from 1 it is. Seeing as we want the output to be absolutely smaller the closer to 1 it is, taking the negative accomplishes this perfectly.</p>
<p>Lastly, we summed the loss measure across the samples in the mini batch since we want to optimize globally, not just for individual samples.</p>
<p>So let’s test it!</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># Testing the loss function works(assuming mini_x of 3 samples)</span></span>
<span id="cb21-2"><a href="#cb21-2"></a>lossResults <span class="op">=</span> my_loss(myModel(mini_x), mini_y)</span>
<span id="cb21-3"><a href="#cb21-3"></a>lossResults</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="215">
<pre><code>tensor([14.4973], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>This output might look nice, but it’s a roll of the dice!</p>
<div class="callout-warning callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Beware Of Implementing Your Own Math
</div>
</div>
<div class="callout-body-container callout-body">
<p>While the formula for the SoftMax is easy enough to understand, Our home-brewed version won’t work. I tried to do so at first and had all kinds of headaches arising from arithmetic underflow in the outcomes! How the hell do you deal with that? The best solution: Use the function implementation baked into the PyTorch libraries! Let the pro’s handle those <em>deep</em> issues. We’re still riding on training wheels!</p>
</div>
</div>
<p>It turns out the <code>-log</code> of the SoftMax is the Cross Entropy Loss Function. And just importing it from the PyTorch library will save us from the pitfall described in the preceding blurb… So here is our final loss function!</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="kw">def</span> my_loss(preds, target):</span>
<span id="cb23-2"><a href="#cb23-2"></a>    loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb23-3"><a href="#cb23-3"></a>    <span class="co"># Turn tensor of singleton tensors one per target into a single tensor with all as elements</span></span>
<span id="cb23-4"><a href="#cb23-4"></a>    tgts <span class="op">=</span> target.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb23-5"><a href="#cb23-5"></a>    <span class="cf">return</span> loss_fn(preds, tgts)  <span class="co"># Nice</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now if you’re reading this you’re probably thinking that <em>gradient descent</em> is the next step here, so as to reduce the loss between minibatches, but we’ll take a quick detour first.</p>
</section>
<section id="measuring-accuracy" class="level2">
<h2 class="anchored" data-anchor-id="measuring-accuracy">Measuring Accuracy</h2>
<p>Before going further, let’s take a minute to set up some functions we’ll need in the training process. The first yields the percentage accuracy of the model across a batch, and the second function builds on that to iterate across the batchers in our valdiation set and yield the accuracy rate across a whole epoch<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="kw">def</span> batch_accuracy(mdl, xb, yb):</span>
<span id="cb24-2"><a href="#cb24-2"></a>    otpts <span class="op">=</span> mdl(xb)  <span class="co"># Get output activations from model</span></span>
<span id="cb24-3"><a href="#cb24-3"></a>    <span class="co"># The indices of the max activation is the predicted digit of the input</span></span>
<span id="cb24-4"><a href="#cb24-4"></a>    preds <span class="op">=</span> otpts.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb24-5"><a href="#cb24-5"></a>    <span class="co"># Types must be tensors to return sequence of true/false</span></span>
<span id="cb24-6"><a href="#cb24-6"></a>    correct <span class="op">=</span> preds <span class="op">==</span> yb.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb24-7"><a href="#cb24-7"></a>    <span class="co">#                    Use view to take it from shape=[5,1] to [5], same as preds. else will broadcast and end result all messed up</span></span>
<span id="cb24-8"><a href="#cb24-8"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span>
<span id="cb24-9"><a href="#cb24-9"></a></span>
<span id="cb24-10"><a href="#cb24-10"></a></span>
<span id="cb24-11"><a href="#cb24-11"></a><span class="kw">def</span> validate_epoch(mdl):</span>
<span id="cb24-12"><a href="#cb24-12"></a>    outcomes <span class="op">=</span> [batch_accuracy(mdl, xb, yb) <span class="cf">for</span> xb, yb <span class="kw">in</span> valid_dl]</span>
<span id="cb24-13"><a href="#cb24-13"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(outcomes).mean().item(), <span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>It’s always good to double check these functions are working as intended after making them… Let’s grab some validation set data, double checking along the way that the outputs (integer assignment) match the input (drawings of digit):</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb25-2"><a href="#cb25-2"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb25-4"><a href="#cb25-4"></a>    show_image(xv[i].view((<span class="dv">8</span>, <span class="dv">8</span>)))</span>
<span id="cb25-5"><a href="#cb25-5"></a>    <span class="bu">print</span>(yv[i].data.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>2
3
3
6
8</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-2.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-3.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-4.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-5.png" width="93" height="93"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-6.png" width="93" height="93"></p>
</div>
</div>
<p>Ok nice, now lets see that our validation function is yielding the correct accuracy score on classifying a mini batch. Since we haven’t trained the model yet we’ll expect the performacne to be junk but that doesn’t mean we can’t test the % accuracy function.</p>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb27-2"><a href="#cb27-2"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb27-3"><a href="#cb27-3"></a>o <span class="op">=</span> myModel(xv)</span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="bu">print</span>(o.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices, yv, batch_accuracy(myModel, xv, yv))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([5, 5, 5, 5, 5]) tensor([[0],
        [2],
        [5],
        [0],
        [1]]) tensor(0.2000)</code></pre>
</div>
</div>
<p>We want to see the last number spit out be an accurate representation of how many numbers match between the first and second tensor output. I don’t know what the final blog output will be since the inputs to the neural net are random every time, but I re ran this code cell a number of times, seeing correct assignments of one or two integers to confirm the functions were working as expected.</p>
</section>
<section id="execute-training" class="level2">
<h2 class="anchored" data-anchor-id="execute-training">Execute Training</h2>
<p>Alright now we get to the good stuff! First we’ll make an iterator to load our test dataset.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a>testLoader <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And now we we will run our first training loop… the steps are:</p>
<ul>
<li>Define learning rate<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Pull out a minibatch of inputs and target values from the test data set.</li>
<li>Calculate the loss on this batch by passing the outputs from the model through the loss function</li>
<li>Execute the <code>.backward()</code> method to calculate the gradient for all parameters.</li>
<li>The next bits will be executed with torch.no_grad(), because we don’t want the math inherent to calibrating the parameters themselves to have its gradient captured.</li>
<li>Having the gradient stored in the tensor object of each parameter, update each parameter by subtracting the gadient multiplied by the learning rate.</li>
<li>Reset the gradient to zero for the next learning iteration.</li>
</ul>
<div class="cell" data-execution_count="21">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb30-3"><a href="#cb30-3"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb30-4"><a href="#cb30-4"></a>loss.backward()</span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb30-6"><a href="#cb30-6"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb30-7"><a href="#cb30-7"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb30-8"><a href="#cb30-8"></a>        p.grad.zero_()</span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb30-10"><a href="#cb30-10"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loss before training: 2.3810155391693115  |   Loss after training: 2.3350515365600586</code></pre>
</div>
</div>
<p>Success! Our measure of loss has reduced after re running the same batch with the adjusted N parameters. Across a large enough dataset<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, we would expect to see a reduced error rate associated with this.</p>
<p>Let’s try it again, this time printing out the average values of the parameters in the weights and bias vectors, to get a sense for how much they change from one iteration to the next.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the learning (copied from last cell):</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Average parameters:  tensor([[ 0.0079,  0.0000],
        [-0.0091,  0.0000],
        [-0.0001,  0.0000],
        [-0.0585,  0.0000]])
Loss before training: 2.317502021789551  |   Loss after training: 2.2996890544891357
Average parameters:  tensor([[ 0.0072,  0.0000],
        [-0.0092,  0.0000],
        [-0.0001,  0.0000],
        [-0.0585,  0.0000]])</code></pre>
</div>
</div>
<p>What we see here is that our change in paramters created a reduction in the measure of loss, <em>even though</em> it wasn’t even identifiable at 4 decimal points accuracy!</p>
<p>So let’s take it to the next level. We trained on one minibatch of data. Let’s try doing a whole epoch- iterating over every minibatch in the training set.</p>
<p>First, we’ll reset our weights to random numbers, clearing the slate of that last learning:</p>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb34-2"><a href="#cb34-2"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb34-3"><a href="#cb34-3"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb34-4"><a href="#cb34-4"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb34-5"><a href="#cb34-5"></a></span>
<span id="cb34-6"><a href="#cb34-6"></a><span class="co"># Print average param values again:</span></span>
<span id="cb34-7"><a href="#cb34-7"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb34-8"><a href="#cb34-8"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb34-9"><a href="#cb34-9"></a><span class="cf">else</span>:</span>
<span id="cb34-10"><a href="#cb34-10"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb34-11"><a href="#cb34-11"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb34-12"><a href="#cb34-12"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb34-13"><a href="#cb34-13"></a>        p.grad.zero_()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([-0.0298, -0.0852, -0.0206, -0.0224])</code></pre>
</div>
</div>
<p>And now execute learning.</p>
<div class="cell" data-execution_count="24">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb36-2"><a href="#cb36-2"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb36-3"><a href="#cb36-3"></a>    loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb36-4"><a href="#cb36-4"></a>    loss.backward()</span>
<span id="cb36-5"><a href="#cb36-5"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb36-6"><a href="#cb36-6"></a>        <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb36-7"><a href="#cb36-7"></a>            p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb36-8"><a href="#cb36-8"></a>            p.grad.zero_()</span>
<span id="cb36-9"><a href="#cb36-9"></a>    <span class="bu">print</span>(<span class="st">"Loss: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()))  <span class="co"># Prints loss for each minibatch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Loss: 2.3089029788970947
Loss: 2.3026552200317383
Loss: 2.3036789894104004
Loss: 2.299530267715454
Loss: 2.3146543502807617
Loss: 2.295773983001709
Loss: 2.3114850521087646
Loss: 2.312140703201294
Loss: 2.305482864379883
Loss: 2.3025853633880615
Loss: 2.296191930770874
Loss: 2.3044071197509766
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.3013594150543213
Loss: 2.3110408782958984
Loss: 2.305534601211548
Loss: 2.3025853633880615
Loss: 2.297464370727539
Loss: 2.2984910011291504
Loss: 2.3149735927581787
Loss: 2.3091654777526855
Loss: 2.3044753074645996
Loss: 2.3025853633880615
Loss: 2.3103907108306885
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.3118462562561035
Loss: 2.301429510116577
Loss: 2.300888776779175
Loss: 2.3073575496673584
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.3043949604034424
Loss: 2.3025853633880615
Loss: 2.2923624515533447
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.2961018085479736
Loss: 2.304161787033081
Loss: 2.2973952293395996
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.3025853633880615
Loss: 2.3019461631774902
Loss: 2.303677797317505
Loss: 2.3008153438568115
Loss: 2.3025853633880615
Loss: 2.2960872650146484
Loss: 2.3025853633880615
Loss: 2.3028862476348877
Loss: 2.2993662357330322
Loss: 2.3038816452026367
Loss: 2.3073089122772217
Loss: 2.29280161857605
Loss: 2.3034892082214355
Loss: 2.3089585304260254
Loss: 2.3039684295654297
Loss: 2.30177903175354
Loss: 2.3025853633880615
Loss: 2.299685478210449
Loss: 2.3098132610321045</code></pre>
</div>
</div>
<p>Hmmm…. sometimes when I run this I see the loss stay around the same value, other times it drops just a little before stabilizing…</p>
<p>We are going to need a better way to look at the trending of the loss function across iterations, so we may as well make that function now:</p>
<div class="cell" data-execution_count="25">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="kw">def</span> calc_grad(xb, yb, model, f_loss):</span>
<span id="cb38-2"><a href="#cb38-2"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb38-3"><a href="#cb38-3"></a>    loss <span class="op">=</span> f_loss(preds, yb)</span>
<span id="cb38-4"><a href="#cb38-4"></a>    loss.backward()</span>
<span id="cb38-5"><a href="#cb38-5"></a>    <span class="cf">return</span> loss.data.item()  <span class="co"># Return the loss (see why later)</span></span>
<span id="cb38-6"><a href="#cb38-6"></a></span>
<span id="cb38-7"><a href="#cb38-7"></a></span>
<span id="cb38-8"><a href="#cb38-8"></a>calc_grad(mini_x, mini_y, myModel, my_loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="225">
<pre><code>2.3025851249694824</code></pre>
</div>
</div>
<p>This is another step towards modularizing (if that is a word) our code. Making use of that last function, our training loop now looks like this:</p>
<div class="cell" data-execution_count="26">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="co"># Initializing weights again for a fresh start</span></span>
<span id="cb40-2"><a href="#cb40-2"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb40-3"><a href="#cb40-3"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb40-4"><a href="#cb40-4"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb40-5"><a href="#cb40-5"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb40-6"><a href="#cb40-6"></a></span>
<span id="cb40-7"><a href="#cb40-7"></a><span class="bu">print</span>(<span class="st">"PreTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb40-8"><a href="#cb40-8"></a></span>
<span id="cb40-9"><a href="#cb40-9"></a></span>
<span id="cb40-10"><a href="#cb40-10"></a><span class="kw">def</span> train_epoch(model, lr, params, f_loss):</span>
<span id="cb40-11"><a href="#cb40-11"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb40-12"><a href="#cb40-12"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb40-13"><a href="#cb40-13"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb40-14"><a href="#cb40-14"></a>            <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb40-15"><a href="#cb40-15"></a>                p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb40-16"><a href="#cb40-16"></a>                p.grad.zero_()</span>
<span id="cb40-17"><a href="#cb40-17"></a></span>
<span id="cb40-18"><a href="#cb40-18"></a></span>
<span id="cb40-19"><a href="#cb40-19"></a>train_epoch(myModel, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb40-20"><a href="#cb40-20"></a><span class="bu">print</span>(<span class="st">"PostTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>PreTrain Accuracy: 0.09
PostTrain Accuracy: 0.1</code></pre>
</div>
</div>
<p>UH OH!</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Overfitting
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Our loss is going down, but our error is erratic, staying the same, or going up! This is an indicator that we’ve reached a point where the mathematical strategy we’re using to optimize is now playing games with us. The modifications <em>do</em> reduce loss, but that is no longer aligned with an improvement with the task. Recall that a loss function <em>should</em> always correlate with NN efficacy, but this can break down at the fringes.</p>
</div>
</div>
</div>
<p>Is this a problem? Yes! But one that can be solved. Overall we are still in good territory.. We know the math and programming is working from a nuts and bolts perspective. What this indicates to us is that we need to tune the <em>metaparameters</em> of our neural network function. How many layers are there? How wide are the layers? We may be in a situation where the NN doesn’t have sufficient <em>capacity</em><a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> to get us the outcomes we’re looking for.</p>
</section>
<section id="building-test-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="building-test-infrastructure">Building Test Infrastructure</h2>
<p>We’ve built an NN from first principles now, putting together the building blocks of parameter initialization, running the algebra through the net, backpropagation to compute the gradient, and parameter calibration through the gradient descent method. This all executing repeatedly across batches of data.</p>
<p>Before we adjust the network structure to optimize the outcomes, let’s define a yet higher encapsulation of these tools to further condense the commands needed to execute this whole process, as well as enable more execution flexibility. This is where the <code>BasicOptim</code> class omes in:</p>
<div class="cell" data-execution_count="27">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a><span class="kw">class</span> BasicOptim:</span>
<span id="cb42-2"><a href="#cb42-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr): <span class="va">self</span>.params, <span class="va">self</span>.lr <span class="op">=</span> <span class="bu">list</span>(params), lr</span>
<span id="cb42-3"><a href="#cb42-3"></a></span>
<span id="cb42-4"><a href="#cb42-4"></a>    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb42-5"><a href="#cb42-5"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb42-6"><a href="#cb42-6"></a>            p.data <span class="op">-=</span> p.grad.data <span class="op">*</span> <span class="va">self</span>.lr</span>
<span id="cb42-7"><a href="#cb42-7"></a></span>
<span id="cb42-8"><a href="#cb42-8"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb42-9"><a href="#cb42-9"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb42-10"><a href="#cb42-10"></a>            p.grad <span class="op">=</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now, we can condense the training loop, and also neatly add the function of tracking loss across batches.</p>
<div class="cell" data-execution_count="28">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a><span class="kw">def</span> train_epoch(model, opt, lr, params, f_loss):</span>
<span id="cb43-2"><a href="#cb43-2"></a>    losses <span class="op">=</span> []  <span class="co"># Will allow for recording epoch wise loss</span></span>
<span id="cb43-3"><a href="#cb43-3"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb43-4"><a href="#cb43-4"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb43-5"><a href="#cb43-5"></a>        opt.step()</span>
<span id="cb43-6"><a href="#cb43-6"></a>        losses.append(calc_grad(xb, yb, model, f_loss))</span>
<span id="cb43-7"><a href="#cb43-7"></a>        opt.zero_grad()</span>
<span id="cb43-8"><a href="#cb43-8"></a>    <span class="cf">return</span> tensor(losses)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And per usual, give it a test:</p>
<div class="cell" data-execution_count="29">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>my_opt <span class="op">=</span> BasicOptim([w1, b1, w2, b2], <span class="fl">0.01</span>)</span>
<span id="cb44-2"><a href="#cb44-2"></a>res <span class="op">=</span> train_epoch(myModel, my_opt, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb44-3"><a href="#cb44-3"></a>res.mean(), res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="229">
<pre><code>(tensor(2.0516),
 tensor([2.2995, 2.1674, 1.8490, 2.2199, 2.0592, 1.9886, 1.9089, 2.1189, 1.9221, 2.2172, 1.8646, 2.0680, 1.9976, 2.1243, 2.1113, 2.0379, 2.3080, 2.0574, 2.0447, 1.8639, 1.9598, 1.9709, 2.1587, 2.2716,
         2.1667, 2.0777, 2.0421, 2.1068, 1.9676, 2.0037, 2.0020, 2.0820, 2.0589, 1.9614, 2.1646, 2.1836, 1.9012, 2.1544, 2.1017, 1.8865, 1.9585, 2.0961, 2.1407, 1.8869, 2.1004, 2.0525, 1.7599, 2.1563,
         2.1390, 2.1625, 2.0392, 2.1607, 1.9849, 2.0615, 1.9621, 1.9761, 2.1602, 1.9650, 1.9886, 2.1052, 2.0060, 1.8893]))</code></pre>
</div>
</div>
<p>Great: bug free. But… still this issue of the loss bouncing around withot a clear improvement trend…</p>
<p>Knowing that I want to experiment with the models metaprameters to improve it, I’ll finally define yet another object to contain those we’ve made so far and run trials with customized parameters, and providing the means for monitoring these trials more easily.</p>
<div class="cell" data-execution_count="30">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="kw">class</span> cTrial:</span>
<span id="cb46-2"><a href="#cb46-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, numE<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.01</span>, model<span class="op">=</span>myModel, opt<span class="op">=</span>my_opt, params<span class="op">=</span>[w1, b1, w2, b2], f_loss<span class="op">=</span>my_loss):</span>
<span id="cb46-3"><a href="#cb46-3"></a>        <span class="va">self</span>.numE <span class="op">=</span> numE</span>
<span id="cb46-4"><a href="#cb46-4"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb46-5"><a href="#cb46-5"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb46-6"><a href="#cb46-6"></a>        <span class="va">self</span>.opt <span class="op">=</span> opt</span>
<span id="cb46-7"><a href="#cb46-7"></a>        <span class="va">self</span>.params <span class="op">=</span> params</span>
<span id="cb46-8"><a href="#cb46-8"></a>        <span class="va">self</span>.f_loss <span class="op">=</span> f_loss</span>
<span id="cb46-9"><a href="#cb46-9"></a>        <span class="va">self</span>.res <span class="op">=</span> []</span>
<span id="cb46-10"><a href="#cb46-10"></a>        <span class="va">self</span>.valids <span class="op">=</span> []</span>
<span id="cb46-11"><a href="#cb46-11"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> []  <span class="co"># For tracking change in weights across learning</span></span>
<span id="cb46-12"><a href="#cb46-12"></a></span>
<span id="cb46-13"><a href="#cb46-13"></a>    <span class="kw">def</span> run(<span class="va">self</span>, numE<span class="op">=</span><span class="va">None</span>, wkLr<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb46-14"><a href="#cb46-14"></a>        <span class="va">self</span>.valids <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb46-15"><a href="#cb46-15"></a>        epch_losses <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb46-16"><a href="#cb46-16"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> [[], [], [], []]  <span class="co"># 4 contents, w1,b1,w2,b2</span></span>
<span id="cb46-17"><a href="#cb46-17"></a>        <span class="cf">if</span> numE <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb46-18"><a href="#cb46-18"></a>            numE <span class="op">=</span> <span class="va">self</span>.numE</span>
<span id="cb46-19"><a href="#cb46-19"></a>        <span class="cf">if</span> wkLr <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb46-20"><a href="#cb46-20"></a>            wkLr <span class="op">=</span> <span class="va">self</span>.lr</span>
<span id="cb46-21"><a href="#cb46-21"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numE):</span>
<span id="cb46-22"><a href="#cb46-22"></a>            <span class="co"># -- Record wts for analysis</span></span>
<span id="cb46-23"><a href="#cb46-23"></a>            <span class="va">self</span>.wtsHist[<span class="dv">0</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb46-24"><a href="#cb46-24"></a>            <span class="va">self</span>.wtsHist[<span class="dv">1</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb46-25"><a href="#cb46-25"></a>            <span class="va">self</span>.wtsHist[<span class="dv">2</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb46-26"><a href="#cb46-26"></a>            <span class="va">self</span>.wtsHist[<span class="dv">3</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb46-27"><a href="#cb46-27"></a>            <span class="co"># --</span></span>
<span id="cb46-28"><a href="#cb46-28"></a>            res <span class="op">=</span> train_epoch(<span class="va">self</span>.model, <span class="va">self</span>.opt, <span class="va">self</span>.lr,</span>
<span id="cb46-29"><a href="#cb46-29"></a>                              <span class="va">self</span>.params, <span class="va">self</span>.f_loss)</span>
<span id="cb46-30"><a href="#cb46-30"></a>            epch_losses.append(res)</span>
<span id="cb46-31"><a href="#cb46-31"></a>            <span class="va">self</span>.valids.append(validate_epoch(<span class="va">self</span>.model))</span>
<span id="cb46-32"><a href="#cb46-32"></a>        <span class="va">self</span>.res <span class="op">=</span> torch.stack(epch_losses)</span>
<span id="cb46-33"><a href="#cb46-33"></a>        <span class="va">self</span>.valids <span class="op">=</span> tensor(<span class="va">self</span>.valids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This object allows for defining all the parameters necessary for a trial, and then running it one line of code, alternaitvely overwriting the number of Epochs, and learning rate parameters.</p>
</section>
<section id="experimenting-to-optimize-nn" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experimenting-to-optimize-nn">Experimenting To Optimize NN</h2>
<p>Let’s get into it. We make a trial object and run it:</p>
<div class="cell" data-execution_count="31">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>my_try <span class="op">=</span> cTrial()</span>
<span id="cb47-2"><a href="#cb47-2"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb47-3"><a href="#cb47-3"></a>my_try.res.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="231">
<pre><code>torch.Size([50, 62])</code></pre>
</div>
</div>
<p>Our functions are logging loss for each mini batch across all epochs. So this makes sense. We had 1540 data points, with minibatches of 25. <code>25*62=1550</code>, so we have 61 minibatches of 25 one last minibatch of 15. And 50 collections of 62, one for each epoch.</p>
<section id="first-try" class="level3">
<h3 class="anchored" data-anchor-id="first-try">First Try</h3>
<p>So let’s see the results by taking the average loss across each epoch, hopefully, it’ll be dropping!</p>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-loss1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-loss1-output-1.png" width="585" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: Average loss across epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>So loss is going down, it’s working right? The real test isn’t reduction to our measure loss, but to see that carry through to an increase in testing accuracy:</p>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>plt.plot(my_try.valids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-val1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-val1-output-1.png" width="595" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Prediction accuracy across epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>Disaster strikes! We’re never getting more than 20% accuracy! Well, they call it <em>deep learning</em>, so let’s make our model deeper. Two layers? A pittance!</p>
</section>
<section id="second-try" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="second-try">Second Try</h3>
<p>Our Model V2 will look like this:</p>
<ul>
<li>64 input activations (image pixels) to 32 neurons in the first layer, with a unique (not broadcasted) bias for each neuron</li>
<li>The next layer will reduce the 32 neurons to 16, with anothe runique bias for each</li>
<li>The next will reduce teh 16 layers to 10 (output layer activations) with a unique bias for each</li>
<li>Between each layer we will use the ReLu for nonlinearity.</li>
</ul>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Visualizing V2 Of Our NN
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Can you picture the matrix multiplication? It’s important to get it straight! What would the equivalent of <a href="#fig-Layer1">Figure&nbsp;2</a> look like for our NN V2?</p>
</div>
</div>
</div>
<p>Defining parameters:</p>
<div class="cell" data-execution_count="34">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a><span class="co"># Initialize parameters using the functions defined above to make it easy to generate variously sized tensors</span></span>
<span id="cb51-2"><a href="#cb51-2"></a>W1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb51-3"><a href="#cb51-3"></a>B1 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb51-4"><a href="#cb51-4"></a>W2 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb51-5"><a href="#cb51-5"></a>B2 <span class="op">=</span> init_params(<span class="dv">16</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb51-6"><a href="#cb51-6"></a>W3 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb51-7"><a href="#cb51-7"></a>B3 <span class="op">=</span> init_params(<span class="dv">10</span>)  <span class="co"># Feature-specific biases added</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Define the model:</p>
<div class="cell" data-execution_count="35">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a><span class="kw">def</span> mdlV2(xb):</span>
<span id="cb52-2"><a href="#cb52-2"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1<span class="op">+</span>B1</span>
<span id="cb52-3"><a href="#cb52-3"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb52-4"><a href="#cb52-4"></a>    res <span class="op">=</span> res<span class="op">@</span>W2<span class="op">+</span>B2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb52-5"><a href="#cb52-5"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb52-6"><a href="#cb52-6"></a>    res <span class="op">=</span> res<span class="op">@</span>W3<span class="op">+</span>B3  <span class="co"># returns 10 features for each input</span></span>
<span id="cb52-7"><a href="#cb52-7"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Test it with a mini sample to ensure the Tensors were defined in the correct size to achieve our purpose:</p>
<div class="cell" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>mdlV2(mini_x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="236">
<pre><code>tensor([[-0.3376,  0.2335,  0.0899, -0.3816,  0.3013,  0.3804,  0.2879,  0.3500,  0.1701,  0.0681],
        [-0.0732,  0.0330,  0.1219, -0.1864,  0.0446,  0.2158,  0.4660,  0.0515,  0.3962, -0.5011],
        [-0.1899, -0.0934,  0.1381, -0.2746,  0.0784,  0.3310,  0.4186,  0.0863,  0.4079, -0.6188],
        [-0.3665,  0.2661,  0.2646, -0.4887,  0.2982,  0.1492,  0.3429,  0.2664,  0.4471, -0.1568],
        [-0.3377,  0.0690,  0.3718, -0.4352,  0.1145,  0.2599,  0.3576,  0.1508,  0.5164, -0.3415]], grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>
<p>Run it!</p>
<div id="fig-V2" class="quarto-figure quarto-figure-center anchored page-columns page-full">
<figure class="figure page-columns page-full">
<div class="cell page-columns page-full" data-execution_count="37">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV2, opt<span class="op">=</span>BasicOptim(</span>
<span id="cb55-2"><a href="#cb55-2"></a>    [W1, B1, W2, B2, W3, B3], <span class="fl">0.01</span>), params<span class="op">=</span>[W1, B1, W2, B2, W3, B3])</span>
<span id="cb55-3"><a href="#cb55-3"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb55-4"><a href="#cb55-4"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb55-5"><a href="#cb55-5"></a>plt.plot(my_try.valids)</span>
<span id="cb55-6"><a href="#cb55-6"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display column-page">
<p><img src="index_files/figure-html/cell-38-output-1.png" width="575" height="414" class="figure-img"></p>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Accuracy Increasing As Loss Reduces</figcaption><p></p>
</figure>
</div>
<p>Fantastic! Like an Olympic high jumper, we vault just over the top of the target! Time for champagne?! Not so fast! These results are those from exeucting on the training set. Remember when we partitioned off the validation set? Let’s see how the model does on that data.</p>
<p>The point of a validation set is to increase cofnidence that our model hasn’t faux-optimized by ‘playing our game’ so to speak, and learning to score well on test data while not actually generalizing across the domain of all possible inputs</p>
<div id="fig-V2conf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<div class="cell" data-execution_count="38">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="kw">def</span> confMtx(model):</span>
<span id="cb56-2"><a href="#cb56-2"></a>    conf <span class="op">=</span> torch.zeros([<span class="dv">10</span>, <span class="dv">10</span>], dtype<span class="op">=</span>torch.int32)</span>
<span id="cb56-3"><a href="#cb56-3"></a>    acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-4"><a href="#cb56-4"></a>    tot <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb56-5"><a href="#cb56-5"></a>    <span class="cf">for</span> xv, yv <span class="kw">in</span> dls[<span class="dv">1</span>]:</span>
<span id="cb56-6"><a href="#cb56-6"></a>        preds <span class="op">=</span> model(xv).<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb56-7"><a href="#cb56-7"></a>        tot <span class="op">+=</span> <span class="bu">len</span>(preds)</span>
<span id="cb56-8"><a href="#cb56-8"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb56-9"><a href="#cb56-9"></a>            conf[yv[i].item()][preds[i].item()] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb56-10"><a href="#cb56-10"></a>            <span class="cf">if</span> yv[i].item() <span class="op">==</span> preds[i].item():</span>
<span id="cb56-11"><a href="#cb56-11"></a>                acc <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb56-12"><a href="#cb56-12"></a>    df <span class="op">=</span> pd.DataFrame(conf)</span>
<span id="cb56-13"><a href="#cb56-13"></a>    df.style.set_properties(<span class="op">**</span>{<span class="st">'font-size'</span>: <span class="st">'6pt'</span>}</span>
<span id="cb56-14"><a href="#cb56-14"></a>                            ).background_gradient(<span class="st">'Greys'</span>)</span>
<span id="cb56-15"><a href="#cb56-15"></a>    df.style.set_table_styles([<span class="bu">dict</span>(selector<span class="op">=</span><span class="st">'th'</span>, props<span class="op">=</span>[(</span>
<span id="cb56-16"><a href="#cb56-16"></a>        <span class="st">'text-align'</span>, <span class="st">'center'</span>)])]).set_properties(<span class="op">**</span>{<span class="st">'text-align'</span>: <span class="st">'center'</span>})</span>
<span id="cb56-17"><a href="#cb56-17"></a>    df.style.set_caption(<span class="st">"Top Axis: Predicted value. Left Axis: Actual Value"</span>)</span>
<span id="cb56-18"><a href="#cb56-18"></a>    <span class="cf">return</span> df, acc<span class="op">/</span>tot</span>
<span id="cb56-19"><a href="#cb56-19"></a></span>
<span id="cb56-20"><a href="#cb56-20"></a></span>
<span id="cb56-21"><a href="#cb56-21"></a>    <span class="co"># return df</span></span>
<span id="cb56-22"><a href="#cb56-22"></a>df, acc <span class="op">=</span> confMtx(mdlV2)</span>
<span id="cb56-23"><a href="#cb56-23"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="238">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>15</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>17</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>19</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>2</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>17</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="39">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy on validation set: "</span><span class="op">+</span><span class="bu">str</span>(acc))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Accuracy on validation set: 0.9</code></pre>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: Confusion Matrix. Left Axis: Actual Number. Top Axis: Predicted Number.</figcaption><p></p>
</figure>
</div>
<p>Excellent! The NN has learned to generalize. But, dear reader, I have lied to you</p>
</section>
</section>
</section>
<section id="outro" class="level1">
<h1>Outro</h1>
<section id="getting-here" class="level2">
<h2 class="anchored" data-anchor-id="getting-here">Getting Here</h2>
<p>When I was first working through this mini-project, the final outcome did not come so easily. There are a lot of meta-parameters that we can play with, which creates a large solution space to search. These include:</p>
<ul>
<li>Number of layers</li>
<li>Number of neurons per layer</li>
<li>Broadcasting a single bias vs using many unique biases</li>
<li>Learning rate, including whether or not to change the learning rate across epochs, and if so, according to what function</li>
<li>What non-linearity function to use</li>
<li>What distribution (and with what parameters) to use for initializing parameters</li>
<li>How many epochs to run</li>
<li>What mini batch size to use</li>
<li>Whether or not to shuffle the data</li>
</ul>
<p>That’s a lot of possibilities- I’m still learning what the best practices are to make decisions on these matters. I sure did spend a good numbers of hours experimenting with what seemed like dead ends though!</p>
<p>What finally made the difference was reducing the variance of tha random distribution I was using to initialize my parameters. My earlier models were very naive, always defaulting to just selecting one digit or another. Looking back, my hypothesis is that the larger variance in initialization distribution led to the largest parameters dominating the NN through the layers. But it’s difficult to make any conclusive statements like that.</p>
</section>
<section id="lessons-learned" class="level2">
<h2 class="anchored" data-anchor-id="lessons-learned">Lessons Learned</h2>
<p>A number of great first lessons in NN’s to take away from this case study.</p>
<p>First of all, the great structure provided by the FastAI course with regard to defining the various classes and functions whose interplay really enables quick and effective implementaiton of the strategy. This, I think, is <em>so</em> important. It’s the difference between understanding the first principles of how this stuff works ( i.e.&nbsp;“A nail is driven into a beam by application of force to its head perpendicular to the recieving surface”) vs.&nbsp;the making of the toolkit at hadn to implement the idea (i.e.&nbsp;“Here is a hammer! Get to work!) Here is a summary of this toolkit, from the outermost layer, in:</p>
<ul>
<li>A ‘Trial’ taking parameters like number of epochs, learning rate, the model, optimizer, parameters, and loss function</li>
<li>An optimizer to execute the ‘parameter adjustment’ inherent to the learning process</li>
<li>An function to run an epoch, givne all necessary inputs</li>
<li>A function to execute the model on a batch, and calculate the gradient</li>
<li>A function calculate loss</li>
<li>A function to calculate inputs</li>
</ul>
<p>Another <em>important</em> lesson was not to understimate the pitfalls of implementing your own math. I thought I had my loss-function issue solved when I abandoned my <code>SoftMax</code> formula for PyTorch’s pre-built function, but I was further humbled by the need to resort the built in <code>CrossEntropyLoss</code> function. Sometimes it really is so much easier to understand something in principle than it is to implement it; we could drown deep currents of complexity hidden by the birdges built by those who came before. We should know where they are so as to use them and not fall off.</p>
<p>This case study also forced me to grapple with the whole matrix multiplication process to ensure I was understanding exactly how to define how many neurons each layer would have, and execute that in code, etc. it was a bit of a mental hurdle to go from just ahving done it on paper in the classroom to implementing with code. Though a lot easier to be sure!</p>
<p>Another important and very actionable lesson to prod the solution space by testing orders of magnitude different meta parameters (learning rate, parameter initialization variance). This is important because one realistically has no basis on whihc to guess at how much or how little a meta parameter should vary when beginning to optimize an NN. By testing different orders of magnitude <code>(10,1,0.1,0.01 ...)</code> we can drop quickly and easily identify which are ain whihc to focus our efforts.</p>
<p>Lastly, to have hope and keep fighting the good fight! I was having some real trouble with making an NN that didn’t just naively default to one digit or another. But in the end, with enough experimentation and further study, I prevailed… exciting. And for your pleasure, I end this post with my NN cemetery, those failed networks that I couldn’t get any good results out of. Although who knows, maybe they would perform better than my simple V2 model in the end, with a slight change to some metaparameter. After all, I had abandoned my V2 model after it didnt work at first, until I changed my parameter initialization much later, and decided it might be worth giving it another shot. You never know when you’ll strike gold.</p>
</section>
<section id="nn-graveyard" class="level2">
<h2 class="anchored" data-anchor-id="nn-graveyard">NN Graveyard</h2>
<p>Our dearly departed…</p>
<section id="the-funnel" class="level3">
<h3 class="anchored" data-anchor-id="the-funnel">The Funnel</h3>
<p>Started wide and iteratively narrowing down to the output size over many layers.</p>
<div class="cell" data-execution_count="40">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>), <span class="dv">1</span>)  <span class="co"># 64 in, 32 out</span></span>
<span id="cb59-2"><a href="#cb59-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-3"><a href="#cb59-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>), <span class="dv">1</span>)  <span class="co"># 32 in, 16 out</span></span>
<span id="cb59-4"><a href="#cb59-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-5"><a href="#cb59-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>), <span class="dv">1</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb59-6"><a href="#cb59-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb59-7"><a href="#cb59-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>), <span class="dv">1</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb59-8"><a href="#cb59-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>, <span class="dv">1</span>)</span>
<span id="cb59-9"><a href="#cb59-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>), <span class="dv">1</span>)</span>
<span id="cb59-10"><a href="#cb59-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>, <span class="dv">1</span>)</span>
<span id="cb59-11"><a href="#cb59-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>), <span class="dv">1</span>)</span>
<span id="cb59-12"><a href="#cb59-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="dv">1</span>)</span>
<span id="cb59-13"><a href="#cb59-13"></a></span>
<span id="cb59-14"><a href="#cb59-14"></a></span>
<span id="cb59-15"><a href="#cb59-15"></a><span class="kw">def</span> mdlV3(xb):</span>
<span id="cb59-16"><a href="#cb59-16"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1v3<span class="op">+</span>B1v3</span>
<span id="cb59-17"><a href="#cb59-17"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-18"><a href="#cb59-18"></a>    res <span class="op">=</span> res<span class="op">@</span>W2v3<span class="op">+</span>B2v3</span>
<span id="cb59-19"><a href="#cb59-19"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-20"><a href="#cb59-20"></a>    res <span class="op">=</span> res<span class="op">@</span>W3v3<span class="op">+</span>B3v3</span>
<span id="cb59-21"><a href="#cb59-21"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-22"><a href="#cb59-22"></a>    res <span class="op">=</span> res<span class="op">@</span>W4v3<span class="op">+</span>B4v3</span>
<span id="cb59-23"><a href="#cb59-23"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-24"><a href="#cb59-24"></a>    res <span class="op">=</span> res<span class="op">@</span>W5v3<span class="op">+</span>B5v3</span>
<span id="cb59-25"><a href="#cb59-25"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb59-26"><a href="#cb59-26"></a>    res <span class="op">=</span> res<span class="op">@</span>W6v3<span class="op">+</span>B6v3</span>
<span id="cb59-27"><a href="#cb59-27"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="bad-parameter-initialization" class="level3">
<h3 class="anchored" data-anchor-id="bad-parameter-initialization">Bad Parameter Initialization</h3>
<p>Notice that in the above, I had the parameter initialization with a variance of 1 passed to the random number generator distribution. And so I kept getting <strong>junk</strong> results like the following!</p>
<div class="cell" data-execution_count="41">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb60-2"><a href="#cb60-2"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb60-3"><a href="#cb60-3"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb60-4"><a href="#cb60-4"></a></span>
<span id="cb60-5"><a href="#cb60-5"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb60-6"><a href="#cb60-6"></a>plt.plot(my_try.valids)</span>
<span id="cb60-7"><a href="#cb60-7"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funnel" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funnel-output-1.png" width="575" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: ‘Funnel’ model loss and accuracy across epochs</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>What does a flat lining accuracy mean exactly? The confusion matrix tells all<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>…</p>
<div class="cell" data-execution_count="42">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>df, acc <span class="op">=</span> confMtx(mdlV3)</span>
<span id="cb61-2"><a href="#cb61-2"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="242">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Yup.. Literally every input yields an output of <code>0</code>, so naturally, given the data set composition, its dead right, <em>10% of the time</em>!</p>
</section>
<section id="better-parameter-initialization" class="level3">
<h3 class="anchored" data-anchor-id="better-parameter-initialization">Better Parameter Initialization</h3>
<p>But looks at what happens when we put the variance of the number generator to 0.1, its default!</p>
<div class="cell" data-execution_count="43">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb62-2"><a href="#cb62-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb62-3"><a href="#cb62-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb62-4"><a href="#cb62-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb62-5"><a href="#cb62-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb62-6"><a href="#cb62-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb62-7"><a href="#cb62-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb62-8"><a href="#cb62-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb62-9"><a href="#cb62-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb62-10"><a href="#cb62-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb62-11"><a href="#cb62-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb62-12"><a href="#cb62-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb62-13"><a href="#cb62-13"></a></span>
<span id="cb62-14"><a href="#cb62-14"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb62-15"><a href="#cb62-15"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb62-16"><a href="#cb62-16"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb62-17"><a href="#cb62-17"></a></span>
<span id="cb62-18"><a href="#cb62-18"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb62-19"><a href="#cb62-19"></a>plt.plot(my_try.valids)</span>
<span id="cb62-20"><a href="#cb62-20"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funnel_try2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funnel_try2-output-1.png" width="575" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8: Retrying the ‘Funnel model’, with smaller initial parameter variation</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>And the confusion matrix:</p>
<div class="cell" data-execution_count="44">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>df, acc <span class="op">=</span> confMtx(mdlV3)</span>
<span id="cb63-2"><a href="#cb63-2"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-funnel_try2conf" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>20</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>19</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>1</td>
      <td>2</td>
      <td>14</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>19</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>3</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>16</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>18</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0</td>
      <td>4</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>16</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>8</td>
      <td>1</td>
      <td>11</td>
    </tr>
  </tbody>
</table>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;9: Retrying the ‘Funnel model’, Confusion Matrix</figcaption><p></p>
</figure>
</div>
</div>
</section>
<section id="different-learning-rates" class="level3">
<h3 class="anchored" data-anchor-id="different-learning-rates">Different Learning Rates</h3>
<p>Lets use this model to illustrate how different results can be with different learning rates.</p>
<div id="fig-lr" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell quarto-layout-cell" data-execution_count="45" style="flex-basis: 25.0%;justify-content: center;">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb64-2"><a href="#cb64-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb64-3"><a href="#cb64-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb64-4"><a href="#cb64-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb64-5"><a href="#cb64-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb64-6"><a href="#cb64-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb64-7"><a href="#cb64-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb64-8"><a href="#cb64-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb64-9"><a href="#cb64-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb64-10"><a href="#cb64-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb64-11"><a href="#cb64-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb64-12"><a href="#cb64-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb64-13"><a href="#cb64-13"></a></span>
<span id="cb64-14"><a href="#cb64-14"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.1</span>), params<span class="op">=</span>[</span>
<span id="cb64-15"><a href="#cb64-15"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb64-16"><a href="#cb64-16"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb64-17"><a href="#cb64-17"></a></span>
<span id="cb64-18"><a href="#cb64-18"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb64-19"><a href="#cb64-19"></a>plt.plot(my_try.valids)</span>
<span id="cb64-20"><a href="#cb64-20"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funlr1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funlr1-output-1.png" data-ref-parent="fig-lr" width="575" height="430" class="figure-img"></p>
<p></p><figcaption class="figure-caption">(a) Funnel model, lr=1</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" data-execution_count="46" style="flex-basis: 25.0%;justify-content: center;">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb65-2"><a href="#cb65-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb65-3"><a href="#cb65-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb65-4"><a href="#cb65-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb65-5"><a href="#cb65-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb65-6"><a href="#cb65-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb65-7"><a href="#cb65-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb65-8"><a href="#cb65-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb65-9"><a href="#cb65-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb65-10"><a href="#cb65-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb65-11"><a href="#cb65-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb65-12"><a href="#cb65-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb65-13"><a href="#cb65-13"></a></span>
<span id="cb65-14"><a href="#cb65-14"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.1</span>), params<span class="op">=</span>[</span>
<span id="cb65-15"><a href="#cb65-15"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb65-16"><a href="#cb65-16"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb65-17"><a href="#cb65-17"></a></span>
<span id="cb65-18"><a href="#cb65-18"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb65-19"><a href="#cb65-19"></a>plt.plot(my_try.valids)</span>
<span id="cb65-20"><a href="#cb65-20"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funlr2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funlr2-output-1.png" data-ref-parent="fig-lr" width="575" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">(b) Funnel model, lr=0.1</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" data-execution_count="47" style="flex-basis: 25.0%;justify-content: center;">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb66-2"><a href="#cb66-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb66-3"><a href="#cb66-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb66-4"><a href="#cb66-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb66-5"><a href="#cb66-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb66-6"><a href="#cb66-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb66-7"><a href="#cb66-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb66-8"><a href="#cb66-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb66-9"><a href="#cb66-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb66-10"><a href="#cb66-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb66-11"><a href="#cb66-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb66-12"><a href="#cb66-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb66-13"><a href="#cb66-13"></a></span>
<span id="cb66-14"><a href="#cb66-14"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb66-15"><a href="#cb66-15"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb66-16"><a href="#cb66-16"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb66-17"><a href="#cb66-17"></a></span>
<span id="cb66-18"><a href="#cb66-18"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb66-19"><a href="#cb66-19"></a>plt.plot(my_try.valids)</span>
<span id="cb66-20"><a href="#cb66-20"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funlr3" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funlr3-output-1.png" data-ref-parent="fig-lr" width="575" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">(c) Funnel model, lr=.01</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="cell quarto-layout-cell" data-execution_count="48" style="flex-basis: 25.0%;justify-content: center;">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb67-2"><a href="#cb67-2"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb67-3"><a href="#cb67-3"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb67-4"><a href="#cb67-4"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb67-5"><a href="#cb67-5"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb67-6"><a href="#cb67-6"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb67-7"><a href="#cb67-7"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb67-8"><a href="#cb67-8"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb67-9"><a href="#cb67-9"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb67-10"><a href="#cb67-10"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb67-11"><a href="#cb67-11"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb67-12"><a href="#cb67-12"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb67-13"><a href="#cb67-13"></a></span>
<span id="cb67-14"><a href="#cb67-14"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb67-15"><a href="#cb67-15"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb67-16"><a href="#cb67-16"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb67-17"><a href="#cb67-17"></a></span>
<span id="cb67-18"><a href="#cb67-18"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb67-19"><a href="#cb67-19"></a>plt.plot(my_try.valids)</span>
<span id="cb67-20"><a href="#cb67-20"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div id="fig-funlr4" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="index_files/figure-html/fig-funlr4-output-1.png" data-ref-parent="fig-lr" width="575" height="414" class="figure-img"></p>
<p></p><figcaption class="figure-caption">(d) Funnel model, lr=0.001</figcaption><p></p>
</figure>
</div>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;10: Trial outcomes with different learning rates.</figcaption><p></p>
</figure>
</div>
<p>Smaller learning rates are characterized by smoother loss curves since they traverse a smaller region of the solution space. It’s a double edged sword, though. On the one hand, a small learning rate can help hone in on a local optimum. That might be undesirable early on in the learnign process though, as the random initialization may put us in a region of parameter space where the nearest local optimum isn’t great with respect to the optima across the wider parameter space. A larger learning rate can help us take large steps from an initial poor position to a much better swath of parameter space, where a smaller rate can then hone in to better results.</p>
</section>
<section id="the-wormhole" class="level3">
<h3 class="anchored" data-anchor-id="the-wormhole">The Wormhole</h3>
<p>Along my wanderings in the desert, I thought to try having the network layer structure narrow rapidly but extend across many layers. This implementation has 12 layers, with the last 8 all having 10 neurons in and out.</p>
<div class="cell" data-execution_count="49">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1"></a>W1v4 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>), <span class="fl">0.001</span>)  <span class="co"># 64 in, 32 out</span></span>
<span id="cb68-2"><a href="#cb68-2"></a>B1v4 <span class="op">=</span> init_params(<span class="dv">32</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb68-3"><a href="#cb68-3"></a>W2v4 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>), <span class="fl">0.001</span>)  <span class="co"># 32 in, 16 out</span></span>
<span id="cb68-4"><a href="#cb68-4"></a>B2v4 <span class="op">=</span> init_params(<span class="dv">16</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb68-5"><a href="#cb68-5"></a>W3v4 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb68-6"><a href="#cb68-6"></a>B3v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb68-7"><a href="#cb68-7"></a>W4v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb68-8"><a href="#cb68-8"></a>B4v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-9"><a href="#cb68-9"></a>W5v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-10"><a href="#cb68-10"></a>B5v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-11"><a href="#cb68-11"></a>W6v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-12"><a href="#cb68-12"></a>B6v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-13"><a href="#cb68-13"></a>W7v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-14"><a href="#cb68-14"></a>B7v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-15"><a href="#cb68-15"></a>W8v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-16"><a href="#cb68-16"></a>B8v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-17"><a href="#cb68-17"></a>W9v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-18"><a href="#cb68-18"></a>B9v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-19"><a href="#cb68-19"></a>W10v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-20"><a href="#cb68-20"></a>B10v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-21"><a href="#cb68-21"></a>W11v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-22"><a href="#cb68-22"></a>B11v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-23"><a href="#cb68-23"></a>W12v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb68-24"><a href="#cb68-24"></a>B12v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb68-25"><a href="#cb68-25"></a></span>
<span id="cb68-26"><a href="#cb68-26"></a></span>
<span id="cb68-27"><a href="#cb68-27"></a><span class="kw">def</span> mdlV4(xb):</span>
<span id="cb68-28"><a href="#cb68-28"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1v4<span class="op">+</span>B1v4</span>
<span id="cb68-29"><a href="#cb68-29"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-30"><a href="#cb68-30"></a>    res <span class="op">=</span> res<span class="op">@</span>W2v4<span class="op">+</span>B2v4</span>
<span id="cb68-31"><a href="#cb68-31"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-32"><a href="#cb68-32"></a>    res <span class="op">=</span> res<span class="op">@</span>W3v4<span class="op">+</span>B3v4</span>
<span id="cb68-33"><a href="#cb68-33"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-34"><a href="#cb68-34"></a>    res <span class="op">=</span> res<span class="op">@</span>W4v4<span class="op">+</span>B4v4</span>
<span id="cb68-35"><a href="#cb68-35"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-36"><a href="#cb68-36"></a>    res <span class="op">=</span> res<span class="op">@</span>W5v4<span class="op">+</span>B5v4</span>
<span id="cb68-37"><a href="#cb68-37"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-38"><a href="#cb68-38"></a>    res <span class="op">=</span> res<span class="op">@</span>W6v4<span class="op">+</span>B6v4</span>
<span id="cb68-39"><a href="#cb68-39"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-40"><a href="#cb68-40"></a>    res <span class="op">=</span> res<span class="op">@</span>W7v4<span class="op">+</span>B7v4</span>
<span id="cb68-41"><a href="#cb68-41"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-42"><a href="#cb68-42"></a>    res <span class="op">=</span> res<span class="op">@</span>W8v4<span class="op">+</span>B8v4</span>
<span id="cb68-43"><a href="#cb68-43"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-44"><a href="#cb68-44"></a>    res <span class="op">=</span> res<span class="op">@</span>W9v4<span class="op">+</span>B9v4</span>
<span id="cb68-45"><a href="#cb68-45"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-46"><a href="#cb68-46"></a>    res <span class="op">=</span> res<span class="op">@</span>W10v4<span class="op">+</span>B10v4</span>
<span id="cb68-47"><a href="#cb68-47"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-48"><a href="#cb68-48"></a>    res <span class="op">=</span> res<span class="op">@</span>W11v4<span class="op">+</span>B11v4</span>
<span id="cb68-49"><a href="#cb68-49"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb68-50"><a href="#cb68-50"></a>    res <span class="op">=</span> res<span class="op">@</span>W12v4<span class="op">+</span>B12v4</span>
<span id="cb68-51"><a href="#cb68-51"></a>    <span class="cf">return</span> res</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And trying it…</p>
<div class="cell" data-execution_count="50">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV4, opt<span class="op">=</span>BasicOptim([W1v4, B1v4, W2v4, B2v4, W3v4, B3v4, W4v4, B4v4, W5v4, B5v4, W6v4, B6v4, W7v4, B7v4, W8v4, B8v4, W9v4, B9v4, W10v4, B10v4, W11v4, B11v4, W12v4, B12v4], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb69-2"><a href="#cb69-2"></a>                W1v4, B1v4, W2v4, B2v4, W3v4, B3v4, W4v4, B4v4, W5v4, B5v4, W6v4, B6v4, W7v4, B7v4, W8v4, B8v4, W9v4, B9v4, W10v4, B10v4, W11v4, B11v4, W12v4, B12v4])</span>
<span id="cb69-3"><a href="#cb69-3"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="fl">.1</span>)</span>
<span id="cb69-4"><a href="#cb69-4"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb69-5"><a href="#cb69-5"></a>plt.plot(my_try.valids)</span>
<span id="cb69-6"><a href="#cb69-6"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-51-output-1.png" width="575" height="414"></p>
</div>
</div>
<p>Curiously, absolute garbage performance. After investigating, my best estimate is that the reason this doesn’t perform well is that the larger number of layers leads to overblown activation values. but can’t be sure. I tried starting with much smaller initialziation values to no effect, as well well larger and smaller learning rates. We’ll leave this one as a lost cause.</p>


<!-- -->

</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>If it ain’t ‘py’, it ain’t python, right?<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Increasing the size of the bias vector so as to have a unique bias for each output neuron will be an opportunity to increase the capacity of the model.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>An epoch is a full pass through the entire data set, in a context where we will make multiple passes (epochs) so as to keep learning.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>The learning rate is the factor by which we will multiply the gradient when adding the negative gradient to each parameter so as to update it for the next learning iteration.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The changes to parameters can be so small that the actual outcome on any single run doesn’t change across a few inputs. But with enough learning iterations we will see the desired outcome.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>(Further reading)[https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/] on capacity.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p><em>“Tonight at 9, only on Fox!”</em><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb70" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "A Simple Digit Classifier"</span></span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "David De Sa"</span></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-03-23"</span></span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [python, pyTorch, NeuralNetworks]</span></span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "NN101_thumbnail.png"</span></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: true</span></span>
<span id="cb70-10"><a href="#cb70-10" aria-hidden="true" tabindex="-1"></a><span class="co">    toc: true</span></span>
<span id="cb70-11"><a href="#cb70-11" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb70-12"><a href="#cb70-12" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb70-13"><a href="#cb70-13" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb70-14"><a href="#cb70-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-15"><a href="#cb70-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## TL;DR</span></span>
<span id="cb70-16"><a href="#cb70-16" aria-hidden="true" tabindex="-1"></a>The best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! Part 2 of 2. I built this in following along with the awesome lessons over at <span class="co">[</span><span class="ot">fast.ai</span><span class="co">](course.fast.ai)</span>. Many thanks to that team.</span>
<span id="cb70-17"><a href="#cb70-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-18"><a href="#cb70-18" aria-hidden="true" tabindex="-1"></a><span class="fu"># Overview</span></span>
<span id="cb70-19"><a href="#cb70-19" aria-hidden="true" tabindex="-1"></a>We're going to jump in where we left off in Part 1: We'll just reimport our data, and get into it.</span>
<span id="cb70-20"><a href="#cb70-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-21"><a href="#cb70-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### What?</span></span>
<span id="cb70-22"><a href="#cb70-22" aria-hidden="true" tabindex="-1"></a>We're going to create a neural network that, given a picture of a numeric digit, identifies the number.</span>
<span id="cb70-23"><a href="#cb70-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-24"><a href="#cb70-24" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why?</span></span>
<span id="cb70-25"><a href="#cb70-25" aria-hidden="true" tabindex="-1"></a>Does this really need explaining? Because the technology is amazing in both practical and philosophical ways.</span>
<span id="cb70-26"><a href="#cb70-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-27"><a href="#cb70-27" aria-hidden="true" tabindex="-1"></a><span class="fu">### Who?</span></span>
<span id="cb70-28"><a href="#cb70-28" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Who am I</span><span class="co">](https://davidd003.github.io/Coding_Blog/about.html)</span>!? Who are you?!</span>
<span id="cb70-29"><a href="#cb70-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-30"><a href="#cb70-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### How?</span></span>
<span id="cb70-31"><a href="#cb70-31" aria-hidden="true" tabindex="-1"></a>Using <span class="co">[</span><span class="ot">PyTorch</span><span class="co">](https://pytorch.org/)</span>^<span class="co">[</span><span class="ot">If it ain't 'py', it ain't python, right?</span><span class="co">]</span>, an opensource toolkit for building neural networks. Truly an elevator ride up onto the shoulders of giants.</span>
<span id="cb70-32"><a href="#cb70-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-33"><a href="#cb70-33" aria-hidden="true" tabindex="-1"></a><span class="fu"># Code Review</span></span>
<span id="cb70-34"><a href="#cb70-34" aria-hidden="true" tabindex="-1"></a>We'll first just re-import our data, and then get into building the groundwork for our neural network</span>
<span id="cb70-35"><a href="#cb70-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-36"><a href="#cb70-36" aria-hidden="true" tabindex="-1"></a>Let's get into it!</span>
<span id="cb70-37"><a href="#cb70-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-38"><a href="#cb70-38" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb70-39"><a href="#cb70-39" aria-hidden="true" tabindex="-1"></a>The required dependencies!:<span class="in">`scikit-learn`</span>, <span class="in">`fastbook`</span>, <span class="in">`matplotlib`</span></span>
<span id="cb70-40"><a href="#cb70-40" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-41"><a href="#cb70-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-44"><a href="#cb70-44" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-45"><a href="#cb70-45" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-46"><a href="#cb70-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Install dependency</span></span>
<span id="cb70-47"><a href="#cb70-47" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastbook <span class="im">import</span> <span class="op">*</span></span>
<span id="cb70-48"><a href="#cb70-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> fastai.vision.<span class="bu">all</span> <span class="im">import</span> <span class="op">*</span></span>
<span id="cb70-49"><a href="#cb70-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> load_digits</span>
<span id="cb70-50"><a href="#cb70-50" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fastbook</span>
<span id="cb70-51"><a href="#cb70-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-52"><a href="#cb70-52" aria-hidden="true" tabindex="-1"></a>fastbook.setup_book()</span>
<span id="cb70-53"><a href="#cb70-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-54"><a href="#cb70-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-55"><a href="#cb70-55" aria-hidden="true" tabindex="-1"></a>matplotlib.rc(<span class="st">"image"</span>, cmap<span class="op">=</span><span class="st">"Greys"</span>)</span>
<span id="cb70-56"><a href="#cb70-56" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-57"><a href="#cb70-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Data Acquisition</span></span>
<span id="cb70-58"><a href="#cb70-58" aria-hidden="true" tabindex="-1"></a>A quick flashback to Part 1 of my Building A Digit Classifier blog!:</span>
<span id="cb70-59"><a href="#cb70-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-60"><a href="#cb70-60" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb70-61"><a href="#cb70-61" aria-hidden="true" tabindex="-1"></a><span class="fu">## Understand Your Input!</span></span>
<span id="cb70-62"><a href="#cb70-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-63"><a href="#cb70-63" aria-hidden="true" tabindex="-1"></a>Pre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the <span class="co">[</span><span class="ot">source page</span><span class="co">](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits)</span>: </span>
<span id="cb70-64"><a href="#cb70-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-65"><a href="#cb70-65" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.</span></span>
<span id="cb70-66"><a href="#cb70-66" aria-hidden="true" tabindex="-1"></a><span class="at">:::</span></span>
<span id="cb70-67"><a href="#cb70-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-70"><a href="#cb70-70" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-71"><a href="#cb70-71" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-72"><a href="#cb70-72" aria-hidden="true" tabindex="-1"></a>mnist <span class="op">=</span> load_digits()</span>
<span id="cb70-73"><a href="#cb70-73" aria-hidden="true" tabindex="-1"></a>stacked <span class="op">=</span> []</span>
<span id="cb70-74"><a href="#cb70-74" aria-hidden="true" tabindex="-1"></a><span class="co"># This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects</span></span>
<span id="cb70-75"><a href="#cb70-75" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb70-76"><a href="#cb70-76" aria-hidden="true" tabindex="-1"></a>    stacked.append([])</span>
<span id="cb70-77"><a href="#cb70-77" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign all images to the right collection in the 'stacked' list, indexed by target</span></span>
<span id="cb70-78"><a href="#cb70-78" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(mnist[<span class="st">"target"</span>])):</span>
<span id="cb70-79"><a href="#cb70-79" aria-hidden="true" tabindex="-1"></a>    stacked[mnist[<span class="st">"target"</span>][i]].append(mnist[<span class="st">"images"</span>][i])</span>
<span id="cb70-80"><a href="#cb70-80" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-81"><a href="#cb70-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-82"><a href="#cb70-82" aria-hidden="true" tabindex="-1"></a><span class="fu">### Segmentation</span></span>
<span id="cb70-83"><a href="#cb70-83" aria-hidden="true" tabindex="-1"></a>The next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn't generate a validation set with a large imbalance in the number of classes to be tested in it. </span>
<span id="cb70-84"><a href="#cb70-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-85"><a href="#cb70-85" aria-hidden="true" tabindex="-1"></a>First we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.</span>
<span id="cb70-86"><a href="#cb70-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-87"><a href="#cb70-87" aria-hidden="true" tabindex="-1"></a>We'll print out the size of these collections and take a peek at a sample to make sure we indexed right.</span>
<span id="cb70-88"><a href="#cb70-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-89"><a href="#cb70-89" aria-hidden="true" tabindex="-1"></a>:::{#fig-checkSamp}</span>
<span id="cb70-92"><a href="#cb70-92" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-93"><a href="#cb70-93" aria-hidden="true" tabindex="-1"></a><span class="co"># | warning: False</span></span>
<span id="cb70-94"><a href="#cb70-94" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-95"><a href="#cb70-95" aria-hidden="true" tabindex="-1"></a><span class="co"># To make dataset a tensor, make it same number of dimensions</span></span>
<span id="cb70-96"><a href="#cb70-96" aria-hidden="true" tabindex="-1"></a>stacked <span class="op">=</span> tensor([x[:<span class="dv">174</span>] <span class="cf">for</span> x <span class="kw">in</span> stacked])</span>
<span id="cb70-97"><a href="#cb70-97" aria-hidden="true" tabindex="-1"></a><span class="co"># Segmentation: Pull 20 of each digit out of training set</span></span>
<span id="cb70-98"><a href="#cb70-98" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> [dig[<span class="op">-</span><span class="dv">20</span>:] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb70-99"><a href="#cb70-99" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> [dig[:<span class="op">-</span><span class="dv">20</span>] <span class="cf">for</span> dig <span class="kw">in</span> stacked]</span>
<span id="cb70-100"><a href="#cb70-100" aria-hidden="true" tabindex="-1"></a><span class="co"># Confirm counts of samples</span></span>
<span id="cb70-101"><a href="#cb70-101" aria-hidden="true" tabindex="-1"></a>[<span class="bu">len</span>(test[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)], [<span class="bu">len</span>(train[i]) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb70-102"><a href="#cb70-102" aria-hidden="true" tabindex="-1"></a>show_image(stacked[<span class="dv">3</span>][<span class="dv">0</span>])  <span class="co"># Check sample</span></span>
<span id="cb70-103"><a href="#cb70-103" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-104"><a href="#cb70-104" aria-hidden="true" tabindex="-1"></a>Is it a 3?</span>
<span id="cb70-105"><a href="#cb70-105" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-106"><a href="#cb70-106" aria-hidden="true" tabindex="-1"></a>Nice.</span>
<span id="cb70-107"><a href="#cb70-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-108"><a href="#cb70-108" aria-hidden="true" tabindex="-1"></a>It's important to keep track of what's what.</span>
<span id="cb70-111"><a href="#cb70-111" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-112"><a href="#cb70-112" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-113"><a href="#cb70-113" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-114"><a href="#cb70-114" aria-hidden="true" tabindex="-1"></a><span class="bu">type</span>(train), <span class="bu">type</span>(train[<span class="dv">0</span>]), <span class="bu">type</span>(train[<span class="dv">0</span>][<span class="dv">0</span>]), [</span>
<span id="cb70-115"><a href="#cb70-115" aria-hidden="true" tabindex="-1"></a>    <span class="bu">type</span>(test), <span class="bu">type</span>(test[<span class="dv">0</span>]), <span class="bu">type</span>(test[<span class="dv">0</span>][<span class="dv">0</span>])]</span>
<span id="cb70-116"><a href="#cb70-116" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-117"><a href="#cb70-117" aria-hidden="true" tabindex="-1"></a>Ok so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.</span>
<span id="cb70-118"><a href="#cb70-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-119"><a href="#cb70-119" aria-hidden="true" tabindex="-1"></a>Instead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it's an easy conversion:</span>
<span id="cb70-122"><a href="#cb70-122" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-123"><a href="#cb70-123" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-124"><a href="#cb70-124" aria-hidden="true" tabindex="-1"></a><span class="co"># Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)</span></span>
<span id="cb70-125"><a href="#cb70-125" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> torch.stack(train)</span>
<span id="cb70-126"><a href="#cb70-126" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> torch.stack(test)</span>
<span id="cb70-127"><a href="#cb70-127" aria-hidden="true" tabindex="-1"></a>train.shape, test.shape</span>
<span id="cb70-128"><a href="#cb70-128" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-129"><a href="#cb70-129" aria-hidden="true" tabindex="-1"></a>Now here is a critical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size. </span>
<span id="cb70-130"><a href="#cb70-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-131"><a href="#cb70-131" aria-hidden="true" tabindex="-1"></a><span class="fu">### Setup DataLoaders</span></span>
<span id="cb70-132"><a href="#cb70-132" aria-hidden="true" tabindex="-1"></a>First get data into the requisite shape for the processes that will follow. </span>
<span id="cb70-135"><a href="#cb70-135" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-136"><a href="#cb70-136" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-137"><a href="#cb70-137" aria-hidden="true" tabindex="-1"></a><span class="co"># Build training/valid sets for passing to DataLoader</span></span>
<span id="cb70-138"><a href="#cb70-138" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> train]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb70-139"><a href="#cb70-139" aria-hidden="true" tabindex="-1"></a>test_x <span class="op">=</span> torch.cat([x <span class="cf">for</span> x <span class="kw">in</span> test]).view(<span class="op">-</span><span class="dv">1</span>, <span class="dv">8</span><span class="op">*</span><span class="dv">8</span>)</span>
<span id="cb70-140"><a href="#cb70-140" aria-hidden="true" tabindex="-1"></a>train_y, test_y <span class="op">=</span> [], []</span>
<span id="cb70-141"><a href="#cb70-141" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb70-142"><a href="#cb70-142" aria-hidden="true" tabindex="-1"></a>    train_y.extend([n]<span class="op">*</span><span class="dv">154</span>)</span>
<span id="cb70-143"><a href="#cb70-143" aria-hidden="true" tabindex="-1"></a>    test_y.extend([n]<span class="op">*</span><span class="dv">20</span>)</span>
<span id="cb70-144"><a href="#cb70-144" aria-hidden="true" tabindex="-1"></a>train_y <span class="op">=</span> tensor(train_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb70-145"><a href="#cb70-145" aria-hidden="true" tabindex="-1"></a>test_y <span class="op">=</span> tensor(test_y).unsqueeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb70-146"><a href="#cb70-146" aria-hidden="true" tabindex="-1"></a>train_x.shape, test_x.shape, train_y.shape, test_y.shape</span>
<span id="cb70-147"><a href="#cb70-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-148"><a href="#cb70-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-149"><a href="#cb70-149" aria-hidden="true" tabindex="-1"></a>Note the unsqueezing such that the <span class="in">`_y`</span> tensors have a singleton dimension. Might seem strange on first blush but makes sense when considering that our 'target' just happens to be individual integers. But in other use cases, the target could be something more complex, requiring a larger data structure. In which case that dimension could be larger than a singleton.</span>
<span id="cb70-150"><a href="#cb70-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-151"><a href="#cb70-151" aria-hidden="true" tabindex="-1"></a>Now we can prime our DataLoaders. DataLoaders are a super handy object from the Pytorch library that will make life easier by holding our data for us, and serving it up in randomized batches.</span>
<span id="cb70-152"><a href="#cb70-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-155"><a href="#cb70-155" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-156"><a href="#cb70-156" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-157"><a href="#cb70-157" aria-hidden="true" tabindex="-1"></a><span class="co"># Zip each input data item to its target output</span></span>
<span id="cb70-158"><a href="#cb70-158" aria-hidden="true" tabindex="-1"></a>dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(train_x, train_y))</span>
<span id="cb70-159"><a href="#cb70-159" aria-hidden="true" tabindex="-1"></a>valid_dset <span class="op">=</span> <span class="bu">list</span>(<span class="bu">zip</span>(test_x, test_y))</span>
<span id="cb70-160"><a href="#cb70-160" aria-hidden="true" tabindex="-1"></a><span class="co"># Define DataLoader objects to pass to learner</span></span>
<span id="cb70-161"><a href="#cb70-161" aria-hidden="true" tabindex="-1"></a>dl <span class="op">=</span> DataLoader(dset, batch_size<span class="op">=</span><span class="dv">25</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb70-162"><a href="#cb70-162" aria-hidden="true" tabindex="-1"></a>valid_dl <span class="op">=</span> DataLoader(valid_dset, batch_size<span class="op">=</span><span class="dv">5</span>, shuffle<span class="op">=</span>true)</span>
<span id="cb70-163"><a href="#cb70-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-164"><a href="#cb70-164" aria-hidden="true" tabindex="-1"></a>dls <span class="op">=</span> DataLoaders(dl, valid_dl)</span>
<span id="cb70-165"><a href="#cb70-165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-166"><a href="#cb70-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-167"><a href="#cb70-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Laying The NN Foundations</span></span>
<span id="cb70-168"><a href="#cb70-168" aria-hidden="true" tabindex="-1"></a>Every NN needs weights and biases. We're going to want to initialize lots of them, and in differently shaped structures, so first we define a function to help us do that:</span>
<span id="cb70-169"><a href="#cb70-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-172"><a href="#cb70-172" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-173"><a href="#cb70-173" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-174"><a href="#cb70-174" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_params(size, std<span class="op">=</span><span class="fl">.1</span>): <span class="cf">return</span> (torch.randn(size)<span class="op">*</span>std).requires_grad_()</span>
<span id="cb70-175"><a href="#cb70-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-176"><a href="#cb70-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-177"><a href="#cb70-177" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-178"><a href="#cb70-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-179"><a href="#cb70-179" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb70-180"><a href="#cb70-180" aria-hidden="true" tabindex="-1"></a><span class="fu">## Requiring Grad!</span></span>
<span id="cb70-181"><a href="#cb70-181" aria-hidden="true" tabindex="-1"></a>Note that any memory structures created with that function will also store their gradient in memory as computation is performed on them. This will be critical to executing back propagation of the gradient so as to optimize our function.</span>
<span id="cb70-182"><a href="#cb70-182" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-183"><a href="#cb70-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-184"><a href="#cb70-184" aria-hidden="true" tabindex="-1"></a>Now we can generate our weights and biases. We define the structure sizes with our input and output size in mind, and the principles of matrix multiplication. </span>
<span id="cb70-185"><a href="#cb70-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-186"><a href="#cb70-186" aria-hidden="true" tabindex="-1"></a>The general structure we will start with will be, of course, an initial layer with one neuron for each input, which will yield a single activation per input. After a nonlinearty, the second layer will have 10 weights and biases, such that the output is a 1D vector of size 10, per input. Each output neuron will represent an estimation of likelihood that the integer is that which the neuron represents (0-9).</span>
<span id="cb70-187"><a href="#cb70-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-188"><a href="#cb70-188" aria-hidden="true" tabindex="-1"></a>@fig-Layer1 shows the start of the proces... A row in the input batch represents a single image. At the end of the 1^st^ layer, it is reduced to a single activation value to be passed through a non-linearity prior to going through the 2^nd^ layer</span>
<span id="cb70-189"><a href="#cb70-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-190"><a href="#cb70-190" aria-hidden="true" tabindex="-1"></a><span class="al">![Visualizing Layer 1](Layer 1 Diagram.png)</span>{#fig-Layer1}  </span>
<span id="cb70-191"><a href="#cb70-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-192"><a href="#cb70-192" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip }</span>
<span id="cb70-193"><a href="#cb70-193" aria-hidden="true" tabindex="-1"></a><span class="fu">## Matrix Multiplication!</span></span>
<span id="cb70-194"><a href="#cb70-194" aria-hidden="true" tabindex="-1"></a>Never fear! It's easy. I never forgot the trick Dr. Mohammed Jaber taught us 4th year as we had to use these methods from first year: ROW. COLUMN.</span>
<span id="cb70-195"><a href="#cb70-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-196"><a href="#cb70-196" aria-hidden="true" tabindex="-1"></a>Take row i from the first matrix, and column j from the second. The element in index i,j of the resulting matrix will be the dot product of that row and column vector.</span>
<span id="cb70-197"><a href="#cb70-197" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-198"><a href="#cb70-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-201"><a href="#cb70-201" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-202"><a href="#cb70-202" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-203"><a href="#cb70-203" aria-hidden="true" tabindex="-1"></a><span class="co"># Have to provide extra dimension for matrix mutiplication, making it a single-column vector</span></span>
<span id="cb70-204"><a href="#cb70-204" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb70-205"><a href="#cb70-205" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-206"><a href="#cb70-206" aria-hidden="true" tabindex="-1"></a><span class="co"># Have to provide an extra dimension (1) for matrix multiplication</span></span>
<span id="cb70-207"><a href="#cb70-207" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb70-208"><a href="#cb70-208" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-209"><a href="#cb70-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-210"><a href="#cb70-210" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Layer 1:</span>
<span id="cb70-211"><a href="#cb70-211" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>64 weights, one per pixel. Formatted in a column vector because they are the second term in the matrix multiplication equation, with a single input (image) being fed as a single row vector of 64 elements, one per pixel. </span>
<span id="cb70-212"><a href="#cb70-212" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>1 bias; it will be broadcast such that the same bias is added to all instances in our batch. See @fig-Layer1</span>
<span id="cb70-213"><a href="#cb70-213" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Layer 2:</span>
<span id="cb70-214"><a href="#cb70-214" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>10 weights, to generate one output neuron per category/class to assign. </span>
<span id="cb70-215"><a href="#cb70-215" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>1 bias, to be *broadcast* across each input vector.^<span class="co">[</span><span class="ot">Increasing the size of the bias vector so as to have a unique bias for each output neuron will be an opportunity to increase the capacity of the model.</span><span class="co">]</span></span>
<span id="cb70-216"><a href="#cb70-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-217"><a href="#cb70-217" aria-hidden="true" tabindex="-1"></a>We will use the basic <span class="co">[</span><span class="ot">ReLU</span><span class="co">](https://deepai.org/machine-learning-glossary-and-terms/relu)</span> activation function as the non-linearity between the two linear layers.</span>
<span id="cb70-218"><a href="#cb70-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-219"><a href="#cb70-219" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building the NN</span></span>
<span id="cb70-220"><a href="#cb70-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-221"><a href="#cb70-221" aria-hidden="true" tabindex="-1"></a>Similarly to our parameter generation function, we want o make our little network into a function block so that we can treat it as a black box to plug and play with other code sections:</span>
<span id="cb70-224"><a href="#cb70-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-225"><a href="#cb70-225" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-226"><a href="#cb70-226" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-227"><a href="#cb70-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-228"><a href="#cb70-228" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-229"><a href="#cb70-229" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> myModel(xb):</span>
<span id="cb70-230"><a href="#cb70-230" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>w1<span class="op">+</span>b1</span>
<span id="cb70-231"><a href="#cb70-231" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-232"><a href="#cb70-232" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>w2<span class="op">+</span>b2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb70-233"><a href="#cb70-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb70-234"><a href="#cb70-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-235"><a href="#cb70-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-236"><a href="#cb70-236" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-237"><a href="#cb70-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-238"><a href="#cb70-238" aria-hidden="true" tabindex="-1"></a>Let's take it for a spin! We'll manually identify a subset of our training dataset for testing purposes.</span>
<span id="cb70-241"><a href="#cb70-241" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-242"><a href="#cb70-242" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-243"><a href="#cb70-243" aria-hidden="true" tabindex="-1"></a>mini_samp <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb70-244"><a href="#cb70-244" aria-hidden="true" tabindex="-1"></a>mini_x <span class="op">=</span> train_x[:mini_samp]</span>
<span id="cb70-245"><a href="#cb70-245" aria-hidden="true" tabindex="-1"></a>mini_y <span class="op">=</span> train_y[:mini_samp]</span>
<span id="cb70-246"><a href="#cb70-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-249"><a href="#cb70-249" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-250"><a href="#cb70-250" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-251"><a href="#cb70-251" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-252"><a href="#cb70-252" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> myModel(mini_x)</span>
<span id="cb70-253"><a href="#cb70-253" aria-hidden="true" tabindex="-1"></a>res, res.shape</span>
<span id="cb70-254"><a href="#cb70-254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-255"><a href="#cb70-255" aria-hidden="true" tabindex="-1"></a>Excellent! For the 5 input images passed in, we have an output that is 5 vectors of size 10, that is, each input has 10 output values, one for each possible classification outcome 0 through 9. </span>
<span id="cb70-256"><a href="#cb70-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-257"><a href="#cb70-257" aria-hidden="true" tabindex="-1"></a>And note that the result tensor retains the gradient! This comes into play in the next step.</span>
<span id="cb70-258"><a href="#cb70-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-259"><a href="#cb70-259" aria-hidden="true" tabindex="-1"></a><span class="fu">## Calculating Loss</span></span>
<span id="cb70-260"><a href="#cb70-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-261"><a href="#cb70-261" aria-hidden="true" tabindex="-1"></a>Loss is the measure of the neural networks success. The most critical characteristic of a loss function is that it be differentiable with a smooth gradient. But just because the math doesn't break doesn't mean the function will eb any good. There are a variety of well understood functions that can be used, with selection being a matter of preference as well as the use case at hand.</span>
<span id="cb70-262"><a href="#cb70-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-263"><a href="#cb70-263" aria-hidden="true" tabindex="-1"></a>Since we are assigning a single class to the input, from multiple options, we'll use the SoftMax function. It reduces all outputs to a number between 0 and 1, with a given number being exponentially closer to one, the larger it is compared to the other numbers in the set. </span>
<span id="cb70-264"><a href="#cb70-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-265"><a href="#cb70-265" aria-hidden="true" tabindex="-1"></a>This is a double-hitter: it allows the net to learn to win by giving the correct digit's output neuron the highest activation, while also motivating the reduction in activation of the incorrect neurons. </span>
<span id="cb70-266"><a href="#cb70-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-269"><a href="#cb70-269" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-270"><a href="#cb70-270" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-271"><a href="#cb70-271" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-272"><a href="#cb70-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-273"><a href="#cb70-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-274"><a href="#cb70-274" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_loss(preds, target, lr<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb70-275"><a href="#cb70-275" aria-hidden="true" tabindex="-1"></a>    maxes <span class="op">=</span> F.softmax(preds, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb70-276"><a href="#cb70-276" aria-hidden="true" tabindex="-1"></a>    correctIndices <span class="op">=</span> [r[t] <span class="cf">for</span> r, t <span class="kw">in</span> <span class="bu">zip</span>(maxes, target)]</span>
<span id="cb70-277"><a href="#cb70-277" aria-hidden="true" tabindex="-1"></a>    resy <span class="op">=</span> [<span class="op">-</span>torch.log(tens) <span class="cf">for</span> tens <span class="kw">in</span> correctIndices]</span>
<span id="cb70-278"><a href="#cb70-278" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(resy)</span>
<span id="cb70-279"><a href="#cb70-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-280"><a href="#cb70-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-281"><a href="#cb70-281" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-282"><a href="#cb70-282" aria-hidden="true" tabindex="-1"></a>Keep in mind that we need to ensure these functions can work with different batch sizes. Fortunately, the PyTorch built in functions are designed for this. </span>
<span id="cb70-283"><a href="#cb70-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-284"><a href="#cb70-284" aria-hidden="true" tabindex="-1"></a>Our function first takes the SoftMax of the outputs for each input. Then, it pulls out the resulting value for the index of the correct class. Recall, this will be a number between 0 and 1. If we took this number alone as our loss, measure, there would be a couple of issues.</span>
<span id="cb70-285"><a href="#cb70-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-286"><a href="#cb70-286" aria-hidden="true" tabindex="-1"></a>First of all, convention is that we want to *reduce* loss in the course of our optimization. But we have defined the index of the *highest* softmax result as our classification integer. So that doesn't jive. Secondly, it isn't desirable to have the measure of loss constrained to such a small range 0 to 1, because this essentially reduces the information we're getting - large differences in inputs will show very small changes in loss. For effective learning, we would want a strong learning signal, i.e. small changes to input parameters reflecting meaningful differences in the measure of loss.</span>
<span id="cb70-287"><a href="#cb70-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-288"><a href="#cb70-288" aria-hidden="true" tabindex="-1"></a>The solution? First, taking the log of the SoftMax activation. As the following code shows, a difference of 0.001 in the input to the log function creates a differences of more than 0.01 in the output, an order of magnitude difference.</span>
<span id="cb70-289"><a href="#cb70-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-292"><a href="#cb70-292" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-293"><a href="#cb70-293" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-294"><a href="#cb70-294" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-295"><a href="#cb70-295" aria-hidden="true" tabindex="-1"></a>torch.log(Tensor([<span class="fl">.5</span>, <span class="fl">.499</span>]))</span>
<span id="cb70-296"><a href="#cb70-296" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-297"><a href="#cb70-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-298"><a href="#cb70-298" aria-hidden="true" tabindex="-1"></a>After the log, we took the negative of those values. That's because the log of a number between 0 and 1 will always be negative, getting absolutely larger the further from 1 it is. Seeing as we want the output to be absolutely smaller the closer to 1 it is, taking the negative accomplishes this perfectly.</span>
<span id="cb70-299"><a href="#cb70-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-300"><a href="#cb70-300" aria-hidden="true" tabindex="-1"></a>Lastly, we summed the loss measure across the samples in the mini batch since we want to optimize globally, not just for individual samples.</span>
<span id="cb70-301"><a href="#cb70-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-302"><a href="#cb70-302" aria-hidden="true" tabindex="-1"></a>So let's test it!</span>
<span id="cb70-303"><a href="#cb70-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-306"><a href="#cb70-306" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-307"><a href="#cb70-307" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-308"><a href="#cb70-308" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing the loss function works(assuming mini_x of 3 samples)</span></span>
<span id="cb70-309"><a href="#cb70-309" aria-hidden="true" tabindex="-1"></a>lossResults <span class="op">=</span> my_loss(myModel(mini_x), mini_y)</span>
<span id="cb70-310"><a href="#cb70-310" aria-hidden="true" tabindex="-1"></a>lossResults</span>
<span id="cb70-311"><a href="#cb70-311" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-312"><a href="#cb70-312" aria-hidden="true" tabindex="-1"></a>This output might look nice, but it's a roll of the dice!</span>
<span id="cb70-313"><a href="#cb70-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-314"><a href="#cb70-314" aria-hidden="true" tabindex="-1"></a>::: {.callout-warning}</span>
<span id="cb70-315"><a href="#cb70-315" aria-hidden="true" tabindex="-1"></a><span class="fu">## Beware Of Implementing Your Own Math</span></span>
<span id="cb70-316"><a href="#cb70-316" aria-hidden="true" tabindex="-1"></a>While the formula for the SoftMax is easy enough to understand, Our home-brewed version won't work. I tried to do so at first and had all kinds of headaches arising from arithmetic underflow in the outcomes! How the hell do you deal with that? The best solution: Use the function implementation baked into the PyTorch libraries! Let the pro's handle those *deep* issues. We're still riding on training wheels!</span>
<span id="cb70-317"><a href="#cb70-317" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-318"><a href="#cb70-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-319"><a href="#cb70-319" aria-hidden="true" tabindex="-1"></a>It turns out the <span class="in">`-log`</span> of the SoftMax is the Cross Entropy Loss Function. And just importing it from the PyTorch library will save us from the pitfall described in the preceding blurb... So here is our final loss function!</span>
<span id="cb70-322"><a href="#cb70-322" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-323"><a href="#cb70-323" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-324"><a href="#cb70-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-325"><a href="#cb70-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-326"><a href="#cb70-326" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_loss(preds, target):</span>
<span id="cb70-327"><a href="#cb70-327" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb70-328"><a href="#cb70-328" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Turn tensor of singleton tensors one per target into a single tensor with all as elements</span></span>
<span id="cb70-329"><a href="#cb70-329" aria-hidden="true" tabindex="-1"></a>    tgts <span class="op">=</span> target.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb70-330"><a href="#cb70-330" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_fn(preds, tgts)  <span class="co"># Nice</span></span>
<span id="cb70-331"><a href="#cb70-331" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-332"><a href="#cb70-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-333"><a href="#cb70-333" aria-hidden="true" tabindex="-1"></a>Now if you're reading this you're probably thinking that *gradient descent* is the next step here, so as to reduce the loss between minibatches, but we'll take a quick detour first.</span>
<span id="cb70-334"><a href="#cb70-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-335"><a href="#cb70-335" aria-hidden="true" tabindex="-1"></a><span class="fu">## Measuring Accuracy</span></span>
<span id="cb70-336"><a href="#cb70-336" aria-hidden="true" tabindex="-1"></a>Before going further, let's take a minute to set up some functions we'll need in the training process. The first yields the percentage accuracy of the model across a batch, and the second function builds on that to iterate across the batchers in our valdiation set and yield the accuracy rate across a whole epoch^<span class="co">[</span><span class="ot">An epoch is a full pass through the entire data set, in a context where we will make multiple passes (epochs) so as to keep learning.</span><span class="co">]</span>.</span>
<span id="cb70-339"><a href="#cb70-339" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-340"><a href="#cb70-340" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-341"><a href="#cb70-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-342"><a href="#cb70-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-343"><a href="#cb70-343" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> batch_accuracy(mdl, xb, yb):</span>
<span id="cb70-344"><a href="#cb70-344" aria-hidden="true" tabindex="-1"></a>    otpts <span class="op">=</span> mdl(xb)  <span class="co"># Get output activations from model</span></span>
<span id="cb70-345"><a href="#cb70-345" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The indices of the max activation is the predicted digit of the input</span></span>
<span id="cb70-346"><a href="#cb70-346" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> otpts.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb70-347"><a href="#cb70-347" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Types must be tensors to return sequence of true/false</span></span>
<span id="cb70-348"><a href="#cb70-348" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> preds <span class="op">==</span> yb.view(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb70-349"><a href="#cb70-349" aria-hidden="true" tabindex="-1"></a>    <span class="co">#                    Use view to take it from shape=[5,1] to [5], same as preds. else will broadcast and end result all messed up</span></span>
<span id="cb70-350"><a href="#cb70-350" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> correct.<span class="bu">float</span>().mean()</span>
<span id="cb70-351"><a href="#cb70-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-352"><a href="#cb70-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-353"><a href="#cb70-353" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> validate_epoch(mdl):</span>
<span id="cb70-354"><a href="#cb70-354" aria-hidden="true" tabindex="-1"></a>    outcomes <span class="op">=</span> [batch_accuracy(mdl, xb, yb) <span class="cf">for</span> xb, yb <span class="kw">in</span> valid_dl]</span>
<span id="cb70-355"><a href="#cb70-355" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">round</span>(torch.stack(outcomes).mean().item(), <span class="dv">4</span>)</span>
<span id="cb70-356"><a href="#cb70-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-357"><a href="#cb70-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-358"><a href="#cb70-358" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-359"><a href="#cb70-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-360"><a href="#cb70-360" aria-hidden="true" tabindex="-1"></a>It's always good to double check these functions are working as intended after making them... Let's grab some validation set data, double checking along the way that the outputs (integer assignment) match the input (drawings of digit):</span>
<span id="cb70-363"><a href="#cb70-363" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-364"><a href="#cb70-364" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-365"><a href="#cb70-365" aria-hidden="true" tabindex="-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb70-366"><a href="#cb70-366" aria-hidden="true" tabindex="-1"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb70-367"><a href="#cb70-367" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb70-368"><a href="#cb70-368" aria-hidden="true" tabindex="-1"></a>    show_image(xv[i].view((<span class="dv">8</span>, <span class="dv">8</span>)))</span>
<span id="cb70-369"><a href="#cb70-369" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(yv[i].data.item())</span>
<span id="cb70-370"><a href="#cb70-370" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-371"><a href="#cb70-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-372"><a href="#cb70-372" aria-hidden="true" tabindex="-1"></a>Ok nice, now lets see that our validation function is yielding the correct accuracy score on classifying a mini batch. Since we haven't trained the model yet we'll expect the performacne to be junk but that doesn't mean we can't test the % accuracy function.</span>
<span id="cb70-375"><a href="#cb70-375" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-376"><a href="#cb70-376" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-377"><a href="#cb70-377" aria-hidden="true" tabindex="-1"></a>validIter <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">1</span>])</span>
<span id="cb70-378"><a href="#cb70-378" aria-hidden="true" tabindex="-1"></a>xv, yv <span class="op">=</span> <span class="bu">next</span>(validIter)</span>
<span id="cb70-379"><a href="#cb70-379" aria-hidden="true" tabindex="-1"></a>o <span class="op">=</span> myModel(xv)</span>
<span id="cb70-380"><a href="#cb70-380" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(o.<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices, yv, batch_accuracy(myModel, xv, yv))</span>
<span id="cb70-381"><a href="#cb70-381" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-382"><a href="#cb70-382" aria-hidden="true" tabindex="-1"></a>We want to see the last number spit out be an accurate representation of how many numbers match between the first and second tensor output. I don't know what the final blog output will be since the inputs to the neural net are random every time, but I re ran this code cell a number of times, seeing correct assignments of one or two integers to confirm the functions were working as expected.</span>
<span id="cb70-383"><a href="#cb70-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-384"><a href="#cb70-384" aria-hidden="true" tabindex="-1"></a><span class="fu">## Execute Training</span></span>
<span id="cb70-385"><a href="#cb70-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-386"><a href="#cb70-386" aria-hidden="true" tabindex="-1"></a>Alright now we get to the good stuff! First we'll make an iterator to load our test dataset.</span>
<span id="cb70-389"><a href="#cb70-389" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-390"><a href="#cb70-390" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-391"><a href="#cb70-391" aria-hidden="true" tabindex="-1"></a>testLoader <span class="op">=</span> <span class="bu">iter</span>(dls[<span class="dv">0</span>])</span>
<span id="cb70-392"><a href="#cb70-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-393"><a href="#cb70-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-394"><a href="#cb70-394" aria-hidden="true" tabindex="-1"></a>And now we we will run our first training loop... the steps are:</span>
<span id="cb70-395"><a href="#cb70-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-396"><a href="#cb70-396" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Define learning rate^<span class="co">[</span><span class="ot">The learning rate is the factor by which we will multiply the gradient when adding the negative gradient to each parameter so as to update it for the next learning iteration.</span><span class="co">]</span>.</span>
<span id="cb70-397"><a href="#cb70-397" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Pull out a minibatch of inputs and target values from the test data set.</span>
<span id="cb70-398"><a href="#cb70-398" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Calculate the loss on this batch by passing the outputs from the model through the loss function</span>
<span id="cb70-399"><a href="#cb70-399" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Execute the <span class="in">`.backward()`</span> method to calculate the gradient for all parameters.</span>
<span id="cb70-400"><a href="#cb70-400" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The next bits will be executed with torch.no_grad(), because we don't want the math inherent to calibrating the parameters themselves to have its gradient captured.</span>
<span id="cb70-401"><a href="#cb70-401" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Having the gradient stored in the tensor object of each parameter, update each parameter by subtracting the gadient multiplied by the learning rate.</span>
<span id="cb70-402"><a href="#cb70-402" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Reset the gradient to zero for the next learning iteration.</span>
<span id="cb70-403"><a href="#cb70-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-404"><a href="#cb70-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-407"><a href="#cb70-407" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-408"><a href="#cb70-408" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-409"><a href="#cb70-409" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-410"><a href="#cb70-410" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb70-411"><a href="#cb70-411" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb70-412"><a href="#cb70-412" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb70-413"><a href="#cb70-413" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb70-414"><a href="#cb70-414" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-415"><a href="#cb70-415" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-416"><a href="#cb70-416" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb70-417"><a href="#cb70-417" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb70-418"><a href="#cb70-418" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb70-419"><a href="#cb70-419" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb70-420"><a href="#cb70-420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-421"><a href="#cb70-421" aria-hidden="true" tabindex="-1"></a>Success! Our measure of loss has reduced after re running the same batch with the adjusted N parameters. Across a large enough dataset^<span class="co">[</span><span class="ot">The changes to parameters can be so small that the actual outcome on any single run doesn't change across a few inputs. But with enough learning iterations we will see the desired outcome.</span><span class="co">]</span>, we would expect to see a reduced error rate associated with this.</span>
<span id="cb70-422"><a href="#cb70-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-423"><a href="#cb70-423" aria-hidden="true" tabindex="-1"></a>Let's try it again, this time printing out the average values of the parameters in the weights and bias vectors, to get a sense for how much they change from one iteration to the next.</span>
<span id="cb70-424"><a href="#cb70-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-427"><a href="#cb70-427" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-428"><a href="#cb70-428" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb70-429"><a href="#cb70-429" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-430"><a href="#cb70-430" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb70-431"><a href="#cb70-431" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb70-432"><a href="#cb70-432" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-433"><a href="#cb70-433" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-434"><a href="#cb70-434" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb70-435"><a href="#cb70-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-436"><a href="#cb70-436" aria-hidden="true" tabindex="-1"></a><span class="co"># Run the learning (copied from last cell):</span></span>
<span id="cb70-437"><a href="#cb70-437" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb70-438"><a href="#cb70-438" aria-hidden="true" tabindex="-1"></a>xb, yb <span class="op">=</span> <span class="bu">next</span>(testLoader)</span>
<span id="cb70-439"><a href="#cb70-439" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb70-440"><a href="#cb70-440" aria-hidden="true" tabindex="-1"></a>loss.backward()</span>
<span id="cb70-441"><a href="#cb70-441" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-442"><a href="#cb70-442" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-443"><a href="#cb70-443" aria-hidden="true" tabindex="-1"></a>        p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb70-444"><a href="#cb70-444" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb70-445"><a href="#cb70-445" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Loss before training: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()) <span class="op">+</span></span>
<span id="cb70-446"><a href="#cb70-446" aria-hidden="true" tabindex="-1"></a>      <span class="st">"  |   Loss after training: "</span><span class="op">+</span><span class="bu">str</span>(my_loss(myModel(xb), yb).item()))</span>
<span id="cb70-447"><a href="#cb70-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-448"><a href="#cb70-448" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb70-449"><a href="#cb70-449" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb70-450"><a href="#cb70-450" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-451"><a href="#cb70-451" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb70-452"><a href="#cb70-452" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb70-453"><a href="#cb70-453" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-454"><a href="#cb70-454" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-455"><a href="#cb70-455" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb70-456"><a href="#cb70-456" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-457"><a href="#cb70-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-458"><a href="#cb70-458" aria-hidden="true" tabindex="-1"></a>What we see here is that our change in paramters created a reduction in the measure of loss, *even though* it wasn't even identifiable at 4 decimal points accuracy!</span>
<span id="cb70-459"><a href="#cb70-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-460"><a href="#cb70-460" aria-hidden="true" tabindex="-1"></a>So let's take it to the next level. We trained on one minibatch of data. Let's try doing a whole epoch- iterating over every minibatch in the training set.</span>
<span id="cb70-461"><a href="#cb70-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-462"><a href="#cb70-462" aria-hidden="true" tabindex="-1"></a>First, we'll reset our weights to random numbers, clearing the slate of that last learning:</span>
<span id="cb70-463"><a href="#cb70-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-466"><a href="#cb70-466" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-467"><a href="#cb70-467" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-468"><a href="#cb70-468" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb70-469"><a href="#cb70-469" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-470"><a href="#cb70-470" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb70-471"><a href="#cb70-471" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-472"><a href="#cb70-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-473"><a href="#cb70-473" aria-hidden="true" tabindex="-1"></a><span class="co"># Print average param values again:</span></span>
<span id="cb70-474"><a href="#cb70-474" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">type</span>(w1.grad) <span class="op">==</span> NoneType:</span>
<span id="cb70-475"><a href="#cb70-475" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(tensor([(x.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-476"><a href="#cb70-476" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb70-477"><a href="#cb70-477" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Average parameters: "</span>, tensor(</span>
<span id="cb70-478"><a href="#cb70-478" aria-hidden="true" tabindex="-1"></a>        [(x.data.mean(), x.grad.data.mean()) <span class="cf">for</span> x <span class="kw">in</span> [w1, b1, w2, b2]]))</span>
<span id="cb70-479"><a href="#cb70-479" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-480"><a href="#cb70-480" aria-hidden="true" tabindex="-1"></a>        p.grad.zero_()</span>
<span id="cb70-481"><a href="#cb70-481" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-482"><a href="#cb70-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-483"><a href="#cb70-483" aria-hidden="true" tabindex="-1"></a>And now execute learning.</span>
<span id="cb70-486"><a href="#cb70-486" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-487"><a href="#cb70-487" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-488"><a href="#cb70-488" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-489"><a href="#cb70-489" aria-hidden="true" tabindex="-1"></a>lr <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb70-490"><a href="#cb70-490" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb70-491"><a href="#cb70-491" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> my_loss(myModel(xb), yb)</span>
<span id="cb70-492"><a href="#cb70-492" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb70-493"><a href="#cb70-493" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-494"><a href="#cb70-494" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-495"><a href="#cb70-495" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb70-496"><a href="#cb70-496" aria-hidden="true" tabindex="-1"></a>            p.grad.zero_()</span>
<span id="cb70-497"><a href="#cb70-497" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Loss: "</span><span class="op">+</span><span class="bu">str</span>(loss.item()))  <span class="co"># Prints loss for each minibatch</span></span>
<span id="cb70-498"><a href="#cb70-498" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-499"><a href="#cb70-499" aria-hidden="true" tabindex="-1"></a>Hmmm.... sometimes when I run this I see the loss stay around the same value, other times it drops just a little before stabilizing... </span>
<span id="cb70-500"><a href="#cb70-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-501"><a href="#cb70-501" aria-hidden="true" tabindex="-1"></a>We are going to need a better way to look at the trending of the loss function across iterations, so we may as well make that function now:</span>
<span id="cb70-502"><a href="#cb70-502" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-505"><a href="#cb70-505" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-506"><a href="#cb70-506" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-507"><a href="#cb70-507" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-508"><a href="#cb70-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-509"><a href="#cb70-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-510"><a href="#cb70-510" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_grad(xb, yb, model, f_loss):</span>
<span id="cb70-511"><a href="#cb70-511" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> model(xb)</span>
<span id="cb70-512"><a href="#cb70-512" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> f_loss(preds, yb)</span>
<span id="cb70-513"><a href="#cb70-513" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb70-514"><a href="#cb70-514" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss.data.item()  <span class="co"># Return the loss (see why later)</span></span>
<span id="cb70-515"><a href="#cb70-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-516"><a href="#cb70-516" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-517"><a href="#cb70-517" aria-hidden="true" tabindex="-1"></a>calc_grad(mini_x, mini_y, myModel, my_loss)</span>
<span id="cb70-518"><a href="#cb70-518" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-519"><a href="#cb70-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-520"><a href="#cb70-520" aria-hidden="true" tabindex="-1"></a>This is another step towards modularizing (if that is a word) our code. Making use of that last function, our training loop now looks like this:</span>
<span id="cb70-521"><a href="#cb70-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-524"><a href="#cb70-524" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-525"><a href="#cb70-525" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-526"><a href="#cb70-526" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-527"><a href="#cb70-527" aria-hidden="true" tabindex="-1"></a><span class="co"># Initializing weights again for a fresh start</span></span>
<span id="cb70-528"><a href="#cb70-528" aria-hidden="true" tabindex="-1"></a>w1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">1</span>))</span>
<span id="cb70-529"><a href="#cb70-529" aria-hidden="true" tabindex="-1"></a>b1 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-530"><a href="#cb70-530" aria-hidden="true" tabindex="-1"></a>w2 <span class="op">=</span> init_params((<span class="dv">1</span>, <span class="dv">10</span>))</span>
<span id="cb70-531"><a href="#cb70-531" aria-hidden="true" tabindex="-1"></a>b2 <span class="op">=</span> init_params(<span class="dv">1</span>)</span>
<span id="cb70-532"><a href="#cb70-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-533"><a href="#cb70-533" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PreTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb70-534"><a href="#cb70-534" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-535"><a href="#cb70-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-536"><a href="#cb70-536" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, lr, params, f_loss):</span>
<span id="cb70-537"><a href="#cb70-537" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb70-538"><a href="#cb70-538" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb70-539"><a href="#cb70-539" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb70-540"><a href="#cb70-540" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> w1, b1, w2, b2:</span>
<span id="cb70-541"><a href="#cb70-541" aria-hidden="true" tabindex="-1"></a>                p.data <span class="op">=</span> p.data<span class="op">-</span>p.grad.data<span class="op">*</span>lr</span>
<span id="cb70-542"><a href="#cb70-542" aria-hidden="true" tabindex="-1"></a>                p.grad.zero_()</span>
<span id="cb70-543"><a href="#cb70-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-544"><a href="#cb70-544" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-545"><a href="#cb70-545" aria-hidden="true" tabindex="-1"></a>train_epoch(myModel, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb70-546"><a href="#cb70-546" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"PostTrain Accuracy: "</span><span class="op">+</span><span class="bu">str</span>(validate_epoch(myModel)))</span>
<span id="cb70-547"><a href="#cb70-547" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-548"><a href="#cb70-548" aria-hidden="true" tabindex="-1"></a>UH OH!</span>
<span id="cb70-549"><a href="#cb70-549" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-550"><a href="#cb70-550" aria-hidden="true" tabindex="-1"></a>::: {.callout-note collapse="true"}</span>
<span id="cb70-551"><a href="#cb70-551" aria-hidden="true" tabindex="-1"></a><span class="fu">## Overfitting</span></span>
<span id="cb70-552"><a href="#cb70-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-553"><a href="#cb70-553" aria-hidden="true" tabindex="-1"></a>Our loss is going down, but our error is erratic, staying the same, or going up! This is an indicator that we've reached a point where the mathematical strategy we're using to optimize is now playing games with us. The modifications *do* reduce loss, but that is no longer aligned with an improvement with the task. Recall that a loss function *should* always correlate with NN efficacy, but this can break down at the fringes.</span>
<span id="cb70-554"><a href="#cb70-554" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-555"><a href="#cb70-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-556"><a href="#cb70-556" aria-hidden="true" tabindex="-1"></a>Is this a problem? Yes! But one that can be solved. Overall we are still in good territory.. We know the math and programming is working from a nuts and bolts perspective. What this indicates to us is that we need to tune the *metaparameters* of our neural network function. How many layers are there? How wide are the layers? We may be in a situation where the NN doesn't have sufficient *capacity*^<span class="co">[</span><span class="ot">(Further reading)[https://machinelearningmastery.com/how-to-control-neural-network-model-capacity-with-nodes-and-layers/] on capacity.</span><span class="co">]</span> to get us the outcomes we're looking for.</span>
<span id="cb70-557"><a href="#cb70-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-558"><a href="#cb70-558" aria-hidden="true" tabindex="-1"></a><span class="fu">## Building Test Infrastructure</span></span>
<span id="cb70-559"><a href="#cb70-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-560"><a href="#cb70-560" aria-hidden="true" tabindex="-1"></a>We've built an NN from first principles now, putting together the building blocks of parameter initialization, running the algebra through the net, backpropagation to compute the gradient, and parameter calibration through the gradient descent method. This all executing repeatedly across batches of data.</span>
<span id="cb70-561"><a href="#cb70-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-562"><a href="#cb70-562" aria-hidden="true" tabindex="-1"></a>Before we adjust the network structure to optimize the outcomes, let's define a yet higher encapsulation of these tools to further condense the commands needed to execute this whole process, as well as enable more execution flexibility. This is where the <span class="in">`BasicOptim`</span> class omes in:</span>
<span id="cb70-563"><a href="#cb70-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-566"><a href="#cb70-566" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-567"><a href="#cb70-567" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-568"><a href="#cb70-568" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-569"><a href="#cb70-569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-570"><a href="#cb70-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-571"><a href="#cb70-571" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BasicOptim:</span>
<span id="cb70-572"><a href="#cb70-572" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, params, lr): <span class="va">self</span>.params, <span class="va">self</span>.lr <span class="op">=</span> <span class="bu">list</span>(params), lr</span>
<span id="cb70-573"><a href="#cb70-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-574"><a href="#cb70-574" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb70-575"><a href="#cb70-575" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb70-576"><a href="#cb70-576" aria-hidden="true" tabindex="-1"></a>            p.data <span class="op">-=</span> p.grad.data <span class="op">*</span> <span class="va">self</span>.lr</span>
<span id="cb70-577"><a href="#cb70-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-578"><a href="#cb70-578" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> zero_grad(<span class="va">self</span>, <span class="op">*</span>args, <span class="op">**</span>kwargs):</span>
<span id="cb70-579"><a href="#cb70-579" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.params:</span>
<span id="cb70-580"><a href="#cb70-580" aria-hidden="true" tabindex="-1"></a>            p.grad <span class="op">=</span> <span class="va">None</span></span>
<span id="cb70-581"><a href="#cb70-581" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-582"><a href="#cb70-582" aria-hidden="true" tabindex="-1"></a>Now, we can condense the training loop, and also neatly add the function of tracking loss across batches.</span>
<span id="cb70-583"><a href="#cb70-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-586"><a href="#cb70-586" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-587"><a href="#cb70-587" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-588"><a href="#cb70-588" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-589"><a href="#cb70-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-590"><a href="#cb70-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-591"><a href="#cb70-591" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train_epoch(model, opt, lr, params, f_loss):</span>
<span id="cb70-592"><a href="#cb70-592" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []  <span class="co"># Will allow for recording epoch wise loss</span></span>
<span id="cb70-593"><a href="#cb70-593" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xb, yb <span class="kw">in</span> dls[<span class="dv">0</span>]:</span>
<span id="cb70-594"><a href="#cb70-594" aria-hidden="true" tabindex="-1"></a>        calc_grad(xb, yb, model, f_loss)</span>
<span id="cb70-595"><a href="#cb70-595" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb70-596"><a href="#cb70-596" aria-hidden="true" tabindex="-1"></a>        losses.append(calc_grad(xb, yb, model, f_loss))</span>
<span id="cb70-597"><a href="#cb70-597" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb70-598"><a href="#cb70-598" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tensor(losses)</span>
<span id="cb70-599"><a href="#cb70-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-600"><a href="#cb70-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-601"><a href="#cb70-601" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-602"><a href="#cb70-602" aria-hidden="true" tabindex="-1"></a>And per usual, give it a test:</span>
<span id="cb70-605"><a href="#cb70-605" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-606"><a href="#cb70-606" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-607"><a href="#cb70-607" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-608"><a href="#cb70-608" aria-hidden="true" tabindex="-1"></a>my_opt <span class="op">=</span> BasicOptim([w1, b1, w2, b2], <span class="fl">0.01</span>)</span>
<span id="cb70-609"><a href="#cb70-609" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> train_epoch(myModel, my_opt, <span class="fl">0.01</span>, [w1, b1, w2, b2], my_loss)</span>
<span id="cb70-610"><a href="#cb70-610" aria-hidden="true" tabindex="-1"></a>res.mean(), res</span>
<span id="cb70-611"><a href="#cb70-611" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-612"><a href="#cb70-612" aria-hidden="true" tabindex="-1"></a>Great: bug free. But... still this issue of the loss bouncing around withot a clear improvement trend...</span>
<span id="cb70-613"><a href="#cb70-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-614"><a href="#cb70-614" aria-hidden="true" tabindex="-1"></a>Knowing that I want to experiment with the models metaprameters to improve it, I'll finally define yet another object to contain those we've made so far and run trials with customized parameters, and providing the means for monitoring these trials more easily.</span>
<span id="cb70-617"><a href="#cb70-617" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-618"><a href="#cb70-618" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-619"><a href="#cb70-619" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-620"><a href="#cb70-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-621"><a href="#cb70-621" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-622"><a href="#cb70-622" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> cTrial:</span>
<span id="cb70-623"><a href="#cb70-623" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, numE<span class="op">=</span><span class="dv">10</span>, lr<span class="op">=</span><span class="fl">0.01</span>, model<span class="op">=</span>myModel, opt<span class="op">=</span>my_opt, params<span class="op">=</span>[w1, b1, w2, b2], f_loss<span class="op">=</span>my_loss):</span>
<span id="cb70-624"><a href="#cb70-624" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.numE <span class="op">=</span> numE</span>
<span id="cb70-625"><a href="#cb70-625" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lr <span class="op">=</span> lr</span>
<span id="cb70-626"><a href="#cb70-626" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb70-627"><a href="#cb70-627" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.opt <span class="op">=</span> opt</span>
<span id="cb70-628"><a href="#cb70-628" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.params <span class="op">=</span> params</span>
<span id="cb70-629"><a href="#cb70-629" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.f_loss <span class="op">=</span> f_loss</span>
<span id="cb70-630"><a href="#cb70-630" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res <span class="op">=</span> []</span>
<span id="cb70-631"><a href="#cb70-631" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> []</span>
<span id="cb70-632"><a href="#cb70-632" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> []  <span class="co"># For tracking change in weights across learning</span></span>
<span id="cb70-633"><a href="#cb70-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-634"><a href="#cb70-634" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>, numE<span class="op">=</span><span class="va">None</span>, wkLr<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb70-635"><a href="#cb70-635" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb70-636"><a href="#cb70-636" aria-hidden="true" tabindex="-1"></a>        epch_losses <span class="op">=</span> []  <span class="co"># Reset</span></span>
<span id="cb70-637"><a href="#cb70-637" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.wtsHist <span class="op">=</span> [[], [], [], []]  <span class="co"># 4 contents, w1,b1,w2,b2</span></span>
<span id="cb70-638"><a href="#cb70-638" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> numE <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb70-639"><a href="#cb70-639" aria-hidden="true" tabindex="-1"></a>            numE <span class="op">=</span> <span class="va">self</span>.numE</span>
<span id="cb70-640"><a href="#cb70-640" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> wkLr <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb70-641"><a href="#cb70-641" aria-hidden="true" tabindex="-1"></a>            wkLr <span class="op">=</span> <span class="va">self</span>.lr</span>
<span id="cb70-642"><a href="#cb70-642" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(numE):</span>
<span id="cb70-643"><a href="#cb70-643" aria-hidden="true" tabindex="-1"></a>            <span class="co"># -- Record wts for analysis</span></span>
<span id="cb70-644"><a href="#cb70-644" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">0</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb70-645"><a href="#cb70-645" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">1</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b1.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb70-646"><a href="#cb70-646" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">2</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> w2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb70-647"><a href="#cb70-647" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.wtsHist[<span class="dv">3</span>].append(<span class="bu">list</span>(x.item() <span class="cf">for</span> x <span class="kw">in</span> b2.data.view(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb70-648"><a href="#cb70-648" aria-hidden="true" tabindex="-1"></a>            <span class="co"># --</span></span>
<span id="cb70-649"><a href="#cb70-649" aria-hidden="true" tabindex="-1"></a>            res <span class="op">=</span> train_epoch(<span class="va">self</span>.model, <span class="va">self</span>.opt, <span class="va">self</span>.lr,</span>
<span id="cb70-650"><a href="#cb70-650" aria-hidden="true" tabindex="-1"></a>                              <span class="va">self</span>.params, <span class="va">self</span>.f_loss)</span>
<span id="cb70-651"><a href="#cb70-651" aria-hidden="true" tabindex="-1"></a>            epch_losses.append(res)</span>
<span id="cb70-652"><a href="#cb70-652" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.valids.append(validate_epoch(<span class="va">self</span>.model))</span>
<span id="cb70-653"><a href="#cb70-653" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.res <span class="op">=</span> torch.stack(epch_losses)</span>
<span id="cb70-654"><a href="#cb70-654" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.valids <span class="op">=</span> tensor(<span class="va">self</span>.valids)</span>
<span id="cb70-655"><a href="#cb70-655" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-656"><a href="#cb70-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-657"><a href="#cb70-657" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-658"><a href="#cb70-658" aria-hidden="true" tabindex="-1"></a>This object allows for defining all the parameters necessary for a trial, and then running it one line of code, alternaitvely overwriting the number of Epochs, and learning rate parameters.</span>
<span id="cb70-659"><a href="#cb70-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-660"><a href="#cb70-660" aria-hidden="true" tabindex="-1"></a><span class="fu">## Experimenting To Optimize NN</span></span>
<span id="cb70-661"><a href="#cb70-661" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-662"><a href="#cb70-662" aria-hidden="true" tabindex="-1"></a>Let's get into it. We make a trial object and run it:</span>
<span id="cb70-663"><a href="#cb70-663" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-666"><a href="#cb70-666" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-667"><a href="#cb70-667" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-668"><a href="#cb70-668" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-669"><a href="#cb70-669" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial()</span>
<span id="cb70-670"><a href="#cb70-670" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb70-671"><a href="#cb70-671" aria-hidden="true" tabindex="-1"></a>my_try.res.shape</span>
<span id="cb70-672"><a href="#cb70-672" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-673"><a href="#cb70-673" aria-hidden="true" tabindex="-1"></a>Our functions are logging loss for each mini batch across all epochs. So this makes sense. We had 1540 data points, with minibatches of 25. <span class="in">`25*62=1550`</span>, so we have 61 minibatches of 25 one last minibatch of 15. And 50 collections of 62, one for each epoch.</span>
<span id="cb70-674"><a href="#cb70-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-675"><a href="#cb70-675" aria-hidden="true" tabindex="-1"></a><span class="fu">### First Try</span></span>
<span id="cb70-676"><a href="#cb70-676" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-677"><a href="#cb70-677" aria-hidden="true" tabindex="-1"></a>So let's see the results by taking the average loss across each epoch, hopefully, it'll be dropping!</span>
<span id="cb70-680"><a href="#cb70-680" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-681"><a href="#cb70-681" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-loss1</span></span>
<span id="cb70-682"><a href="#cb70-682" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Average loss across epochs"</span></span>
<span id="cb70-683"><a href="#cb70-683" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-684"><a href="#cb70-684" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb70-685"><a href="#cb70-685" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-686"><a href="#cb70-686" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-687"><a href="#cb70-687" aria-hidden="true" tabindex="-1"></a>So loss is going down, it's working right? The real test isn't reduction to our measure loss, but to see that carry through to an increase in testing accuracy:</span>
<span id="cb70-688"><a href="#cb70-688" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-691"><a href="#cb70-691" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-692"><a href="#cb70-692" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-val1</span></span>
<span id="cb70-693"><a href="#cb70-693" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Prediction accuracy across epochs"</span></span>
<span id="cb70-694"><a href="#cb70-694" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-695"><a href="#cb70-695" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-696"><a href="#cb70-696" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-697"><a href="#cb70-697" aria-hidden="true" tabindex="-1"></a>Disaster strikes! We're never getting more than 20% accuracy! Well, they call it *deep learning*, so let's make our model deeper. Two layers? A pittance!</span>
<span id="cb70-698"><a href="#cb70-698" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-699"><a href="#cb70-699" aria-hidden="true" tabindex="-1"></a><span class="fu">### Second Try</span></span>
<span id="cb70-700"><a href="#cb70-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-701"><a href="#cb70-701" aria-hidden="true" tabindex="-1"></a>Our Model V2 will look like this:</span>
<span id="cb70-702"><a href="#cb70-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-703"><a href="#cb70-703" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>64 input activations (image pixels) to 32 neurons in the first layer, with a unique (not broadcasted) bias for each neuron</span>
<span id="cb70-704"><a href="#cb70-704" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>The next layer will reduce the 32 neurons to 16, with anothe runique bias for each</span>
<span id="cb70-705"><a href="#cb70-705" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>The next will reduce teh 16 layers to 10 (output layer activations) with a unique bias for each</span>
<span id="cb70-706"><a href="#cb70-706" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Between each layer we will use the ReLu for nonlinearity.</span>
<span id="cb70-707"><a href="#cb70-707" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-708"><a href="#cb70-708" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip collapse="true"}</span>
<span id="cb70-709"><a href="#cb70-709" aria-hidden="true" tabindex="-1"></a><span class="fu">## Visualizing V2 Of Our NN</span></span>
<span id="cb70-710"><a href="#cb70-710" aria-hidden="true" tabindex="-1"></a>Can you picture the matrix multiplication? It's important to get it straight! What would the equivalent of @fig-Layer1 look like for our NN V2?</span>
<span id="cb70-711"><a href="#cb70-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-712"><a href="#cb70-712" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-713"><a href="#cb70-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-714"><a href="#cb70-714" aria-hidden="true" tabindex="-1"></a>Defining parameters:</span>
<span id="cb70-717"><a href="#cb70-717" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-718"><a href="#cb70-718" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-719"><a href="#cb70-719" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-720"><a href="#cb70-720" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize parameters using the functions defined above to make it easy to generate variously sized tensors</span></span>
<span id="cb70-721"><a href="#cb70-721" aria-hidden="true" tabindex="-1"></a>W1 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-722"><a href="#cb70-722" aria-hidden="true" tabindex="-1"></a>B1 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-723"><a href="#cb70-723" aria-hidden="true" tabindex="-1"></a>W2 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-724"><a href="#cb70-724" aria-hidden="true" tabindex="-1"></a>B2 <span class="op">=</span> init_params(<span class="dv">16</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-725"><a href="#cb70-725" aria-hidden="true" tabindex="-1"></a>W3 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-726"><a href="#cb70-726" aria-hidden="true" tabindex="-1"></a>B3 <span class="op">=</span> init_params(<span class="dv">10</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-727"><a href="#cb70-727" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-728"><a href="#cb70-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-729"><a href="#cb70-729" aria-hidden="true" tabindex="-1"></a>Define the model:</span>
<span id="cb70-732"><a href="#cb70-732" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-733"><a href="#cb70-733" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-734"><a href="#cb70-734" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-735"><a href="#cb70-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-736"><a href="#cb70-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-737"><a href="#cb70-737" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mdlV2(xb):</span>
<span id="cb70-738"><a href="#cb70-738" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1<span class="op">+</span>B1</span>
<span id="cb70-739"><a href="#cb70-739" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-740"><a href="#cb70-740" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W2<span class="op">+</span>B2  <span class="co"># returns 10 features for each input</span></span>
<span id="cb70-741"><a href="#cb70-741" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-742"><a href="#cb70-742" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W3<span class="op">+</span>B3  <span class="co"># returns 10 features for each input</span></span>
<span id="cb70-743"><a href="#cb70-743" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb70-744"><a href="#cb70-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-745"><a href="#cb70-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-746"><a href="#cb70-746" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-747"><a href="#cb70-747" aria-hidden="true" tabindex="-1"></a>Test it with a mini sample to ensure the Tensors were defined in the correct size to achieve our purpose:</span>
<span id="cb70-750"><a href="#cb70-750" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-751"><a href="#cb70-751" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-752"><a href="#cb70-752" aria-hidden="true" tabindex="-1"></a>mdlV2(mini_x)</span>
<span id="cb70-753"><a href="#cb70-753" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-754"><a href="#cb70-754" aria-hidden="true" tabindex="-1"></a>Run it!</span>
<span id="cb70-755"><a href="#cb70-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-756"><a href="#cb70-756" aria-hidden="true" tabindex="-1"></a>:::{#fig-V2}</span>
<span id="cb70-759"><a href="#cb70-759" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-760"><a href="#cb70-760" aria-hidden="true" tabindex="-1"></a><span class="co"># | column: page</span></span>
<span id="cb70-761"><a href="#cb70-761" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-762"><a href="#cb70-762" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV2, opt<span class="op">=</span>BasicOptim(</span>
<span id="cb70-763"><a href="#cb70-763" aria-hidden="true" tabindex="-1"></a>    [W1, B1, W2, B2, W3, B3], <span class="fl">0.01</span>), params<span class="op">=</span>[W1, B1, W2, B2, W3, B3])</span>
<span id="cb70-764"><a href="#cb70-764" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb70-765"><a href="#cb70-765" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-766"><a href="#cb70-766" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-767"><a href="#cb70-767" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-768"><a href="#cb70-768" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-769"><a href="#cb70-769" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-770"><a href="#cb70-770" aria-hidden="true" tabindex="-1"></a>Accuracy Increasing As Loss Reduces</span>
<span id="cb70-771"><a href="#cb70-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-772"><a href="#cb70-772" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-773"><a href="#cb70-773" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-774"><a href="#cb70-774" aria-hidden="true" tabindex="-1"></a>Fantastic! Like an Olympic high jumper, we vault just over the top of the target! Time for champagne?! Not so fast! These results are those from exeucting on the training set. Remember when we partitioned off the validation set? Let's see how the model does on that data.</span>
<span id="cb70-775"><a href="#cb70-775" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-776"><a href="#cb70-776" aria-hidden="true" tabindex="-1"></a>The point of a validation set is to increase cofnidence that our model hasn't faux-optimized by 'playing our game' so to speak, and learning to score well on test data while not actually generalizing across the domain of all possible inputs</span>
<span id="cb70-777"><a href="#cb70-777" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-778"><a href="#cb70-778" aria-hidden="true" tabindex="-1"></a>:::{#fig-V2conf}</span>
<span id="cb70-781"><a href="#cb70-781" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-782"><a href="#cb70-782" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-783"><a href="#cb70-783" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-784"><a href="#cb70-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-785"><a href="#cb70-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-786"><a href="#cb70-786" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> confMtx(model):</span>
<span id="cb70-787"><a href="#cb70-787" aria-hidden="true" tabindex="-1"></a>    conf <span class="op">=</span> torch.zeros([<span class="dv">10</span>, <span class="dv">10</span>], dtype<span class="op">=</span>torch.int32)</span>
<span id="cb70-788"><a href="#cb70-788" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-789"><a href="#cb70-789" aria-hidden="true" tabindex="-1"></a>    tot <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb70-790"><a href="#cb70-790" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> xv, yv <span class="kw">in</span> dls[<span class="dv">1</span>]:</span>
<span id="cb70-791"><a href="#cb70-791" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> model(xv).<span class="bu">max</span>(dim<span class="op">=-</span><span class="dv">1</span>).indices</span>
<span id="cb70-792"><a href="#cb70-792" aria-hidden="true" tabindex="-1"></a>        tot <span class="op">+=</span> <span class="bu">len</span>(preds)</span>
<span id="cb70-793"><a href="#cb70-793" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(xv)):</span>
<span id="cb70-794"><a href="#cb70-794" aria-hidden="true" tabindex="-1"></a>            conf[yv[i].item()][preds[i].item()] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb70-795"><a href="#cb70-795" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> yv[i].item() <span class="op">==</span> preds[i].item():</span>
<span id="cb70-796"><a href="#cb70-796" aria-hidden="true" tabindex="-1"></a>                acc <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb70-797"><a href="#cb70-797" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.DataFrame(conf)</span>
<span id="cb70-798"><a href="#cb70-798" aria-hidden="true" tabindex="-1"></a>    df.style.set_properties(<span class="op">**</span>{<span class="st">'font-size'</span>: <span class="st">'6pt'</span>}</span>
<span id="cb70-799"><a href="#cb70-799" aria-hidden="true" tabindex="-1"></a>                            ).background_gradient(<span class="st">'Greys'</span>)</span>
<span id="cb70-800"><a href="#cb70-800" aria-hidden="true" tabindex="-1"></a>    df.style.set_table_styles([<span class="bu">dict</span>(selector<span class="op">=</span><span class="st">'th'</span>, props<span class="op">=</span>[(</span>
<span id="cb70-801"><a href="#cb70-801" aria-hidden="true" tabindex="-1"></a>        <span class="st">'text-align'</span>, <span class="st">'center'</span>)])]).set_properties(<span class="op">**</span>{<span class="st">'text-align'</span>: <span class="st">'center'</span>})</span>
<span id="cb70-802"><a href="#cb70-802" aria-hidden="true" tabindex="-1"></a>    df.style.set_caption(<span class="st">"Top Axis: Predicted value. Left Axis: Actual Value"</span>)</span>
<span id="cb70-803"><a href="#cb70-803" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> df, acc<span class="op">/</span>tot</span>
<span id="cb70-804"><a href="#cb70-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-805"><a href="#cb70-805" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-806"><a href="#cb70-806" aria-hidden="true" tabindex="-1"></a>    <span class="co"># return df</span></span>
<span id="cb70-807"><a href="#cb70-807" aria-hidden="true" tabindex="-1"></a>df, acc <span class="op">=</span> confMtx(mdlV2)</span>
<span id="cb70-808"><a href="#cb70-808" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb70-809"><a href="#cb70-809" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-810"><a href="#cb70-810" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-813"><a href="#cb70-813" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-814"><a href="#cb70-814" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Accuracy on validation set: "</span><span class="op">+</span><span class="bu">str</span>(acc))</span>
<span id="cb70-815"><a href="#cb70-815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-816"><a href="#cb70-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-817"><a href="#cb70-817" aria-hidden="true" tabindex="-1"></a>Confusion Matrix. Left Axis: Actual Number. Top Axis: Predicted Number.</span>
<span id="cb70-818"><a href="#cb70-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-819"><a href="#cb70-819" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-820"><a href="#cb70-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-821"><a href="#cb70-821" aria-hidden="true" tabindex="-1"></a>Excellent! The NN has learned to generalize. But, dear reader, I have lied to you</span>
<span id="cb70-822"><a href="#cb70-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-823"><a href="#cb70-823" aria-hidden="true" tabindex="-1"></a><span class="fu"># Outro</span></span>
<span id="cb70-824"><a href="#cb70-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-825"><a href="#cb70-825" aria-hidden="true" tabindex="-1"></a><span class="fu">## Getting Here</span></span>
<span id="cb70-826"><a href="#cb70-826" aria-hidden="true" tabindex="-1"></a>When I was first working through this mini-project, the final outcome did not come so easily. There are a lot of meta-parameters that we can play with, which creates a large solution space to search. These include:</span>
<span id="cb70-827"><a href="#cb70-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-828"><a href="#cb70-828" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Number of layers</span>
<span id="cb70-829"><a href="#cb70-829" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Number of neurons per layer</span>
<span id="cb70-830"><a href="#cb70-830" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Broadcasting a single bias vs using many unique biases</span>
<span id="cb70-831"><a href="#cb70-831" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Learning rate, including whether or not to change the learning rate across epochs, and if so, according to what function</span>
<span id="cb70-832"><a href="#cb70-832" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>What non-linearity function to use</span>
<span id="cb70-833"><a href="#cb70-833" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>What distribution (and with what parameters) to use for initializing parameters</span>
<span id="cb70-834"><a href="#cb70-834" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>How many epochs to run</span>
<span id="cb70-835"><a href="#cb70-835" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>What mini batch size to use</span>
<span id="cb70-836"><a href="#cb70-836" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>Whether or not to shuffle the data</span>
<span id="cb70-837"><a href="#cb70-837" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-838"><a href="#cb70-838" aria-hidden="true" tabindex="-1"></a>That's a lot of possibilities- I'm still learning what the best practices are to make decisions on these matters. I sure did spend a good numbers of hours experimenting with what seemed like dead ends though! </span>
<span id="cb70-839"><a href="#cb70-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-840"><a href="#cb70-840" aria-hidden="true" tabindex="-1"></a>What finally made the difference was reducing the variance of tha random distribution I was using to initialize my parameters. My earlier models were very naive, always defaulting to just selecting one digit or another. Looking back, my hypothesis is that the larger variance in initialization distribution led to the largest parameters dominating the NN through the layers. But it's difficult to make any conclusive statements like that.</span>
<span id="cb70-841"><a href="#cb70-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-842"><a href="#cb70-842" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lessons Learned</span></span>
<span id="cb70-843"><a href="#cb70-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-844"><a href="#cb70-844" aria-hidden="true" tabindex="-1"></a>A number of great first lessons in NN's to take away from this case study. </span>
<span id="cb70-845"><a href="#cb70-845" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-846"><a href="#cb70-846" aria-hidden="true" tabindex="-1"></a>First of all, the great structure provided by the FastAI course with regard to defining the various classes and functions whose interplay really enables quick and effective implementaiton of the strategy. This, I think, is *so* important. It's the difference between understanding the first principles of how this stuff works ( i.e. "A nail is driven into a beam by application of force to its head perpendicular to the recieving surface") vs. the making of the toolkit at hadn to implement the idea (i.e. "Here is a hammer! Get to work!) Here is a summary of this toolkit, from the outermost layer, in:</span>
<span id="cb70-847"><a href="#cb70-847" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-848"><a href="#cb70-848" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>A 'Trial' taking parameters like number of epochs, learning rate, the model, optimizer, parameters, and loss function</span>
<span id="cb70-849"><a href="#cb70-849" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>An optimizer to execute the 'parameter adjustment' inherent to the learning process</span>
<span id="cb70-850"><a href="#cb70-850" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>An function to run an epoch, givne all necessary inputs</span>
<span id="cb70-851"><a href="#cb70-851" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>A function to execute the model on a batch, and calculate the gradient</span>
<span id="cb70-852"><a href="#cb70-852" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>A function calculate loss</span>
<span id="cb70-853"><a href="#cb70-853" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>A function to calculate inputs</span>
<span id="cb70-854"><a href="#cb70-854" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-855"><a href="#cb70-855" aria-hidden="true" tabindex="-1"></a>Another *important* lesson was not to understimate the pitfalls of implementing your own math. I thought I had my loss-function issue solved when I abandoned my <span class="in">`SoftMax`</span> formula for PyTorch's pre-built function, but I was further humbled by the need to resort the built in <span class="in">`CrossEntropyLoss`</span> function. Sometimes it really is so much easier to understand something in principle than it is to implement it; we could drown deep currents of complexity hidden by the birdges built by those who came before. We should know where they are so as to use them and not fall off.</span>
<span id="cb70-856"><a href="#cb70-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-857"><a href="#cb70-857" aria-hidden="true" tabindex="-1"></a>This case study also forced me to grapple with the whole matrix multiplication process to ensure I was understanding exactly how to define how many neurons each layer would have, and execute that in code, etc. it was a bit of a mental hurdle to go from just ahving done it on paper in the classroom to implementing with code. Though a lot easier to be sure!</span>
<span id="cb70-858"><a href="#cb70-858" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-859"><a href="#cb70-859" aria-hidden="true" tabindex="-1"></a>Another important and very actionable lesson to prod the solution space by testing orders of magnitude different meta parameters (learning rate, parameter initialization variance). This is important because one realistically has no basis on whihc to guess at how much or how little a meta parameter should vary when beginning to optimize an NN. By testing different orders of magnitude <span class="in">`(10,1,0.1,0.01 ...)`</span> we can drop quickly and easily identify which are ain whihc to focus our efforts.</span>
<span id="cb70-860"><a href="#cb70-860" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-861"><a href="#cb70-861" aria-hidden="true" tabindex="-1"></a>Lastly, to have hope and keep fighting the good fight! I was having some real trouble with making an NN that didn't just naively default to one digit or another. But in the end, with enough experimentation and further study, I prevailed... exciting. And for your pleasure, I end this post with my NN cemetery, those failed networks that I couldn't get any good results out of. Although who knows, maybe they would perform better than my simple V2 model in the end, with a slight change to some metaparameter. After all, I had abandoned my V2 model after it didnt work at first, until I changed my parameter initialization much later, and decided it might be worth giving it another shot. You never know when you'll strike gold.</span>
<span id="cb70-862"><a href="#cb70-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-863"><a href="#cb70-863" aria-hidden="true" tabindex="-1"></a><span class="fu">## NN Graveyard</span></span>
<span id="cb70-864"><a href="#cb70-864" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-865"><a href="#cb70-865" aria-hidden="true" tabindex="-1"></a>Our dearly departed...</span>
<span id="cb70-866"><a href="#cb70-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-867"><a href="#cb70-867" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Funnel</span></span>
<span id="cb70-868"><a href="#cb70-868" aria-hidden="true" tabindex="-1"></a>Started wide and iteratively narrowing down to the output size over many layers.</span>
<span id="cb70-871"><a href="#cb70-871" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-872"><a href="#cb70-872" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-873"><a href="#cb70-873" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-874"><a href="#cb70-874" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>), <span class="dv">1</span>)  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-875"><a href="#cb70-875" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-876"><a href="#cb70-876" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>), <span class="dv">1</span>)  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-877"><a href="#cb70-877" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-878"><a href="#cb70-878" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>), <span class="dv">1</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-879"><a href="#cb70-879" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>, <span class="dv">1</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-880"><a href="#cb70-880" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>), <span class="dv">1</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-881"><a href="#cb70-881" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>, <span class="dv">1</span>)</span>
<span id="cb70-882"><a href="#cb70-882" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>), <span class="dv">1</span>)</span>
<span id="cb70-883"><a href="#cb70-883" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>, <span class="dv">1</span>)</span>
<span id="cb70-884"><a href="#cb70-884" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>), <span class="dv">1</span>)</span>
<span id="cb70-885"><a href="#cb70-885" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="dv">1</span>)</span>
<span id="cb70-886"><a href="#cb70-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-887"><a href="#cb70-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-888"><a href="#cb70-888" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mdlV3(xb):</span>
<span id="cb70-889"><a href="#cb70-889" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1v3<span class="op">+</span>B1v3</span>
<span id="cb70-890"><a href="#cb70-890" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-891"><a href="#cb70-891" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W2v3<span class="op">+</span>B2v3</span>
<span id="cb70-892"><a href="#cb70-892" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-893"><a href="#cb70-893" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W3v3<span class="op">+</span>B3v3</span>
<span id="cb70-894"><a href="#cb70-894" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-895"><a href="#cb70-895" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W4v3<span class="op">+</span>B4v3</span>
<span id="cb70-896"><a href="#cb70-896" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-897"><a href="#cb70-897" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W5v3<span class="op">+</span>B5v3</span>
<span id="cb70-898"><a href="#cb70-898" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-899"><a href="#cb70-899" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W6v3<span class="op">+</span>B6v3</span>
<span id="cb70-900"><a href="#cb70-900" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb70-901"><a href="#cb70-901" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-902"><a href="#cb70-902" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-903"><a href="#cb70-903" aria-hidden="true" tabindex="-1"></a><span class="fu">### Bad Parameter Initialization</span></span>
<span id="cb70-904"><a href="#cb70-904" aria-hidden="true" tabindex="-1"></a>Notice that in the above, I had the parameter initialization with a variance of 1 passed to the random number generator distribution. And so I kept getting **junk** results like the following!</span>
<span id="cb70-907"><a href="#cb70-907" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-908"><a href="#cb70-908" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-funnel</span></span>
<span id="cb70-909"><a href="#cb70-909" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "'Funnel' model loss and accuracy across epochs"</span></span>
<span id="cb70-910"><a href="#cb70-910" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-911"><a href="#cb70-911" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-912"><a href="#cb70-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-913"><a href="#cb70-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-914"><a href="#cb70-914" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb70-915"><a href="#cb70-915" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-916"><a href="#cb70-916" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb70-917"><a href="#cb70-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-918"><a href="#cb70-918" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-919"><a href="#cb70-919" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-920"><a href="#cb70-920" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-921"><a href="#cb70-921" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-922"><a href="#cb70-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-923"><a href="#cb70-923" aria-hidden="true" tabindex="-1"></a>What does a flat lining accuracy mean exactly? The confusion matrix tells all^<span class="co">[</span><span class="ot">*"Tonight at 9, only on Fox!"*</span><span class="co">]</span>...</span>
<span id="cb70-926"><a href="#cb70-926" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-927"><a href="#cb70-927" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-928"><a href="#cb70-928" aria-hidden="true" tabindex="-1"></a>df, acc <span class="op">=</span> confMtx(mdlV3)</span>
<span id="cb70-929"><a href="#cb70-929" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb70-930"><a href="#cb70-930" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-931"><a href="#cb70-931" aria-hidden="true" tabindex="-1"></a>Yup.. Literally every input yields an output of <span class="in">`0`</span>, so naturally, given the data set composition, its dead right, *10% of the time*!</span>
<span id="cb70-932"><a href="#cb70-932" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-933"><a href="#cb70-933" aria-hidden="true" tabindex="-1"></a><span class="fu">### Better Parameter Initialization</span></span>
<span id="cb70-934"><a href="#cb70-934" aria-hidden="true" tabindex="-1"></a>But looks at what happens when we put the variance of the number generator to 0.1, its default!</span>
<span id="cb70-937"><a href="#cb70-937" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-938"><a href="#cb70-938" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-939"><a href="#cb70-939" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-funnel_try2</span></span>
<span id="cb70-940"><a href="#cb70-940" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Retrying the 'Funnel model', with smaller initial parameter variation"</span></span>
<span id="cb70-941"><a href="#cb70-941" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-942"><a href="#cb70-942" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-943"><a href="#cb70-943" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-944"><a href="#cb70-944" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-945"><a href="#cb70-945" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-946"><a href="#cb70-946" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-947"><a href="#cb70-947" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-948"><a href="#cb70-948" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb70-949"><a href="#cb70-949" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb70-950"><a href="#cb70-950" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb70-951"><a href="#cb70-951" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb70-952"><a href="#cb70-952" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb70-953"><a href="#cb70-953" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-954"><a href="#cb70-954" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb70-955"><a href="#cb70-955" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-956"><a href="#cb70-956" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb70-957"><a href="#cb70-957" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-958"><a href="#cb70-958" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-959"><a href="#cb70-959" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-960"><a href="#cb70-960" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-961"><a href="#cb70-961" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-962"><a href="#cb70-962" aria-hidden="true" tabindex="-1"></a>And the confusion matrix:</span>
<span id="cb70-963"><a href="#cb70-963" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-966"><a href="#cb70-966" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-967"><a href="#cb70-967" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-968"><a href="#cb70-968" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-funnel_try2conf</span></span>
<span id="cb70-969"><a href="#cb70-969" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Retrying the 'Funnel model', Confusion Matrix"</span></span>
<span id="cb70-970"><a href="#cb70-970" aria-hidden="true" tabindex="-1"></a>df, acc <span class="op">=</span> confMtx(mdlV3)</span>
<span id="cb70-971"><a href="#cb70-971" aria-hidden="true" tabindex="-1"></a>df</span>
<span id="cb70-972"><a href="#cb70-972" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-973"><a href="#cb70-973" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-974"><a href="#cb70-974" aria-hidden="true" tabindex="-1"></a><span class="fu">### Different Learning Rates</span></span>
<span id="cb70-975"><a href="#cb70-975" aria-hidden="true" tabindex="-1"></a>Lets use this model to illustrate how different results can be with different learning rates. </span>
<span id="cb70-976"><a href="#cb70-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-977"><a href="#cb70-977" aria-hidden="true" tabindex="-1"></a>::: {#fig-lr layout-ncol=4}</span>
<span id="cb70-980"><a href="#cb70-980" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-981"><a href="#cb70-981" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-982"><a href="#cb70-982" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-Funlr1</span></span>
<span id="cb70-983"><a href="#cb70-983" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Funnel model, lr=1"</span></span>
<span id="cb70-984"><a href="#cb70-984" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-985"><a href="#cb70-985" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-986"><a href="#cb70-986" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-987"><a href="#cb70-987" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-988"><a href="#cb70-988" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-989"><a href="#cb70-989" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-990"><a href="#cb70-990" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-991"><a href="#cb70-991" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb70-992"><a href="#cb70-992" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb70-993"><a href="#cb70-993" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb70-994"><a href="#cb70-994" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb70-995"><a href="#cb70-995" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb70-996"><a href="#cb70-996" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-997"><a href="#cb70-997" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.1</span>), params<span class="op">=</span>[</span>
<span id="cb70-998"><a href="#cb70-998" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-999"><a href="#cb70-999" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb70-1000"><a href="#cb70-1000" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1001"><a href="#cb70-1001" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-1002"><a href="#cb70-1002" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-1003"><a href="#cb70-1003" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-1004"><a href="#cb70-1004" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1007"><a href="#cb70-1007" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-1008"><a href="#cb70-1008" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-1009"><a href="#cb70-1009" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-Funlr2</span></span>
<span id="cb70-1010"><a href="#cb70-1010" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Funnel model, lr=0.1"</span></span>
<span id="cb70-1011"><a href="#cb70-1011" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-1012"><a href="#cb70-1012" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1013"><a href="#cb70-1013" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-1014"><a href="#cb70-1014" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1015"><a href="#cb70-1015" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1016"><a href="#cb70-1016" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1017"><a href="#cb70-1017" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1018"><a href="#cb70-1018" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb70-1019"><a href="#cb70-1019" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb70-1020"><a href="#cb70-1020" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb70-1021"><a href="#cb70-1021" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb70-1022"><a href="#cb70-1022" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb70-1023"><a href="#cb70-1023" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1024"><a href="#cb70-1024" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.1</span>), params<span class="op">=</span>[</span>
<span id="cb70-1025"><a href="#cb70-1025" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-1026"><a href="#cb70-1026" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb70-1027"><a href="#cb70-1027" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1028"><a href="#cb70-1028" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-1029"><a href="#cb70-1029" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-1030"><a href="#cb70-1030" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-1031"><a href="#cb70-1031" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1034"><a href="#cb70-1034" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-1035"><a href="#cb70-1035" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-1036"><a href="#cb70-1036" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-Funlr3</span></span>
<span id="cb70-1037"><a href="#cb70-1037" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Funnel model, lr=.01"</span></span>
<span id="cb70-1038"><a href="#cb70-1038" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-1039"><a href="#cb70-1039" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1040"><a href="#cb70-1040" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-1041"><a href="#cb70-1041" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1042"><a href="#cb70-1042" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1043"><a href="#cb70-1043" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1044"><a href="#cb70-1044" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1045"><a href="#cb70-1045" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb70-1046"><a href="#cb70-1046" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb70-1047"><a href="#cb70-1047" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb70-1048"><a href="#cb70-1048" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb70-1049"><a href="#cb70-1049" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb70-1050"><a href="#cb70-1050" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1051"><a href="#cb70-1051" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb70-1052"><a href="#cb70-1052" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-1053"><a href="#cb70-1053" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.01</span>)</span>
<span id="cb70-1054"><a href="#cb70-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1055"><a href="#cb70-1055" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-1056"><a href="#cb70-1056" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-1057"><a href="#cb70-1057" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-1058"><a href="#cb70-1058" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1061"><a href="#cb70-1061" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-1062"><a href="#cb70-1062" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-1063"><a href="#cb70-1063" aria-hidden="true" tabindex="-1"></a><span class="co"># | label: fig-Funlr4</span></span>
<span id="cb70-1064"><a href="#cb70-1064" aria-hidden="true" tabindex="-1"></a><span class="co"># | fig-cap: "Funnel model, lr=0.001"</span></span>
<span id="cb70-1065"><a href="#cb70-1065" aria-hidden="true" tabindex="-1"></a>W1v3 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">48</span>))  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-1066"><a href="#cb70-1066" aria-hidden="true" tabindex="-1"></a>B1v3 <span class="op">=</span> init_params(<span class="dv">48</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1067"><a href="#cb70-1067" aria-hidden="true" tabindex="-1"></a>W2v3 <span class="op">=</span> init_params((<span class="dv">48</span>, <span class="dv">32</span>))  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-1068"><a href="#cb70-1068" aria-hidden="true" tabindex="-1"></a>B2v3 <span class="op">=</span> init_params(<span class="dv">32</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1069"><a href="#cb70-1069" aria-hidden="true" tabindex="-1"></a>W3v3 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">24</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1070"><a href="#cb70-1070" aria-hidden="true" tabindex="-1"></a>B3v3 <span class="op">=</span> init_params(<span class="dv">24</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1071"><a href="#cb70-1071" aria-hidden="true" tabindex="-1"></a>W4v3 <span class="op">=</span> init_params((<span class="dv">24</span>, <span class="dv">18</span>))  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1072"><a href="#cb70-1072" aria-hidden="true" tabindex="-1"></a>B4v3 <span class="op">=</span> init_params(<span class="dv">18</span>)</span>
<span id="cb70-1073"><a href="#cb70-1073" aria-hidden="true" tabindex="-1"></a>W5v3 <span class="op">=</span> init_params((<span class="dv">18</span>, <span class="dv">14</span>))</span>
<span id="cb70-1074"><a href="#cb70-1074" aria-hidden="true" tabindex="-1"></a>B5v3 <span class="op">=</span> init_params(<span class="dv">14</span>,)</span>
<span id="cb70-1075"><a href="#cb70-1075" aria-hidden="true" tabindex="-1"></a>W6v3 <span class="op">=</span> init_params((<span class="dv">14</span>, <span class="dv">10</span>))</span>
<span id="cb70-1076"><a href="#cb70-1076" aria-hidden="true" tabindex="-1"></a>B6v3 <span class="op">=</span> init_params(<span class="dv">10</span>)</span>
<span id="cb70-1077"><a href="#cb70-1077" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1078"><a href="#cb70-1078" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV3, opt<span class="op">=</span>BasicOptim([W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb70-1079"><a href="#cb70-1079" aria-hidden="true" tabindex="-1"></a>                W1v3, B1v3, W2v3, B2v3, W3v3, B3v3, W4v3, B4v3, W5v3, B5v3, W6v3, B6v3])</span>
<span id="cb70-1080"><a href="#cb70-1080" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">50</span>, wkLr<span class="op">=</span><span class="fl">0.001</span>)</span>
<span id="cb70-1081"><a href="#cb70-1081" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1082"><a href="#cb70-1082" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-1083"><a href="#cb70-1083" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-1084"><a href="#cb70-1084" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-1085"><a href="#cb70-1085" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1086"><a href="#cb70-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1087"><a href="#cb70-1087" aria-hidden="true" tabindex="-1"></a>Trial outcomes with different learning rates.</span>
<span id="cb70-1088"><a href="#cb70-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1089"><a href="#cb70-1089" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb70-1090"><a href="#cb70-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1091"><a href="#cb70-1091" aria-hidden="true" tabindex="-1"></a>Smaller learning rates are characterized by smoother loss curves since they traverse a smaller region of the solution space. It's a double edged sword, though. On the one hand, a small learning rate can help hone in on a local optimum. That might be undesirable early on in the learnign process though, as the random initialization may put us in a region of parameter space where the nearest local optimum isn't great with respect to the optima across the wider parameter space. A larger learning rate can help us take large steps from an initial poor position to a much better swath of parameter space, where a smaller rate can then hone in to better results.</span>
<span id="cb70-1092"><a href="#cb70-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1093"><a href="#cb70-1093" aria-hidden="true" tabindex="-1"></a><span class="fu">### The Wormhole</span></span>
<span id="cb70-1094"><a href="#cb70-1094" aria-hidden="true" tabindex="-1"></a>Along my wanderings in the desert, I thought to try having the network layer structure narrow rapidly but extend across many layers. This implementation has 12 layers, with the last 8 all having 10 neurons in and out.</span>
<span id="cb70-1097"><a href="#cb70-1097" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-1098"><a href="#cb70-1098" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-1099"><a href="#cb70-1099" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-fold: show</span></span>
<span id="cb70-1100"><a href="#cb70-1100" aria-hidden="true" tabindex="-1"></a>W1v4 <span class="op">=</span> init_params((<span class="dv">8</span><span class="op">*</span><span class="dv">8</span>, <span class="dv">32</span>), <span class="fl">0.001</span>)  <span class="co"># 64 in, 32 out</span></span>
<span id="cb70-1101"><a href="#cb70-1101" aria-hidden="true" tabindex="-1"></a>B1v4 <span class="op">=</span> init_params(<span class="dv">32</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1102"><a href="#cb70-1102" aria-hidden="true" tabindex="-1"></a>W2v4 <span class="op">=</span> init_params((<span class="dv">32</span>, <span class="dv">16</span>), <span class="fl">0.001</span>)  <span class="co"># 32 in, 16 out</span></span>
<span id="cb70-1103"><a href="#cb70-1103" aria-hidden="true" tabindex="-1"></a>B2v4 <span class="op">=</span> init_params(<span class="dv">16</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1104"><a href="#cb70-1104" aria-hidden="true" tabindex="-1"></a>W3v4 <span class="op">=</span> init_params((<span class="dv">16</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1105"><a href="#cb70-1105" aria-hidden="true" tabindex="-1"></a>B3v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)  <span class="co"># Feature-specific biases added</span></span>
<span id="cb70-1106"><a href="#cb70-1106" aria-hidden="true" tabindex="-1"></a>W4v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)  <span class="co"># 16 in, 10 out</span></span>
<span id="cb70-1107"><a href="#cb70-1107" aria-hidden="true" tabindex="-1"></a>B4v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1108"><a href="#cb70-1108" aria-hidden="true" tabindex="-1"></a>W5v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1109"><a href="#cb70-1109" aria-hidden="true" tabindex="-1"></a>B5v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1110"><a href="#cb70-1110" aria-hidden="true" tabindex="-1"></a>W6v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1111"><a href="#cb70-1111" aria-hidden="true" tabindex="-1"></a>B6v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1112"><a href="#cb70-1112" aria-hidden="true" tabindex="-1"></a>W7v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1113"><a href="#cb70-1113" aria-hidden="true" tabindex="-1"></a>B7v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1114"><a href="#cb70-1114" aria-hidden="true" tabindex="-1"></a>W8v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1115"><a href="#cb70-1115" aria-hidden="true" tabindex="-1"></a>B8v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1116"><a href="#cb70-1116" aria-hidden="true" tabindex="-1"></a>W9v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1117"><a href="#cb70-1117" aria-hidden="true" tabindex="-1"></a>B9v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1118"><a href="#cb70-1118" aria-hidden="true" tabindex="-1"></a>W10v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1119"><a href="#cb70-1119" aria-hidden="true" tabindex="-1"></a>B10v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1120"><a href="#cb70-1120" aria-hidden="true" tabindex="-1"></a>W11v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1121"><a href="#cb70-1121" aria-hidden="true" tabindex="-1"></a>B11v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1122"><a href="#cb70-1122" aria-hidden="true" tabindex="-1"></a>W12v4 <span class="op">=</span> init_params((<span class="dv">10</span>, <span class="dv">10</span>), <span class="fl">0.001</span>)</span>
<span id="cb70-1123"><a href="#cb70-1123" aria-hidden="true" tabindex="-1"></a>B12v4 <span class="op">=</span> init_params(<span class="dv">10</span>, <span class="fl">0.001</span>)</span>
<span id="cb70-1124"><a href="#cb70-1124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1125"><a href="#cb70-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1126"><a href="#cb70-1126" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mdlV4(xb):</span>
<span id="cb70-1127"><a href="#cb70-1127" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> xb<span class="op">@</span>W1v4<span class="op">+</span>B1v4</span>
<span id="cb70-1128"><a href="#cb70-1128" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1129"><a href="#cb70-1129" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W2v4<span class="op">+</span>B2v4</span>
<span id="cb70-1130"><a href="#cb70-1130" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1131"><a href="#cb70-1131" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W3v4<span class="op">+</span>B3v4</span>
<span id="cb70-1132"><a href="#cb70-1132" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1133"><a href="#cb70-1133" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W4v4<span class="op">+</span>B4v4</span>
<span id="cb70-1134"><a href="#cb70-1134" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1135"><a href="#cb70-1135" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W5v4<span class="op">+</span>B5v4</span>
<span id="cb70-1136"><a href="#cb70-1136" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1137"><a href="#cb70-1137" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W6v4<span class="op">+</span>B6v4</span>
<span id="cb70-1138"><a href="#cb70-1138" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1139"><a href="#cb70-1139" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W7v4<span class="op">+</span>B7v4</span>
<span id="cb70-1140"><a href="#cb70-1140" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1141"><a href="#cb70-1141" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W8v4<span class="op">+</span>B8v4</span>
<span id="cb70-1142"><a href="#cb70-1142" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1143"><a href="#cb70-1143" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W9v4<span class="op">+</span>B9v4</span>
<span id="cb70-1144"><a href="#cb70-1144" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1145"><a href="#cb70-1145" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W10v4<span class="op">+</span>B10v4</span>
<span id="cb70-1146"><a href="#cb70-1146" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1147"><a href="#cb70-1147" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W11v4<span class="op">+</span>B11v4</span>
<span id="cb70-1148"><a href="#cb70-1148" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res.<span class="bu">max</span>(tensor(<span class="fl">0.</span>))</span>
<span id="cb70-1149"><a href="#cb70-1149" aria-hidden="true" tabindex="-1"></a>    res <span class="op">=</span> res<span class="op">@</span>W12v4<span class="op">+</span>B12v4</span>
<span id="cb70-1150"><a href="#cb70-1150" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> res</span>
<span id="cb70-1151"><a href="#cb70-1151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1152"><a href="#cb70-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1153"><a href="#cb70-1153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1154"><a href="#cb70-1154" aria-hidden="true" tabindex="-1"></a>And trying it...</span>
<span id="cb70-1157"><a href="#cb70-1157" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb70-1158"><a href="#cb70-1158" aria-hidden="true" tabindex="-1"></a><span class="co"># | code-line-numbers: true</span></span>
<span id="cb70-1159"><a href="#cb70-1159" aria-hidden="true" tabindex="-1"></a>my_try <span class="op">=</span> cTrial(model<span class="op">=</span>mdlV4, opt<span class="op">=</span>BasicOptim([W1v4, B1v4, W2v4, B2v4, W3v4, B3v4, W4v4, B4v4, W5v4, B5v4, W6v4, B6v4, W7v4, B7v4, W8v4, B8v4, W9v4, B9v4, W10v4, B10v4, W11v4, B11v4, W12v4, B12v4], <span class="fl">0.01</span>), params<span class="op">=</span>[</span>
<span id="cb70-1160"><a href="#cb70-1160" aria-hidden="true" tabindex="-1"></a>                W1v4, B1v4, W2v4, B2v4, W3v4, B3v4, W4v4, B4v4, W5v4, B5v4, W6v4, B6v4, W7v4, B7v4, W8v4, B8v4, W9v4, B9v4, W10v4, B10v4, W11v4, B11v4, W12v4, B12v4])</span>
<span id="cb70-1161"><a href="#cb70-1161" aria-hidden="true" tabindex="-1"></a>my_try.run(numE<span class="op">=</span><span class="dv">20</span>, wkLr<span class="op">=</span><span class="fl">.1</span>)</span>
<span id="cb70-1162"><a href="#cb70-1162" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.res.mean(dim<span class="op">=</span><span class="dv">1</span>)[<span class="dv">1</span>:])</span>
<span id="cb70-1163"><a href="#cb70-1163" aria-hidden="true" tabindex="-1"></a>plt.plot(my_try.valids)</span>
<span id="cb70-1164"><a href="#cb70-1164" aria-hidden="true" tabindex="-1"></a>plt.plot([<span class="fl">0.9</span> <span class="cf">for</span> x <span class="kw">in</span> my_try.valids])</span>
<span id="cb70-1165"><a href="#cb70-1165" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb70-1166"><a href="#cb70-1166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-1167"><a href="#cb70-1167" aria-hidden="true" tabindex="-1"></a>Curiously, absolute garbage performance. After investigating, my best estimate is that the reason this doesn't perform well is that the larger number of layers leads to overblown activation values. but can't be sure. I tried starting with much smaller initialziation values to no effect, as well well larger and smaller learning rates. We'll leave this one as a lost cause. </span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>