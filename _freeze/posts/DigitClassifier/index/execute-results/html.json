{
  "hash": "f161828ad70feb6ef71e61fddb0e28b1",
  "result": {
    "markdown": "---\ntitle: \"A Simple Digit Classifier\"\nauthor: \"David De Sa\"\ndate: \"2023-03-14\"\ncategories: [python, pyTorch, NeuralNetworks]\nimage: \"NN101_thumbnail.png\"\nformat:\n  html:\n    code-fold: true\n    toc: true\n    code-tools: true\ndraft: true\n---\n\n## TL;DR\nThe best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! I built this in following along with the awesome lessons over at [fast.ai](course.fast.ai). Many thanks to that team.\n\n# Overview\n### What?\nWe're going to make a function that, given a picture of a numeric digit, identifes the number.\n\n### Why?\nYou need to crawl before you can [reject unripe tomatoes](https://www.reddit.com/r/oddlysatisfying/comments/zw3iwq/machine_that_rejects_unripe_tomatoes)^[Too fun not to share though I think this is a machine vision implementation without neural nets. Probably just averaging colour across a pixel range to trigger the paddles.], and that before you can comfortably learn to tie a necktie while your Tesla is whipping around corners with you in the drivers seat.\n\n### Who?\n[Who am I](https://davidd003.github.io/Coding_Blog/about.html)!? Who are you?!\n\n### How?\nUsing [PyTorch](https://pytorch.org/)^[If it ain't 'py', it ain't python, right?], an opensource toolkit for building neural networks. Truly the shoulders of giants at our finger tips.\n\n# Code Review\nMaking a neural network to solve a problem is a bunch of mumbo jumbo if we're not actually performing better than a simpler heuristic function. To test that, we will start off by constructing a simple classification that classifies a digit based on which average digit image it is nearest to (You'll see what I mean later). Then we will build a simple neural network (not too deep, not too convolutional, just right), and try to out perform the naive function!\n\nLet's get into it!\n\n::: {.callout-note collapse=\"true\"}\nThe required dependencies!:`scikit-learn`, `fastbook`, `matplotlib`\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n# Install dependency\nfrom fastbook import *\nfrom fastai.vision.all import *\nfrom sklearn.datasets import load_digits\nimport fastbook\n\nfastbook.setup_book()\n\n\nmatplotlib.rc(\"image\", cmap=\"Greys\")\n```\n:::\n\n\n## Data Acquisition\nIn any real world ML application, data acquisition can be one of the more costly  parts of the process, luckily not so for this simple learning example. \n\nWe're using a variant of the classic NIST database, a collection of images of hand drawn numbers that provided the means for benchmarking in earlier days of ML. \n\nI had trouble wrangling with the various sources for this database online, the simplest workable solution I could find for us to get a grip on these images was to just import the datasets library that comes with installing the scikit-learn package.\n\n::: {.callout-tip collapse=\"true\"}\n## Understand Your Input!\n\nPre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the [source page](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits): \n\n> We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nmnist = load_digits()\n```\n:::\n\n\nAlways good to get to know your data..\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nmnist.keys()\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\ndict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])\n```\n:::\n:::\n\n\nWhat's in here?\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Observing y value for data sequence\nmnist[\"target\"], \"# targets: \" + str(len(mnist[\"target\"]))\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n(array([0, 1, 2, ..., 8, 9, 8]), '# targets: 1797')\n```\n:::\n:::\n\n\nSo we have 1797 numbers in this data set.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nn0 = [[n for n in mnist[\"data\"][0][i * 8: i * 8 + 7]] for i in range(0, 8)]\nn0, mnist[\"images\"][0]\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```\n([[0.0, 0.0, 5.0, 13.0, 9.0, 1.0, 0.0],\n  [0.0, 0.0, 13.0, 15.0, 10.0, 15.0, 5.0],\n  [0.0, 3.0, 15.0, 2.0, 0.0, 11.0, 8.0],\n  [0.0, 4.0, 12.0, 0.0, 0.0, 8.0, 8.0],\n  [0.0, 5.0, 8.0, 0.0, 0.0, 9.0, 8.0],\n  [0.0, 4.0, 11.0, 0.0, 1.0, 12.0, 7.0],\n  [0.0, 2.0, 14.0, 5.0, 10.0, 12.0, 0.0],\n  [0.0, 0.0, 6.0, 13.0, 10.0, 0.0, 0.0]],\n array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n        [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n        [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n        [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n        [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n        [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n        [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n        [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]]))\n```\n:::\n:::\n\n\nAnd it looks like the 'data' entity is a list of one dimensional vectors, listing out the 64 pixels of each image, whereas the 'images' entity is the same info already organized into the 8x8 array of pixels.\n\nThe values in the arrays are from 0-16, as described in the source documentation. Important to keep in mind that we might want to normalize them all to a range from 0 to 1 for our purposes. We'll do that later.\n\nI had to do some funny indexing to tease that out. Something I learned along the way was the fantastic .view() function of the Tensor object in pyTorch. Tensors are like a numpy array, have a lot of features that will be critical for quickly creating neural nets. This object type was imported with fastbook.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nTensor(mnist[\"data\"][0]).view(-1, 8)\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```\ntensor([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n        [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n        [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n        [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n        [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n        [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n        [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n        [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])\n```\n:::\n:::\n\n\n::: {.callout-tip}\n## Tensor Views\nUsing -1 in the argument for the view function will auto-size the tensor based on the number of elements in the array, and the other dimensions specified. This should come in handy!\n:::\n\nFor a [classification task](https://docs.aws.amazon.com/machine-learning/latest/dg/types-of-ml-models.html) such as this, it's important to keep in mind that our data should be balanced in quantity per class. Let's take a look at how many we've got. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n[str(i) + \": \" + str(list(mnist[\"target\"]).count(i))\n for i in range(10)]  # Count of each digit in dataset\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```\n['0: 178',\n '1: 182',\n '2: 177',\n '3: 183',\n '4: 181',\n '5: 182',\n '6: 181',\n '7: 179',\n '8: 174',\n '9: 180']\n```\n:::\n:::\n\n\nSo, a little imbalance but nothing crazy. Worth checking though...\n\n::: {.callout-warning}\n## Beware Naive Optimization\nIf we train on a million images of 7's, and only a thousand 1's, we can be duped into thinking we're rocking a 0.1% error rate by a naive model that guesses '7' no matter what you give it!\n:::\n\n### Picturing Inputs\n\n:::{#fig-numbersamples}\n\n::: {.cell .column-page layout-ncol='5' execution_count=8}\n``` {.python .cell-code}\nfor i in range(10):\n    show_image(mnist[\"images\"][-i] / 16)  # Visualizing example digit\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-2.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-3.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-4.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-5.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-6.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-7.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-8.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-9.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-10.png){width=93 height=93}\n:::\n:::\n\n\nA few examples of our data. Can you read them?\n:::\nTurns out that pre-processing that comes baked in does make them pretty grainy. But nothing some training can't solve.\n\n### Bucketing Classes\nWe need to separate out our inputs for training purposes. We'll iterate across the 'targets' list, using the target numbers themselves as the index value to dump the corresponding 'image' data into the storage bin.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-fold=\"show\"}\nstacked = []\n# This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects\nfor i in range(10):\n    stacked.append([])\n# Assign all images to the right collection in the 'stacked' list, indexed by target\nfor i in range(len(mnist[\"target\"])):\n    stacked[mnist[\"target\"][i]].append(mnist[\"images\"][i])\nlens = [len(stacked[i]) for i in range(10)]\nlens, min(lens)  # Confirm counts of samples\n```\n\n::: {.cell-output .cell-output-display execution_count=61}\n```\n([178, 182, 177, 183, 181, 182, 181, 179, 174, 180], 174)\n```\n:::\n:::\n\n\nSo that worked, we now have a list of lists of arrays, the arrays being interpreted as images, the lists being collections of images, with all images in a given collection being an image of the same hand drawn number. And we see that we have the fewest samples of numbers 8's, so we'll take only that many samples (174) of every other image for our dataset. \n\n### Segmentation\nThe next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn't generate a validation set with a large imbalance in the number of classes to be tested in it. \n\nFirst we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.\n\nWe'll print out the size of these collections and take a peek at a sample to make sure we indexed right.\n\n:::{#fig-checkSamp}\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# To make dataset a tensor, make it same number of dimensions\nstacked = tensor([x[:174] for x in stacked])\n# Segmentation: Pull 20 of each digit out of training set\ntest = [dig[-20:] for dig in stacked]\ntrain = [dig[:-20] for dig in stacked]\n# Confirm counts of samples\n[len(test[i]) for i in range(10)], [len(train[i]) for i in range(10)]\nshow_image(stacked[3][0])  # Check sample\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-2.png){width=93 height=93}\n:::\n:::\n\n\nIs it a 3?\n:::\nNice.\n\nIt's important to keep track of what's what.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code code-fold=\"show\"}\ntype(train), type(train[0]), type(train[0][0]), [\n    type(test), type(test[0]), type(test[0][0])]\n```\n\n::: {.cell-output .cell-output-display execution_count=63}\n```\n(list, torch.Tensor, torch.Tensor, [list, torch.Tensor, torch.Tensor])\n```\n:::\n:::\n\n\nOk so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.\n\nInstead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it's an easy conversion\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\n# Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)\ntrain = torch.stack(train)\ntest = torch.stack(test)\ntrain.shape, test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=64}\n```\n(torch.Size([10, 154, 8, 8]), torch.Size([10, 20, 8, 8]))\n```\n:::\n:::\n\n\nNow here is a crtiical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size. \n\n## Building Benchmark Function\nWhere it gets fun then is averaging and such across these dimensions. By doing so we can get the 'average drawing of a number,' which will be integral to creating our benchmark classification function.\n\n### The 'Average' Digit\n\n:::{#fig-ideals}\n\n::: {.cell .column-page layout-ncol='5' execution_count=13}\n``` {.python .cell-code}\nmeans = torch.stack([x.mean(0) for x in train])  # Compute the average digit\nfor i in range(10):\n    show_image(means[i])\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-3.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-4.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-5.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-6.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-7.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-8.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-9.png){width=93 height=93}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-10.png){width=93 height=93}\n:::\n:::\n\n\nNow those are some digits!\n:::\nWhat just happened, how did we get this? let's tear this one apart.\n\nFirst of all, we're dealing with a 4 dimensional tensor, `train`. When we jumped into a list comprehension iterating `for x in train`, we 'stepped into' that 0^th^ dimension, so to speak. Then any given element `x` is a 3 dimensional tensor. \n\nWe will go through 10 of them, one for each integer, and each will contain 174 `8x8` images. When we take the mean in the 0^th^ dimension of `x`, we are saying \"Across these 172 samples of 8x8 containers, what are the average values for element?\" A visual way to think of this is that you have 174 pages, each with an `8x8` grid of numbers on it. We will reduce it to a single page by taking the average through all the pages, for each number; i.e. the 1^st^ number on our single summary page will be the average of the 1^st^ number from all of the 174 pages. The 2^nd^ number will be the average of all the 2^nd^ numbers, etc. \n\nIn practice, this means that the more samples in which a given pixel was inked, the darker that pixel will be in the average.\n\n### Least-Difference As Decision\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}