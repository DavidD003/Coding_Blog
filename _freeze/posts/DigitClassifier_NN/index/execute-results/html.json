{
  "hash": "03365573b343464ae8c89d063b051e82",
  "result": {
    "markdown": "---\ntitle: \"A Simple Digit Classifier\"\nauthor: \"David De Sa\"\ndate: \"2023-03-14\"\ncategories: [python, pyTorch, NeuralNetworks]\nimage: \"NN101_thumbnail.png\"\nformat:\n  html:\n    code-fold: true\n    toc: true\n    code-tools: true\ndraft: true\n---\n\n## TL;DR\nThe best way to learn is to teach, so in this post I try to teach the reader how my first neural network implementation works! Part 2 of 2. I built this in following along with the awesome lessons over at [fast.ai](course.fast.ai). Many thanks to that team.\n\n# Overview\nWe're going to jump in where we left off in Part 1: We'll just reimport our data, and get into it.\n\n### What?\nWe're going to create a neural network that, given a picture of a numeric digit, identifies the number.\n\n### Why?\nDoes this really need explaining? Because the technology is amazing in both practical and philosophical ways.\n\n### Who?\n[Who am I](https://davidd003.github.io/Coding_Blog/about.html)!? Who are you?!\n\n### How?\nUsing [PyTorch](https://pytorch.org/)^[If it ain't 'py', it ain't python, right?], an opensource toolkit for building neural networks. Truly an elevator ride up onto the shoulders of giants.\n\n# Code Review\nWe'll first just re-import our data, and then get into building the groundwork for our neural network\n\nLet's get into it!\n\n::: {.callout-note collapse=\"true\"}\nThe required dependencies!:`scikit-learn`, `fastbook`, `matplotlib`\n:::\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# Install dependency\nfrom fastbook import *\nfrom fastai.vision.all import *\nfrom sklearn.datasets import load_digits\nimport fastbook\n\nfastbook.setup_book()\n\n\nmatplotlib.rc(\"image\", cmap=\"Greys\")\n```\n:::\n\n\n## Data Acquisition\nA quick flashback to Part 1 of my Building A Digit Classifier blog!:\n\n::: {.callout-tip collapse=\"true\"}\n## Understand Your Input!\n\nPre-processing data before even touching any neural net methods can improve your final performance. Note the data set information offered at the [source page](https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits): \n\n> We used preprocessing programs made available by NIST to extract normalized bitmaps of handwritten digits from a preprinted form. From a total of 43 people, 30 contributed to the training set and different 13 to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of 4x4 and the number of on pixels are counted in each block. This generates an input matrix of 8x8 where each element is an integer in the range 0..16. This reduces dimensionality and gives invariance to small distortions.\n:::\n\n::: {.cell execution_count=2}\n``` {.python .cell-code code-line-numbers=\"true\"}\nmnist = load_digits()\nstacked = []\n# This loop because stacked=[[]]*10 makes 1 list in list, with 10 copies of pointers... need separate objects\nfor i in range(10):\n    stacked.append([])\n# Assign all images to the right collection in the 'stacked' list, indexed by target\nfor i in range(len(mnist[\"target\"])):\n    stacked[mnist[\"target\"][i]].append(mnist[\"images\"][i])\n```\n:::\n\n\n### Segmentation\nThe next step is to define which data will be our training, and our validation set. It was important to bucket out our data first so by randomly sampling our data we didn't generate a validation set with a large imbalance in the number of classes to be tested in it. \n\nFirst we convert to a tensor, then segment training from validation data. Arbitrarily taking 20 examples from each digit, so, 11.5% of the total data set towards validation.\n\nWe'll print out the size of these collections and take a peek at a sample to make sure we indexed right.\n\n:::{#fig-checkSamp}\n\n::: {.cell execution_count=3}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# To make dataset a tensor, make it same number of dimensions\nstacked = tensor([x[:174] for x in stacked])\n# Segmentation: Pull 20 of each digit out of training set\ntest = [dig[-20:] for dig in stacked]\ntrain = [dig[:-20] for dig in stacked]\n# Confirm counts of samples\n[len(test[i]) for i in range(10)], [len(train[i]) for i in range(10)]\nshow_image(stacked[3][0])  # Check sample\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n<Axes: >\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-2.png){width=93 height=93}\n:::\n:::\n\n\nIs it a 3?\n:::\nNice.\n\nIt's important to keep track of what's what.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code code-fold=\"show\" code-line-numbers=\"true\"}\ntype(train), type(train[0]), type(train[0][0]), [\n    type(test), type(test[0]), type(test[0][0])]\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(list, torch.Tensor, torch.Tensor, [list, torch.Tensor, torch.Tensor])\n```\n:::\n:::\n\n\nOk so our top level containers for training/testing data are basic python lists. Within those, we have 10 collections, one for each integer. Those are Tensors. And then, each image (a collection of pixels unto itself) within those tensors, are also Tensor type objects.\n\nInstead of a basic Python list, we will need the top level containers as tensors to leverage the pyTorch functionality built into them. luckily it's an easy conversion:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# Converts PyList of tensors to tensor of tesnors (join in new dimensions, retain target indexing)\ntrain = torch.stack(train)\ntest = torch.stack(test)\ntrain.shape, test.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n(torch.Size([10, 154, 8, 8]), torch.Size([10, 20, 8, 8]))\n```\n:::\n:::\n\n\nNow here is a critical piece, working with multidimensional arrays and keeping in mind what we understand these to be. Our test and training tensors have the same dimensionality but not the same size. \n\n### Setup DataLoaders\nFirst get data into the requisite shape for the processes that will follow. \n\n::: {.cell execution_count=6}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# Build training/valid sets for passing to DataLoader\ntrain_x = torch.cat([x for x in train]).view(-1, 8*8)\ntest_x = torch.cat([x for x in test]).view(-1, 8*8)\ntrain_y, test_y = [], []\nfor n in range(10):\n    train_y.extend([n]*154)\n    test_y.extend([n]*20)\ntrain_y = tensor(train_y).unsqueeze(-1)\ntest_y = tensor(test_y).unsqueeze(-1)\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n(torch.Size([1540, 64]),\n torch.Size([200, 64]),\n torch.Size([1540, 1]),\n torch.Size([200, 1]))\n```\n:::\n:::\n\n\nNote the unsqueezing such that the `_y` tensors have a singleton dimension. Might seem strange on first blush but makes sense when considering that our 'target' just happens to be individual integers. But in other use cases, the target could be something more complex, requiring a larger data structure. In which case that dimension could be larger than a singleton.\n\nNow we can prime our DataLoaders. DataLoaders are a super handy object from the Pytorch library that will make life easier by holding our data for us, and serving it up in randomized batches.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# Zip each input data item to its target output\ndset = list(zip(train_x, train_y))\nvalid_dset = list(zip(test_x, test_y))\n# Define DataLoader objects to pass to learner\ndl = DataLoader(dset, batch_size=25, shuffle=true)\nvalid_dl = DataLoader(valid_dset, batch_size=5, shuffle=true)\n\ndls = DataLoaders(dl, valid_dl)\n```\n:::\n\n\n## Building The NN Foundation\nEvery NN needs weights and biases. We're going to want to initialize lots of them, and in differently shaped structures, so first we define a function to help us do that:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-line-numbers=\"true\"}\ndef init_params(size, std=.1): return (torch.randn(size)*std).requires_grad_()\n\n```\n:::\n\n\n::: {.callout-note collapse=\"true\"}\n## Requiring Grad!\nNote that any memory structures created with that function will also store their gradient in memory as computation is performed on them. This will be critical to executing back propagation of the gradient so as to optimize our function.\n:::\n\nNow we can generate our weights and biases. We define the structure sizes with our input and output size in mind, and the principles of matrix multiplication. \n\nThe general structure we will start with will be, of course, an initial layer with one neuron for each input, which will yield a single activation per input. After a nonlinearty, the second layer will have 10 weights and biases, such that the output is a 1D vector of size 10, per input. Each output neuron will represent an estimation of likelihood that the integer is that which the neuron represents (0-9).\n\n![Visualizing Layer 1](Layer 1 Diagram.png){#fig-Layer1}  \n\n::: {.callout-tip }\n## Matrix Multiplication!\nNever fear! It's easy. I never forgot the trick Dr. Mohammed Jaber taught us 4th year as we had to use these methods from first year: ROW. COLUMN.\n\nTake row i from the first matrix, and column j from the second. The element in index i,j of the resulting matrix will be the dot product of that row and column vector.\n:::\n\n::: {.cell execution_count=9}\n``` {.python .cell-code code-line-numbers=\"true\"}\n# Have to provide extra dimension for matrix mutiplication, making it a single-column vector\nw1 = init_params((8*8, 1))\nb1 = init_params(1)\n# Have to provide an extra dimension (1) for matrix multiplication\nw2 = init_params((1, 10))\nb2 = init_params(1)\n```\n:::\n\n\n - Layer 1:\n    - 64 weights, one per pixel. Formatted in a column vector because they are the second term in the matrix multiplication equation, with a single input (image) being fed as a single row vector of 64 elements, one per pixel. \n    - 1 bias; it will be broadcast such that the same bias is added to all instances in our batch. See @fig-Layer1\n - Layer 2:\n    - 10 weights, to generate one output neuron per category/class to assign. \n    - 10 biases, one *for each* output neuron\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}